[{"content":"A Quickstart guide to deploying an Azure Red Hat OpenShift cluster.\nAuthor: Paul Czarkowski\nVideo Walkthrough If you prefer a more visual medium, you can watch Paul Czarkowski walk through this quickstart on YouTube.\nPrerequisites Azure CLI Obviously you’ll need to have an Azure account to configure the CLI against.\nMacOS\nSee Azure Docs for alternative install options.\nInstall Azure CLI using homebrew\nbrew update \u0026\u0026 brew install azure-cli Linux\nSee Azure Docs for alternative install options.\nImport the Microsoft Keys\nsudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository\ncat \u003c\u003c EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI\nsudo dnf install -y azure-cli Prepare Azure Account for Azure OpenShift Log into the Azure CLI by running the following and then authorizing through your Web Browser\naz login Make sure you have enough Quota (change the location if you’re not using East US)\naz vm list-usage --location \"East US\" -o table see Addendum - Adding Quota to ARO account if you have less than 36 Quota left for Total Regional vCPUs.\nRegister resource providers\naz provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait Get Red Hat pull secret This step is optional, but highly recommended\nLog into https://console.redhat.com\nBrowse to https://console.redhat.com/openshift/install/azure/aro-provisioned\nclick the Download pull secret button and remember where you saved it, you’ll reference it later.\nDeploy Azure OpenShift Variables and Resource Group Set some environment variables to use later, and create an Azure Resource Group.\nSet the following environment variables\nChange the values to suit your environment, but these defaults should work.\nAZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=openshift AZR_CLUSTER=cluster AZR_PULL_SECRET=~/Downloads/pull-secret.txt Create an Azure resource group\naz group create \\ --name $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION Networking Create a virtual network with two empty subnets\nCreate virtual network\naz network vnet create \\ --address-prefixes 10.0.0.0/22 \\ --name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP Create control plane subnet\naz network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet\naz network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies on the control plane subnet\nThis is required for the service to be able to connect to and manage the cluster.\naz network vnet subnet update \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --disable-private-link-service-network-policies true Create the cluster\nThis will take between 30 and 45 minutes.\naz aro create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER \\ --vnet \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --master-subnet \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-subnet \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --pull-secret @$AZR_PULL_SECRET Get OpenShift console URL\naz aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query consoleProfile Get OpenShift credentials\naz aro list-credentials \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv Use the URL and the credentials provided by the output of the last two commands to log into OpenShift via a web browser.\nDeploy an application to OpenShift\nSee the following video for a guide on easy application deployment on OpenShift.\nDelete Cluster Once you’re done its a good idea to delete the cluster to ensure that you don’t get a surprise bill.\nDelete the cluster\naz aro delete -y \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER Delete the Azure resource group\nOnly do this if there’s nothing else in the resource group.\naz group delete -y \\ --name $AZR_RESOURCE_GROUP Adendum Adding Quota to ARO account Create an Azure Support Request\nSet Issue Type to “Service and subscription limits (quotas)”\nSet Quota Type to “Compute-VM (cores-vCPUs) subscription limit increases”\nClick Next Solutions »\nClick Enter details\nSet Deployment Model to “Resource Manager\nSet Locations to “(US) East US”\nSet Types to “Standard”\nUnder Standard check “DSv3” and “DSv4”\nSet New vCPU Limit for each (example “60”)\nClick Save and continue\nClick Review + create »\nWait until quota is increased.\n","description":"","tags":["ARO","Azure","Quickstarts"],"title":"ARO Quickstart","uri":"/docs/quickstart-aro/"},{"content":"A Quickstart guide to deploying a Red Hat OpenShift cluster on AWS.\nAuthor: Steve Mirman\nVideo Walkthrough Quick Introduction to ROSA by Charlotte Fung on AWS YouTube channel\nIf you prefer a more visual medium, you can watch Steve Mirman walk through this quickstart on YouTube.\nPrerequisites AWS CLI You’ll need to have an AWS account to configure the CLI against.\nMacOS\nSee AWS Docs for alternative install options.\nInstall AWS CLI using the macOS command line\ncurl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" sudo installer -pkg AWSCLIV2.pkg -target / Linux\nSee AWS Docs for alternative install options.\nInstall AWS CLI using the Linux command line\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Windows\nSee AWS Docs for alternative install options.\nInstall AWS CLI using the Windows command line\nC:\\\u003e msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Docker\nSee AWS Docs for alternative install options.\nTo run the AWS CLI version 2 Docker image, use the docker run command.\ndocker run --rm -it amazon/aws-cli command Prepare AWS Account for OpenShift Configure the AWS CLI by running the following command\naws configure You will be required to enter an AWS Access Key ID and an AWS Secret Access Key along with a default region name and output format\n% aws configure AWS Access Key ID []: AWS Secret Access Key []: Default region name [us-east-2]: Default output format [json]: The AWS Access Key ID and AWS Secret Access Key values can be obtained by logging in to the AWS console and creating an Access Key in the Security Credentials section of the IAM dashboard for your user\nValidate your credentials\naws sts get-caller-identity You should receive output similar to the following\n{ \"UserId\": \u003cyour ID\u003e, \"Account\": \u003cyour account\u003e, \"Arn\": \u003cyour arn\u003e } If this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following\naws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\" Get a Red Hat Offline Access Token Log into cloud.redhat.com\nBrowse to https://cloud.redhat.com/openshift/token/rosa\nCopy the Offline Access Token and save it for the next step\nSet up the OpenShift CLI (oc) Download the OS specific OpenShift CLI from Red Hat\nUnzip the downloaded file on your local machine\nPlace the extracted oc executable in your OS path or local directory\nSet up the ROSA CLI Download the OS specific ROSA CLI from Red Hat\nUnzip the downloaded file on your local machine\nPlace the extracted rosa and kubectl executables in your OS path or local directory\nLog in to ROSA\nrosa login You will be prompted to enter in the Red Hat Offline Access Token you retrieved earlier and should receive the following message\nLogged in as \u003cemail address\u003e on 'https://api.openshift.com' Verify ROSA privileges Verify that ROSA has the minimal permissions\nrosa verify permissions Expected output: AWS SCP policies ok\nVerify that ROSA has the minimal quota\nrosa verify quota Expected output: AWS quota ok\nInitialize ROSA Initialize the ROSA CLI to complete the remaining validation checks and configurations\nrosa init Deploy Red Hat OpenShift on AWS (ROSA) Interactive Installation ROSA can be installed using command line parameters or in interactive mode. For an interactive installation run the following command\nrosa create cluster --interactive --mode auto As part of the interactive install you will be required to enter the following parameters or accept the default values (if applicable)\nCluster name: Multiple availability zones (y/N): AWS region (select): OpenShift version (select): Install into an existing VPC (y/N): Compute nodes instance type (optional): Enable autoscaling (y/N): Compute nodes [2]: Machine CIDR [10.0.0.0/16]: Service CIDR [172.30.0.0/16]: Pod CIDR [10.128.0.0/14]: Host prefix [23]: Private cluster (y/N): Note: the installation process should take between 30 - 45 minutes\nGet the web console link to the ROSA cluster To get the web console link run the following command.\nSubstitute your actual cluster name for \u003ccluster-name\u003e\nrosa describe cluster --cluster=\u003ccluster-name\u003e Create cluster-admin user By default, only the OpenShift SRE team will have access to the ROSA cluster. To add a local admin user, run the following command to create the cluster-admin account in your cluster.\nSubstitute your actual cluster name for \u003ccluster-name\u003e\nrosa create admin --cluster=\u003ccluster-name\u003e Refresh your web browser and you should see the cluster-admin option to log in\nDelete Red Hat OpenShift on AWS (ROSA) Deleting a ROSA cluster consists of two parts\nDelete the cluster instance, including the removal of AWS resources. Substitute your actual cluster name for \u003ccluster-name\u003e\nrosa delete cluster --cluster=\u003ccluster-name\u003e Delete Cluster’s operator-roles and oidc-provider as shown in the above delete cluster command’s output. For e.g.\nrosa delete operator-roles -c \u003ccluster-name\u003e rosa delete oidc-provider -c \u003ccluster-name\u003e Delete the CloudFormation stack, including the removal of the osdCcsAdmin user rosa init --delete-stack ","description":"","tags":["AWS","ROSA","Quickstarts"],"title":"ROSA Quickstart","uri":"/docs/quickstart-rosa/"},{"content":"Azure Red Hat OpenShift is a fully managed, cloud-based service that allows users to quickly and easily deploy and manage containerized applications on the Azure platform. This product is a collaboration between Microsoft Azure and Red Hat, two industry leaders in cloud computing and open source software development. With Azure Red Hat OpenShift, users can leverage the benefits of Azure’s global infrastructure and scalability, as well as Red Hat’s expertise in containerization and open source technologies. This product is ideal for businesses that want to take advantage of the agility and flexibility of containers, but also need the reliability and security of a trusted cloud provider. ","description":"MOBB Docs and Guides for aro","tags":null,"title":"Azure Red Hat OpenShift","uri":"/docs/aro/"},{"content":"OpenShift on AWS allows users to run OpenShift in the Amazon Web Services (AWS) cloud, using AWS resources such as compute, storage, and networking. With OpenShift on AWS, users can leverage the benefits of the AWS cloud while also taking advantage of the features and capabilities of OpenShift. OpenShift on AWS is designed to be easy to use and to provide a smooth experience for developers, enabling them to focus on building and deploying applications rather than worrying about infrastructure. ","description":"MOBB Docs and Guides for rosa","tags":null,"title":"Red Hat Openshift Service on AWS","uri":"/docs/rosa/"},{"content":"","description":"","tags":null,"title":"AWS","uri":"/tags/aws/"},{"content":"Author Shaozhen Ding, Paul Czarkowski\nlast edited: 01/05/2023\nAWS Load Balancer Controller is a controller to help manage Elastic Load Balancers for a Kubernetes cluster.\nIt satisfies Kubernetes Ingress resources by provisioning Application Load Balancers. It satisfies Kubernetes Service resources by provisioning Network Load Balancers. Compared with default AWS In Tree Provider, this controller is actively developed with advanced annotations for both ALB and NLB. Some advanced usecases are:\nUsing native kubernetes ingress with ALB Integrate ALB with WAF Specify NLB source IP ranges Specify NLB internal IP address AWS Load Balancer Operator is used to used to install, manage and configure an instance of aws-load-balancer-controller in a OpenShift cluster.\nPrerequisites A multi AZ ROSA cluster deployed with STS AWS CLI OC CLI Environment Prepare the environment variables\nexport AWS_PAGER=\"\" export ROSA_CLUSTER_NAME=$(oc get infrastructure cluster -o=jsonpath=\"{.status.infrastructureName}\" | sed 's/-[a-z0-9]\\{5\\}$//') export REGION=$(oc get infrastructure cluster -o=jsonpath=\"{.status.platformStatus.aws.region}\") export OIDC_ENDPOINT=$(oc get authentication.config.openshift.io cluster -o jsonpath='{.spec.serviceAccountIssuer}' | sed 's|^https://||') export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export SCRATCH=\"/tmp/${ROSA_CLUSTER_NAME}/alb-operator\" mkdir -p ${SCRATCH} echo \"Cluster: ${ROSA_CLUSTER_NAME}, Region: ${REGION}, OIDC Endpoint: ${OIDC_ENDPOINT}, AWS Account ID: ${AWS_ACCOUNT_ID}\" AWS VPC / Subnets Note: This section only applies to BYO VPC clusters, if you let ROSA create your VPCs you can skip to the following Installation section.\nSet Variables describing your VPC and Subnets:\nexport VPC=\u003cvpc-id\u003e export PUBLIC_SUBNET_IDS=\u003cpublic-subnets\u003e export PRIVATE_SUBNET_IDS=\u003cprivate-subnets\u003e export CLUSTER_NAME=$(oc get infrastructure cluster -o=jsonpath=\"{.status.infrastructureName}\") Tag VPC with the cluster name\naws ec2 create-tags --resources VPC_ID --tags Key=kubernetes.io/cluster/${CLUSTER_NAME},Value=owned Add tags to Public Subnets\naws ec2 create-tags \\ --resources \"${PUBLIC_SUBNET_IDS}\" \\ --tags Key=kubernetes.io/role/elb,Value='' Add tags to Private Subnets\naws ec2 create-tags \\ --resources \"${PRIVATE_SUBNET_IDS}\" \\ --tags Key=kubernetes.io/role/internal-elb,Value='' Installation Create Policy for the aws load balancer controller\nNote: Policy is from AWS Load Balancer Controller Policy plus subnet create tags permission (required by the operator)\noc new-project aws-load-balancer-operator POLICY_ARN=$(aws iam list-policies --query \\ \"Policies[?PolicyName=='aws-load-balancer-operator-policy'].{ARN:Arn}\" \\ --output text) if [[ -z \"${POLICY_ARN}\" ]]; then wget -O \"${SCRATCH}/load-balancer-operator-policy.json\" \\ https://raw.githubusercontent.com/rh-mobb/documentation/main/docs/rosa/aws-load-balancer-operator/load-balancer-operator-policy.json POLICY_ARN=$(aws --region \"$REGION\" --query Policy.Arn \\ --output text iam create-policy \\ --policy-name aws-load-balancer-operator-policy \\ --policy-document \"file://${SCRATCH}/load-balancer-operator-policy.json\") fi echo $POLICY_ARN Create trust policy for ALB Operator\ncat \u003c\u003cEOF \u003e \"${SCRATCH}/trust-policy.json\" { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Condition\": { \"StringEquals\" : { \"${OIDC_ENDPOINT}:sub\": [\"system:serviceaccount:aws-load-balancer-operator:aws-load-balancer-operator-controller-manager\", \"system:serviceaccount:aws-load-balancer-operator:aws-load-balancer-controller-cluster\"] } }, \"Principal\": { \"Federated\": \"arn:aws:iam::$AWS_ACCOUNT_ID:oidc-provider/${OIDC_ENDPOINT}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\" } ] } EOF Create Role for ALB Operator\nROLE_ARN=$(aws iam create-role --role-name \"${ROSA_CLUSTER_NAME}-alb-operator\" \\ --assume-role-policy-document \"file://${SCRATCH}/trust-policy.json\" \\ --query Role.Arn --output text) echo $ROLE_ARN aws iam attach-role-policy --role-name \"${ROSA_CLUSTER_NAME}-alb-operator\" \\ --policy-arn $POLICY_ARN Create secret for ALB Operator\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: aws-load-balancer-operator namespace: aws-load-balancer-operator stringData: credentials: | [default] role_arn = $ROLE_ARN web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token EOF Install Red Hat AWS Load Balancer Operator\ncat \u003c\u003c EOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: aws-load-balancer-operator namespace: aws-load-balancer-operator spec: targetNamespaces: - aws-load-balancer-operator --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: aws-load-balancer-operator namespace: aws-load-balancer-operator spec: channel: stable-v0 installPlanApproval: Automatic name: aws-load-balancer-operator source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: aws-load-balancer-operator.v0.2.0 EOF Install Red Hat AWS Load Balancer Controller\nNote: If you get an error here wait a minute and try again, it likely means the Operator hasn’t completed installing yet.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: networking.olm.openshift.io/v1alpha1 kind: AWSLoadBalancerController metadata: name: cluster spec: credentials: name: aws-load-balancer-operator EOF Check the Operator and Controller pods are both running\noc -n aws-load-balancer-operator get pods You should see the following, if not wait a moment and retry.\nNAME READY STATUS RESTARTS AGE aws-load-balancer-controller-cluster-6ddf658785-pdp5d 1/1 Running 0 99s aws-load-balancer-operator-controller-manager-577d9ffcb9-w6zqn 2/2 Running 0 2m4s Validate the deployment with Echo Server application Deploy Echo Server Ingress with ALB\noc apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/echoservice/echoserver-namespace.yaml oc adm policy add-scc-to-user anyuid system:serviceaccount:echoserver:default oc apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/echoservice/echoserver-deployment.yaml oc apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/echoservice/echoserver-service.yaml oc apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/echoservice/echoserver-ingress.yaml Curl the ALB ingress endpoint to verify the echoserver pod is accessible\nINGRESS=$(oc -n echoserver get ingress echoserver \\ -o jsonpath='{.status.loadBalancer.ingress[0].hostname}') curl -sH \"Host: echoserver.example.com\" \\ \"http://${INGRESS}\" | grep Hostname Hostname: echoserver-7757d5ff4d-ftvf2 Deploy Echo Server NLB Load Balancer\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: Service metadata: name: echoserver-nlb namespace: echoserver annotations: service.beta.kubernetes.io/aws-load-balancer-type: external service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing spec: ports: - port: 80 targetPort: 8080 protocol: TCP type: LoadBalancer selector: app: echoserver EOF Test the NLB endpoint\nNLB=$(oc -n echoserver get service echoserver-nlb \\ -o jsonpath='{.status.loadBalancer.ingress[0].hostname}') curl -s \"http://${NLB}\" | grep Hostname Hostname: echoserver-7757d5ff4d-ftvf2 Clean Up Delete the Operator and the AWS Roles\noc delete subscription aws-load-balancer-operator -n aws-load-balancer-operator aws iam detach-role-policy \\ --role-name \"${ROSA_CLUSTER_NAME}-alb-operator\" \\ --policy-arn $POLICY_ARN aws iam delete-role \\ --role-name \"${ROSA_CLUSTER_NAME}-alb-operator\" If you wish to delete the policy you can run\naws iam delete-policy --policy-arn $POLICY_ARN ","description":"","tags":["AWS","ROSA"],"title":"AWS Load Balancer Operator On ROSA","uri":"/docs/rosa/aws-load-balancer-operator/"},{"content":"","description":"","tags":null,"title":"ROSA","uri":"/tags/rosa/"},{"content":"","description":"","tags":null,"title":"Tags","uri":"/tags/"},{"content":"Author: Thatcher Hubbard\nThis guide shows how to deploy the Cluster Log Forwarder operator and configure it to use the Vector logging agent to forward logs to CloudWatch.\nVector will replaced FluentD as the default logging agent used by the Openshift Logging Operator when version 5.6 is released in Q4 2022. Version 5.5.3 of the operator can enable Vector by configuring it in the ClusterLogging resource.\nVersion 5.5.3 of the operator does not support passing an STS role to Vector, but version 5.6 will. Until 5.6 is released, using Vector will require passing traditional IAM creds, but the conversion from IAM to STS will be relatively straightforward and will be documented here when it’s available.\nPrerequisites A ROSA cluster (configured with STS) The jq cli command The aws cli command Environment Setup Configure the following environment variables\nChange the cluster name to match your ROSA cluster and ensure you’re logged into the cluster as an Administrator. Ensure all fields are outputted correctly before moving on.\nexport ROSA_CLUSTER_NAME=\u003ccluster_name\u003e export ROSA_CLUSTER_ID=$(rosa describe cluster -c ${ROSA_CLUSTER_NAME} --output json | jq -r .id) export REGION=$(rosa describe cluster -c ${ROSA_CLUSTER_NAME} --output json | jq -r .region.id) export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` export AWS_PAGER=\"\" export SCRATCH=\"/tmp/${ROSA_CLUSTER_NAME}/clf-cloudwatch-vector\" mkdir -p ${SCRATCH} echo \"Cluster ID: ${ROSA_CLUSTER_ID}, Region: ${REGION}, AWS Account ID: ${AWS_ACCOUNT_ID}\" Prepare AWS Account Create an IAM Policy for OpenShift Log Forwarding\nPOLICY_ARN=$(aws iam list-policies --query \"Policies[?PolicyName=='RosaCloudWatch'].{ARN:Arn}\" --output text) if [[ -z \"${POLICY_ARN}\" ]]; then cat \u003c\u003c EOF \u003e ${SCRATCH}/policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:DescribeLogGroups\", \"logs:DescribeLogStreams\", \"logs:PutLogEvents\", \"logs:PutRetentionPolicy\" ], \"Resource\": \"arn:aws:logs:*:*:*\" } ] } EOF POLICY_ARN=$(aws iam create-policy --policy-name \"RosaCloudWatch\" \\ --policy-document file:///${SCRATCH}/policy.json --query Policy.Arn --output text) fi echo ${POLICY_ARN} Create an IAM user for logging\naws iam create-user \\ --user-name $ROSA_CLUSTER_NAME-cloud-watch \\ \u003e $SCRATCH/aws-user.json Fetch Access and Secret Keys for IAM User\naws iam create-access-key \\ --user-name $ROSA_CLUSTER_NAME-cloud-watch \\ \u003e $SCRATCH/aws-access-key.json Attach Policy to AWS IAM User\naws iam attach-user-policy \\ --user-name $ROSA_CLUSTER_NAME-cloud-watch \\ --policy-arn ${POLICY_ARN} Create an OCP Secret to hold the AWS creds:\nAWS_ID=`cat $SCRATCH/aws-access-key.json | jq -r '.AccessKey.AccessKeyId'` AWS_KEY=`cat $SCRATCH/aws-access-key.json | jq -r '.AccessKey.SecretAccessKey'` cat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: cloudwatch-credentials namespace: openshift-logging stringData: aws_access_key_id: $AWS_ID aws_secret_access_key: $AWS_KEY EOF Deploy Operators Deploy the Cluster Logging operator\ncat \u003c\u003c EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/cluster-logging.openshift-logging: \"\" name: cluster-logging namespace: openshift-logging spec: channel: stable installPlanApproval: Automatic name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: cluster-logging.5.5.3 EOF Configure Cluster Logging Create a cluster log forwarding resource\ncat \u003c\u003c EOF | oc apply -f - apiVersion: \"logging.openshift.io/v1\" kind: ClusterLogForwarder metadata: name: instance namespace: openshift-logging spec: outputs: - name: cw type: cloudwatch cloudwatch: groupBy: namespaceName groupPrefix: rosa-${ROSA_CLUSTER_NAME} region: ${REGION} secret: name: cloudwatch-credentials pipelines: - name: to-cloudwatch inputRefs: - infrastructure - audit - application outputRefs: - cw EOF Create a cluster logging resource\ncat \u003c\u003c EOF | oc apply -f - apiVersion: logging.openshift.io/v1 kind: ClusterLogging metadata: name: instance namespace: openshift-logging spec: collection: logs: type: vector vector: {} forwarder: managementState: Managed EOF Check AWS CloudWatch for logs Use the AWS console or CLI to validate that there are log streams from the cluster\nNote: If this is a fresh cluster you may not see a log group for application logs as there are no applications running yet.\naws logs describe-log-groups --log-group-name-prefix rosa-${ROSA_CLUSTER_NAME} { \"logGroups\": [ { \"logGroupName\": \"rosa-xxxx.audit\", \"creationTime\": 1661286368369, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:us-east-2:xxxx:log-group:rosa-xxxx.audit:*\", \"storedBytes\": 0 }, { \"logGroupName\": \"rosa-xxxx.infrastructure\", \"creationTime\": 1661286369821, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:us-east-2:xxxx:log-group:rosa-xxxx.infrastructure:*\", \"storedBytes\": 0 } ] } Cleanup Delete the Cluster Log Forwarding resource\noc delete -n openshift-logging clusterlogforwarder instance Delete the Cluster Logging resource\noc delete -n openshift-logging clusterlogging instance Delete the IAM credential secret\noc -n openshift-logging delete secret cloudwatch-credentials Detach the IAM Policy to the IAM Role\naws iam detach-user-policy --user-name \"$ROSA_CLUSTER_NAME-cloud-watch\" \\ --policy-arn \"${POLICY_ARN}\" 1. Delete the IAM User access keys ```bash aws iam delete-access-key --user-name \"$ROSA_CLUSTER_NAME-cloud-watch\" \\ --access-key-id \"${AWS_ID}\" 1. Delete the IAM User ```bash aws iam delete-user --user-name \"$ROSA_CLUSTER_NAME-cloud-watch\" Delete the IAM Policy\nOnly run this command if there are no other resources using the Policy\naws iam delete-policy --policy-arn \"${POLICY_ARN}\" Delete the CloudWatch Log Groups\nIf there are any user workloads on the cluster they’ll have their own log groups that will also need to be deleted\naws logs delete-log-group --log-group-name \"rosa-${ROSA_CLUSTER_NAME}.audit\" aws logs delete-log-group --log-group-name \"rosa-${ROSA_CLUSTER_NAME}.infrastructure\" ","description":"","tags":["AWS","ROSA"],"title":"Configuring the Cluster Log Forwarder for CloudWatch Logs using Vector","uri":"/docs/rosa/clf-cloudwatch-vector/"},{"content":"last edited 11 Oct 2022\nThere may be situations when you prefer not to use wild-card certificates. This ROSA guide talks about certificate management with cert-manager and letsencrypt, to dynamically issue certificates to routes created on a custom domain that’s hosted on AWS Route53.\nPrerequisites Set up environment Prepare AWS Account Set up cert-manager Create the Issuer and the Certficiate Configure Certificate Requestor Create the Certificate, which will later be used by the Custom Domain. Create the Custom Domain, which will be used to access your applications. Dynamic Certificates for Custom Domain Routes. Test an application. Debugging Prerequisites A Red Hat OpenShift on AWS (ROSA) cluster The oc CLI #logged in. The aws CLI #logged in. The rosa CLI #logged in. jq gettext A Public Route53 Hosted Zone, and the related Domain to use. Set up environment Export few environment variables\nexport CLUSTER_NAME=\"sts-pvtlnk-cluster\" export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json \\ | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export SCRATCH_DIR=/tmp/scratch export AWS_PAGER=\"\" export LETSENCRYPT_EMAIL=youremail@work.com export HOSTED_ZONE_ID=ABCDEFGHEXAMPLE export HOSTED_ZONE_REGION=us-east-2 export DOMAIN=lab.domain.com #Custom Hosted Zone Domain for apps mkdir -p $SCRATCH_DIR Install jq \u0026 gettext\nInstalling or ensuring the gettext \u0026 jq package is installed, will allow us to use envsubst to simplify some of our configuration so we can use output of CLI commands as input into YAMLs to reduce the complexity of manual editing.\nbrew install gettext jq # or, for Linux / Windows WSL #sudo dnf install gettext jq Prepare AWS Account In order to make changes to the AWS Route53 Hosted Zone to add/remove DNS TXT challenge records by the cert-manager pod, we first need to create an IAM role with specific policy permissions \u0026 a trust relationship to allow access to the pod.\nMy Custom Domain Hosted Zone is in the same accout as the ROSA cluster. If these are in different accounts, few additional steps for Cross Account Access will be required.\nPrepare an IAM Policy file\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/cert-manager-r53-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"route53:GetChange\", \"Resource\": \"arn:aws:route53:::change/*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\" ], \"Resource\": \"arn:aws:route53:::hostedzone/*\" }, { \"Effect\": \"Allow\", \"Action\": \"route53:ListHostedZonesByName\", \"Resource\": \"*\" } ] } EOF Create the IAM Policy using the above created file.\nThis creates a named policy for the cluster, you could use a generic policy for multiple clusters to keep things simpler.\nPOLICY=$(aws iam create-policy --policy-name \"${CLUSTER_NAME}-cert-manager-r53-policy\" \\ --policy-document file://$SCRATCH_DIR/cert-manager-r53-policy.json \\ --query 'Policy.Arn' --output text) || \\ echo $POLICY Create a Trust Policy\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/TrustPolicy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": [ \"system:serviceaccount:cert-manager:cert-manager\" ] } } } ] } EOF Create an IAM Role for the cert-manager Operator, with the above trust policy.\nROLE=$(aws iam create-role \\ --role-name \"${CLUSTER_NAME}-cert-manager-operator\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $ROLE Attach the permissions policy to the Role\naws iam attach-role-policy \\ --role-name \"${CLUSTER_NAME}-cert-manager-operator\" \\ --policy-arn $POLICY Set up cert-manager Create a project (namespace) in the ROSA cluster.\noc new-project cert-manager --display-name=\"Certificate Manager\" --description=\"Project contains Certificates and Custom Domain related components.\" Install the cert-manager Operator\ncat \u003c\u003cEOF | oc create -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: cert-manager- namespace: cert-manager --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: cert-manager namespace: cert-manager spec: channel: stable installPlanApproval: Automatic name: cert-manager source: community-operators sourceNamespace: openshift-marketplace EOF It will take a few minutes for this operator to install and complete its set up.\nWait for cert-manager operator to finish installing.\nOur next steps depends on the successful installation of the operator. I recommend that you login to your cluster console to confirm the succeess status of cert-manager operator, in the Installed Operators section.\nAnnotate the ServiceAccount.\nThis is to enable the AWS SDK client code running within the cert-manager pod to interact with AWS STS service for temporary tokens, by assuming the IAM Role that was created in an earlier step. This is referred to as IRSA.\noc annotate serviceaccount cert-manager -n cert-manager eks.amazonaws.com/role-arn=$ROLE Normally, after ServiceAccount annotations, a restart of the pod is required. However, the next step will automatically cause a restart of the pod.\nUpdate the CA truststore of the cert-manager pod.\nThis step is usually not required. However, it was noticed that the cert-manager pod had difficulties in trusting the STS \u0026 LetsEncrypt endpoints. So the below commands essentially downloads the CA chain of these endpoints, puts them into a ConfigMap, which then gets attached to the pod as a Volume. Along with this step, I’ll also be setting the NameServers that the cert-manager will use for DNS01 self-check The Volume attachment to the pod and the setting of NameServers will be done together by patching the cert-manager CSV resource to persist these changes to the cert-manager deployment. This will cause the rollout of a new deployment and restart of the cert-manager pod.\nopenssl s_client -showcerts -verify 5 -connect sts.amazonaws.com:443 \u003c /dev/null 2\u003e /dev/null | awk '/BEGIN/,/END/{ if(/BEGIN/){a++}; print}' \u003e $SCRATCH_DIR/extra-ca.pem openssl s_client -showcerts -verify 5 -connect acme-v02.api.letsencrypt.org:443 \u003c /dev/null 2\u003e /dev/null | awk '/BEGIN/,/END/{ if(/BEGIN/){a++}; print}' \u003e\u003e $SCRATCH_DIR/extra-ca.pem oc create configmap extra-ca --from-file=$SCRATCH_DIR/extra-ca.pem -n cert-manager CERT_MANAGER_CSV_NAME=$(oc get csv | grep 'cert-manager' | awk '{print $1}') CLUSTER_DNS_SERVICE_IP=$(oc get svc -n openshift-dns | grep 'dns-default' | awk '{print $3}') echo $CERT_MANAGER_CSV_NAME echo $CLUSTER_DNS_SERVICE_IP oc patch csv $CERT_MANAGER_CSV_NAME --type='json' -p '[{\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/containers/0/args/-\", \"value\":'--dns01-recursive-nameservers-only'}, {\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/containers/0/args/-\", \"value\":'--dns01-recursive-nameservers=$CLUSTER_DNS_SERVICE_IP:53'}]' oc patch csv $CERT_MANAGER_CSV_NAME --type='json' -p '[{\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/volumes\", \"value\": [{\"name\": \"extra-ca\"}]}, {\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/volumes/0/configMap\", \"value\": {\"name\": \"extra-ca\", \"defaultMode\": 420}}, {\"op\": \"add\", \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/containers/0/volumeMounts\", \"value\": [{\"name\": \"extra-ca\", \"mountPath\": \"/etc/ssl/certs/extra-ca-bundle.pem\", \"readOnly\": true, \"subPath\": \"extra-ca-bundle.pem\"}]}]' During an Operator upgrade, the above changes might be lost. There seems to be improvement plans to facilitate these changes directly through the Operator config, but until then, it’d be a good idea to maintain some automation around this to persist these changes if it ever gets overridden to defaults.\nCreate the Issuer and the Certficiate Configure Certificate Requestor Create Cluster Issuer to use Let’s Encrypt\nenvsubst \u003c\u003cEOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencryptissuer spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: $LETSENCRYPT_EMAIL # This key doesn't exist, cert-manager creates it privateKeySecretRef: name: prod-letsencrypt-issuer-account-key solvers: - dns01: route53: hostedZoneID: $HOSTED_ZONE_ID region: $HOSTED_ZONE_REGION secretAccessKeySecretRef: name: '' EOF Describe the ClusterIssuer to confirm it’s ready.\noc describe clusterissuer letsencryptissuer You should see an output that mentions that the issuer is Registered/Ready\nConditions: Last Transition Time: 2022-11-17T10:29:37Z Message: The ACME account was registered with the ACME server Observed Generation: 1 Reason: ACMEAccountRegistered Status: True Type: Ready Events: \u003cnone\u003e Once the above command is complete, the status of the ClusterIssues in the cluster console will look similar to the below screenshot.\nCreate the Certificate, which will later be used by the Custom Domain. I’ve used a SAN certificate here to show how SAN certificates could be created, which will be useful for clusters intended to run only a fixed set of applications. However, this is optional; a single subject/domain certificate works too\nConfigure the Certificate\nenvsubst \u003c\u003cEOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: customdomain-cert namespace: cert-manager spec: secretName: custom-domain-certificate-tls issuerRef: name: letsencryptissuer kind: ClusterIssuer commonName: \"x.apps.$DOMAIN\" dnsNames: - \"x.apps.$DOMAIN\" - \"y.apps.$DOMAIN\" - \"z.apps.$DOMAIN\" EOF View the Certificate status\nIt’ll take upto 5 minutes for the Certificate to show as Ready status. If it takes too long, the oc describe command will mention issues if any.\noc get certificate customdomain-cert -n cert-manager oc describe certificate customdomain-cert -n cert-manager Create the Custom Domain, which will be used to access your applications. Create the Custom Domain\nenvsubst \u003c\u003cEOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: appdomain spec: domain: x.apps.$DOMAIN certificate: name: custom-domain-certificate-tls namespace: cert-manager scope: Internal EOF View the status of the Custom Domain\noc get customdomain appdomain -n cert-manager It will take 2-3 minutes for the custom domain to change from NotReady to Ready status. When ready, an endpoint also will be visible in the output of the above command, as shown below\nNext, we need to add a DNS record in my Custom Domain Route53 Hosted Zone to CNAME the the wildcard applications domain to the above obtained endpoint, as shown below. CUSTOM_DOMAIN_ENDPOINT=$(oc get customdomain appdomain -n cert-manager | grep 'appdomain' | awk '{print $2}') echo $CUSTOM_DOMAIN_ENDPOINT cat \u003c\u003cEOF \u003e $SCRATCH_DIR/add_cname_record.json { \"Comment\":\"Add apps CNAME to Custom Domain Endpoint\", \"Changes\":[{ \"Action\":\"CREATE\", \"ResourceRecordSet\":{ \"Name\": \"*.apps.$DOMAIN\", \"Type\":\"CNAME\", \"TTL\":30, \"ResourceRecords\":[{ \"Value\": \"$CUSTOM_DOMAIN_ENDPOINT\" }] } }] } EOF aws route53 change-resource-record-sets --hosted-zone-id $HOSTED_ZONE_ID --change-batch file://$SCRATCH_DIR/add_cname_record.json The wild card CNAME’ing avoids the need to create a new record for every new application. The certificate that each of these applications use will NOT be a wildcard certificate\nAt this stage, you will be able to expose cluster applications on any of the listed DNS names that were specified in the previously created Certificate. But what if you have many more applications that will need to be securely exposed too. Well, one approach is to keep updating the Certificate resource with additional SAN names as more applications prepare to get onboarded, and this Certificate update which will trigger an update to the Custom Domain to honor the newly added SAN names. Another approach is to dynamically issue a Certificate to every new Route; Read on to find the details about this latter approach.\nDynamic Certificates for Custom Domain Routes. Create OpenShift resources required for issuing Dynamic Certificates to Routes. This step will create a new deployment (and hence a pod) that’ll watch out for specifically annotated routes in the cluster, and if the issuer-kind and issuer-name annotations are found in a new route, it’ll request the Issuer (ClusterIssuer in my case) for a new Certificate that’s unique to this route and which’ll honor the hostname that was specified while creating the route.\noc apply -f https://github.com/cert-manager/openshift-routes/releases/latest/download/cert-manager-openshift-routes.yaml -n cert-manager Additonal OpenShift resources such as a ClusterRole (with permissions to watch and update the routes across the cluster), a ServiceAccount (with these permissions, that will be used to run this newly created pod) and a ClusterRoleBinding to bind these two resources, will be created in this step too. If the cluster does not have access to github, you may as well save the raw contents locally, and run oc apply -f localfilename.yaml -n cert-manager\nView the status of the new pod. Check if all the pods are running successfully and that the events do not mention any errors.\noc get po -n cert-manager Test an application. Create a test applciation in a new namespace.\noc new-project testapp oc new-app --docker-image=docker.io/openshift/hello-openshift -n testapp Expose the test application Service.\nLet’s create a Route to expose the application from outside the cluster, and annotate the Route to give it a new Certificate.\noc create route edge --service=hello-openshift testroute --hostname hello.apps.$DOMAIN -n testapp oc annotate route testroute -n testapp cert-manager.io/issuer-kind=ClusterIssuer cert-manager.io/issuer-name=letsencryptissuer It will take a 2-3 minutes for the Certificate to be created. The renewal of the certitificate will automatically be managed by the cert-manager compoenents as it approaches expiry.\nAccess the application Route.\nDo a curl test (or any http client of your preference) to confirm there are no certificate related errors.\nOutput should print “Hello OpenShfit!”, and you should also notice a line that says “subjectAltName: host hello.apps.$DOMIAN” matched cert’s “hello.apps.$DOMIAN”\ncurl -vv https://hello.apps.$DOMAIN Debugging Please note that while creating certificates, the validation process usually take upto 2-3 minutes to complete.\nIf you face issues during certificate create step, run ‘oc describe’ against each of - ‘certificate,certificaterequest,order \u0026 challenge’ resources to view the events/reasons that’ll mostly help with identifying the cause of the issue.\noc get certificate,certificaterequest,order,challenge This is a very helpful guide in debugging certificates as well.\nYou may also use the cmctl CLI tool for various certificate management activities such as to check the status of certificates, testing renewals etc.\n","description":"","tags":["AWS","ROSA"],"title":"Dynamic Certificates for ROSA Custom Domain","uri":"/docs/rosa/dynamic-certificates/"},{"content":"Michael McNeill\n5 October 2022\nThis guide demonstrates how to create and assign a static public IP address to an OpenShift service in Azure Red Hat OpenShift (ARO). By default, the public IP address assigned to an OpenShift service with a type of LoadBalancer created by an ARO cluster is only valid for the lifespan of that resource. If you delete the OpenShift service, the associated load balancer and IP address are also deleted. If you want to assign a specific IP address or retain an IP address for redeployed OpenShift services, you can create and use a static public IP address.\nThis guide will walk through the following steps:\nCreate a new static public IP address. Grant the Azure Red Hat OpenShift (ARO) cluster’s service principal access to the parent resource group. Create the load balancer service and assign the static public IP address. Prerequisites An existing ARO cluster. If you need an ARO cluster, see the quickstart here. The Azure CLI. If you need to install the Azure CLI, see the Microsoft documentation here. Before you begin Before we begin, we need to set a few environment variables that will help us run the commands included in the guide.\nRESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster PUBLIC_IP_NAME=example-pip # Replace this with the name you want your static public IP to have Create a new static public IP address Create a static public IP address by using the az network public ip create command. The following command creates a static IP resource using the name you specified above in the parent resource group of the cluster object. To create the IP, run the following command:\naz network public-ip create \\ --resource-group ${RESOURCE_GROUP} \\ --name ${PUBLIC_IP_NAME} \\ --sku Standard \\ --allocation-method static The static public IP address provisioned is displayed as a part of the output of the command. It will look something like this:\n{ \"publicIp\": { ... \"ipAddress\": \"40.121.183.52\", ... } } Grant the Azure Red Hat OpenShift (ARO) cluster’s service principal access to the parent resource group Next, we must grant the Azure Red Hat OpenShift (ARO) cluster’s service principal access to the network resources of the parent resource group where we’ve created the static public IP. This must be done because the public IP lives outside of the cluster’s managed resource group (which starts with aro-). To grant the necessary access, run the following command:\nCLIENT_ID=$(az aro show --resource-group ${RESOURCE_GROUP} --name ${CLUSTER_NAME} --query \"servicePrincipalProfile.clientId\" --output tsv) SUB_ID=$(az account show --query \"id\" --output tsv) az role assignment create \\ --assignee ${CLIENT_ID} \\ --role \"Network Contributor\" \\ --scope /subscriptions/${SUB_ID}/resourceGroups/${RESOURCE_GROUP} Create the load balancer service and assign the static public IP address. Finally, we need to create a LoadBalancer service inside of OpenShift that specifies the static public IP address, as well as the parent resource group. Next, generate the necessary YAML for the LoadBalancer service with the loadBalancerIP property and resource group annotation set. To do so, run the following command, making sure to replace the variables specified:\nPUBLIC_IP=$(az network public-ip show --resource-group ${RESOURCE_GROUP} --name ${PUBLIC_IP_NAME} --query ipAddress --output tsv) cat \u003c\u003c EOF \u003e pip-service.yaml apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/azure-load-balancer-resource-group: ${RESOURCE_GROUP} name: static-ip-lb spec: loadBalancerIP: ${PUBLIC_IP} type: LoadBalancer ports: - port: 443 selector: app: static-ip-lb EOF Feel free to further modify this output (which is saved in your current directory as pip-service.yaml).\nFinally, apply the service configuration to the cluster by running the following command (note this will deploy the service directly into the current namespace):\noc apply -f ./pip-service.yaml The cluster should provision the load balancer within a minute or two. You can verify this by running the following command:\noc describe service static-ip-lb The output will look similar to this:\nName: static-ip-lb Namespace: example Labels: \u003cnone\u003e Annotations: service.beta.kubernetes.io/azure-load-balancer-resource-group: example-rg Selector: app=static-ip-lb Type: LoadBalancer IP Family Policy: SingleStack IP Families: IPv4 IP: 172.30.74.108 IPs: 172.30.74.108 IP: 20.168.220.211 LoadBalancer Ingress: 20.168.220.211 Port: port-8080 443/TCP TargetPort: 8080/TCP NodePort: port-8080 31616/TCP Endpoints: 10.129.2.10:8080 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringLoadBalancer 31m service-controller Ensuring load balancer Normal EnsuredLoadBalancer 31m service-controller Ensured load balancer You can now access your load balancer using the IP address provided!\n","description":"","tags":["AWS","ROSA"],"title":"Configure a load balancer service to use a static public IP","uri":"/docs/rosa/static-ip-load-balancer/"},{"content":"","description":"","tags":null,"title":"STS","uri":"/tags/sts/"},{"content":" Tyler Stacey\nLast updated 4 Oct 2022\nTo proceed with the deployment of a ROSA cluster, an account must support the required roles and permissions. AWS Service Control Policies (SCPs) cannot block the API calls made by the installer or operator roles.\nDetails about the IAM resources required for an STS-enabled installation of ROSA can be found here: https://docs.openshift.com/rosa/rosa_architecture/rosa-sts-about-iam-resources.html\nThis guide is validated for ROSA v4.11.X.\nPrerequisites AWS CLI ROSA CLI v1.2.6 jq CLI AWS role with required permissions Verify ROSA Permissions To verify the permissions required for ROSA we can run the script below without ever creating any AWS resources.\nThe script uses the rosa, aws, and jq CLI commands to create files in the working directory that will be used to verify permissions in the account connected to the current AWS configuration.\nThe AWS Policy Simulator is used to verify the permissions of each role policy against the API calls extracted by jq; results are then stored in a text file appended with .results.\nThis script will verify the permissions for the current account and region.\n#!/bin/bash while getopts 'p:' OPTION; do case \"$OPTION\" in p) PREFIX=\"$OPTARG\" ;; ?) echo \"script usage: $(basename \\$0) [-p PREFIX]\" \u003e\u00262 exit 1 ;; esac done shift \"$(($OPTIND -1))\" rosa create account-roles --mode manual --prefix $PREFIX INSTALLER_POLICY=$(cat sts_installer_permission_policy.json | jq ) CONTROL_PLANE_POLICY=$(cat sts_instance_controlplane_permission_policy.json | jq) WORKER_POLICY=$(cat sts_instance_worker_permission_policy.json | jq) SUPPORT_POLICY=$(cat sts_support_permission_policy.json | jq) CCO_POLICY=$(cat openshift_cloud_credential_operator_cloud_credential_operator_iam_ro_creds_policy.json | jq) REGISTRY_POLICY=$(cat openshift_image_registry_installer_cloud_credentials_policy.json | jq) INGRESS_POLICY=$(cat openshift_ingress_operator_cloud_credentials_policy.json | jq) CSI_POLICY=$(cat openshift_cluster_csi_drivers_ebs_cloud_credentials_policy.json | jq) NETWORK_POLICY=$(cat openshift_cloud_network_config_controller_cloud_credentials_policy.json | jq) MACHINE_POLICY=$(cat openshift_machine_api_aws_cloud_credentials_policy.json | jq) simulatePolicy () { outputFile=\"${2}.results\" echo $2 aws iam simulate-custom-policy --policy-input-list \"$1\" --action-names $(jq '.Statement | map(select(.Effect == \"Allow\"))[].Action | if type == \"string\" then . else .[] end' \"$2\" -r) --output text \u003e $outputFile } simulatePolicy \"$INSTALLER_POLICY\" \"sts_installer_permission_policy.json\" simulatePolicy \"$CONTROL_PLANE_POLICY\" \"sts_instance_controlplane_permission_policy.json\" simulatePolicy \"$WORKER_POLICY\" \"sts_instance_worker_permission_policy.json\" simulatePolicy \"$SUPPORT_POLICY\" \"sts_support_permission_policy.json\" simulatePolicy \"$CCO_POLICY\" \"openshift_cloud_credential_operator_cloud_credential_operator_iam_ro_creds_policy.json\" simulatePolicy \"$REGISTRY_POLICY\" \"openshift_image_registry_installer_cloud_credentials_policy.json\" simulatePolicy \"$INGRESS_POLICY\" \"openshift_ingress_operator_cloud_credentials_policy.json\" simulatePolicy \"$CSI_POLICY\" \"openshift_cluster_csi_drivers_ebs_cloud_credentials_policy.json\" simulatePolicy \"$NETWORK_POLICY\" \"openshift_cloud_network_config_controller_cloud_credentials_policy.json\" Usage Instructions To use the script, run the following commands in a bash terminal (the -p option defines a prefix for the roles):\nmkdir scratch cd scratch curl https://raw.githubusercontent.com/rh-mobb/documentation/main/docs/rosa/verify-permissions/verify-permissions.sh --output verify-permissions.sh chmod +x verify-permissions.sh ./verify-permissions.sh -p SimPolTest After the script completes, review each results file to ensure that none of the required API calls are blocked:\n$ cat sts_support_permission_policy.json.results EVALUATIONRESULTS\tcloudtrail:DescribeTrails\tallowed\t* MATCHEDSTATEMENTS\tPolicyInputList.1\tIAM Policy ENDPOSITION\t6\t159 STARTPOSITION\t17\t3 EVALUATIONRESULTS\tcloudtrail:LookupEvents\tallowed\t* MATCHEDSTATEMENTS\tPolicyInputList.1\tIAM Policy ENDPOSITION\t6\t159 STARTPOSITION\t17\t3 EVALUATIONRESULTS\tcloudwatch:GetMetricData\tallowed\t* MATCHEDSTATEMENTS\tPolicyInputList.1\tIAM Policy ENDPOSITION\t6\t159 STARTPOSITION\t17\t3 ... If any actions are blocked, review the error provided by AWS and consult with your Administrator to determine if SCPs are blocking the required API calls.\n","description":"","tags":["AWS","ROSA","STS"],"title":"Verify Permissions for ROSA STS Deployment","uri":"/docs/rosa/verify-permissions/"},{"content":"","description":"","tags":null,"title":"ARO","uri":"/tags/aro/"},{"content":"","description":"","tags":null,"title":"Azure","uri":"/tags/azure/"},{"content":"Azure Red Hat Openshift clusters have built in metrics and logs that can be viewed by both Administrators and Developers via the OpenShift Console. But there are many reasons you might want to store and view these metrics and logs from outside of the cluster.\nThe OpenShift developers have anticipated this needs and have provided ways to ship both metrics and logs outside of the cluster. In Azure we have the Azure Blob storage service which is perfect for storing the data.\nIn this guide we’ll be setting up Thanos and Grafana Agent to forward cluster and user workload metrics to Azure Blob as well the Cluster Logging Operator to forward logs to Loki which stores the logs in Azure Blob.\nPrerequisites Azure CLI Terraform OC CLI Helm Git Preparation Note: This guide was written on Fedora Linux (using the zsh shell) running inside Windows 11 WSL2. You may need to modify these instructions slightly to suit your Operating System / Shell of choice.\nCreate some environment variables to be reused through this guide\nModify these values to suit your environment, especially the storage account name which must be globally unique.\nexport CLUSTER=\"aro-${USERNAME}\" export WORKDIR=\"/tmp/${CLUSTER}\" export NAMESPACE=mobb-aro-obs export AZR_STORAGE_ACCOUNT_NAME=\"aro${USERNAME}obs\" mkdir -p ${WORKDIR} cd \"${WORKDIR}\" Log into Azure CLI\naz login Create ARO Cluster You can skip this step if you already have a cluster, or if you want to create it another way.\nThis will create a default ARO cluster named aro-${USERNAME}, you can modify the TF variables/Makefile to change settings, just update the environment variables loaded above to suit.\nclone down the Black Belt ARO Terraform repo\ngit clone https://github.com/rh-mobb/terraform-aro.git cd terraform-aro Initialize, Create a plan, and apply\nmake create This should take about 35 minutes and the final lines of the output should look like\nazureopenshift_redhatopenshift_cluster.cluster: Still creating... [35m30s elapsed] azureopenshift_redhatopenshift_cluster.cluster: Still creating... [35m40s elapsed] azureopenshift_redhatopenshift_cluster.cluster: Creation complete after 35m48s [id=/subscriptions/e7f88b1a-04fc-4d00-ace9-eec077a5d6af/resourceGroups/my-tf-cluster-rg/providers/Microsoft.RedHatOpenShift/openShiftClusters/my-tf-cluster] Save, display the ARO credentials, and login\naz aro list --query \\ \"[?name=='${CLUSTER}'].{Name:name,Console:consoleProfile.url,API:apiserverProfile.url, ResourceGroup:resourceGroup,Location:location}\" \\ -o tsv | read -r NAME CONSOLE API RESOURCEGROUP LOCATION az aro list-credentials -n $NAME -g $RESOURCEGROUP \\ -o tsv | read -r OCP_PASS OCP_USER oc login ${API} --username ${OCP_USER} --password ${OCP_PASS} echo \"$ oc login ${API} --username ${OCP_USER} --password ${OCP_PASS}\" echo \"Login to ${CONSOLE} as ${OCP_USER} with password ${OCP_PASS}\" Now would be a good time to use the output of this command to log into the OCP Console, you can always run echo \"Login to ${CONSOLE} as ${OCP_USER} with password ${OCP_PASS}\" at any time to remind yourself of the URL and credentials.\nConfigure additional Azure resources These steps create the Storage Account (and two storage containers) in the same Resource Group as the ARO cluster to make cleanup easier. You may want to change it, especially if you plan to host metrics and logs for multiple clusters in the one Storage Account.\nCreate Azure Storage Account and Storage Containers\n# Create Storage Account az storage account create \\ --name $AZR_STORAGE_ACCOUNT_NAME \\ --resource-group $RESOURCEGROUP \\ --location $LOCATION \\ --sku Standard_RAGRS \\ --kind StorageV2 # Fetch the Azure storage key AZR_STORAGE_KEY=$(az storage account keys list -g \"${RESOURCEGROUP}\" \\ -n \"${AZR_STORAGE_ACCOUNT_NAME}\" --query \"[0].value\" -o tsv) # Create Azure Storage Containers az storage container create --name \"${CLUSTER}-metrics\" \\ --account-name \"${AZR_STORAGE_ACCOUNT_NAME}\" \\ --account-key \"${AZR_STORAGE_KEY}\" az storage container create --name \"${CLUSTER}-logs\" \\ --account-name \"${AZR_STORAGE_ACCOUNT_NAME}\" \\ --account-key \"${AZR_STORAGE_KEY}\" Configure MOBB Helm Repository Helm charts do a lot of the heavy lifting for us, and reduce the need for us to copy/paste a pile of YAML. The Managed OpenShift Black Belt team maintain these charts here.\nAdd the MOBB chart repository to your Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your Helm repositories\nhelm repo update Create a namespace to use\noc new-project \"${NAMESPACE}\" Update the Pull Secret and enable OperatorHub This is required to provide credentials to the cluster to pull various Red Hat images in order to enable and configure the Operator Hub.\nDownload a Pull secret from Red Hat Cloud Console and save it in ${SCRATCHDIR}/pullsecret.txt\nUpdate the cluster’s pull secret using the mobb/aro-pull-secret Helm Chart\n# Annotate resources for Helm oc -n openshift-config annotate secret \\ pull-secret meta.helm.sh/release-name=pull-secret oc -n openshift-config annotate secret \\ pull-secret meta.helm.sh/release-namespace=openshift-config oc -n openshift-config label secret \\ pull-secret app.kubernetes.io/managed-by=Helm # Update pull secret (change path needed) cat \u003c\u003c EOF \u003e \"${WORKDIR}/pullsecret.yaml\" pullSecret: | $(\u003c \"${WORKDIR}/pull-secret.txt\") EOF helm upgrade --install pull-secret mobb/aro-pull-secret \\ -n openshift-config --values \"${WORKDIR}/pullsecret.yaml\" # Enable Operator Hub oc patch configs.samples.operator.openshift.io cluster --type=merge \\ -p='{\"spec\":{\"managementState\":\"Managed\"}}' oc patch operatorhub cluster --type=merge \\ -p='{\"spec\":{\"sources\":[ {\"name\":\"redhat-operators\",\"disabled\":false}, {\"name\":\"certified-operators\",\"disabled\":false}, {\"name\":\"community-operators\",\"disabled\":false}, {\"name\":\"redhat-marketplace\",\"disabled\":false} ]}}' Wait for OperatorHub pods to be ready\nwatch oc -n openshift-marketplace get pods NAME READY STATUS RESTARTS AGE certified-operators-xm674 1/1 Running 0 117s community-operators-c5pcq 1/1 Running 0 117s marketplace-operator-7696c9454c-wgtzp 1/1 Running 1 (30m ago) 47m redhat-marketplace-sgnsg 1/1 Running 0 117s redhat-operators-pdbg8 1/1 Running 0 117s Configure Metrics Federation to Azure Blob Storage Next we can configure Metrics Federation to Azure Blob Storage. This is done by deploying the Grafana Operator (to install Grafana to view the metrics later) and the Resource Locker Operator (to configure the User Workload Metrics) and then the mobb/aro-thanos-af Helm Chart to Deploy and Configure Thanos and Grafana Agent to store and retrieve the metrics in Azure Blob.\nGrafana Operator Deploy the Grafana Operator\n# Create a file containing the Grafana operator mkdir -p $WORKDIR/metrics cat \u003c\u003cEOF \u003e $WORKDIR/metrics/grafana-operator.yaml subscriptions: - name: grafana-operator channel: v4 installPlanApproval: Automatic source: community-operators sourceNamespace: openshift-marketplace startingCSV: grafana-operator.v4.7.0 operatorGroups: - name: ${NAMESPACE} targetNamespace: ~ EOF # Deploy the Grafana Operator using Helm helm upgrade -n \"${NAMESPACE}\" clf-operators \\ mobb/operatorhub --install \\ --values \"${WORKDIR}/metrics/grafana-operator.yaml\" # Wait for the Grafana Operator to be installed while ! oc get grafana; do sleep 5; echo -n .; done After a few minutes you should see the following\nerror: the server doesn't have a resource type \"grafana\" error: the server doesn't have a resource type \"grafana\" No resources found in mobb-aro-obs namespace. Resource Locker Operator Deploy the Resource Locker Operator\n# Create the namespace `resource-locker-operator` oc create namespace resource-locker-operator # Create a file containing the Grafana operator cat \u003c\u003cEOF \u003e $WORKDIR/resource-locker-operator.yaml subscriptions: - name: resource-locker-operator channel: alpha installPlanApproval: Automatic source: community-operators sourceNamespace: openshift-marketplace namespace: resource-locker-operator operatorGroups: - name: resource-locker namespace: resource-locker-operator targetNamespace: all EOF # Deploy the Resource Locker Operator using Helm helm upgrade -n resource-locker-operator resource-locker-operator \\ mobb/operatorhub --install \\ --values \"${WORKDIR}\"/resource-locker-operator.yaml # Wait for the Operators to be installed while ! oc get resourcelocker; do sleep 5; echo -n .; done After a few minutes you should see the following\nerror: the server doesn't have a resource type \"resourcelocker\" error: the server doesn't have a resource type \"resourcelocker\" No resources found in mobb-aro-obs namespace. Configure Metrics Federation Deploy mobb/aro-thanos-af Helm Chart to configure metrics federation\nhelm upgrade -n \"${NAMESPACE}\" aro-thanos-af \\ --install mobb/aro-thanos-af --version 0.4.1 \\ --set \"aro.storageAccount=${AZR_STORAGE_ACCOUNT_NAME}\" \\ --set \"aro.storageAccountKey=${AZR_STORAGE_KEY}\" \\ --set \"aro.storageContainer=${CLUSTER}-metrics\" \\ --set \"enableUserWorkloadMetrics=true\" ## Configure Logs Federation to Azure Blob Storage Next we need to deploy the Cluster Logging and Loki Operators so that we can use the `mobb/aro-clf-blob` Helm Chart to deploy and configure Cluster Log Forwarding and the Loki Stack to store metrics in Azure Blob. ### Deploy Operators 1. Deploy the cluster logging and loki operators ```bash # Create nanespaces oc create ns openshift-logging oc create ns openshift-operators-redhat # Configure and deploy operators mkdir -p \"${WORKDIR}/logs\" cat \u003c\u003c EOF \u003e \"${WORKDIR}/logs/log-operators.yaml\" subscriptions: - name: cluster-logging channel: stable installPlanApproval: Automatic source: redhat-operators sourceNamespace: openshift-marketplace namespace: openshift-logging startingCSV: cluster-logging.5.5.2 - name: loki-operator channel: stable installPlanApproval: Automatic source: redhat-operators sourceNamespace: openshift-marketplace namespace: openshift-operators-redhat startingCSV: loki-operator.5.5.2 operatorGroups: - name: openshift-logging namespace: openshift-logging targetNamespace: openshift-logging - name: openshift-operators-redhat namespace: openshift-operators-redhat targetNamespace: all EOF # Deploy the OpenShift Loki Operator and the Red Hat OpenShift Logging Operator helm upgrade -n $NAMESPACE clf-operators \\ mobb/operatorhub --install \\ --values \"${WORKDIR}/logs/log-operators.yaml\" # Wait for the Operators to be installed while ! oc get clusterlogging; do sleep 5; echo -n .; done while ! oc get lokistack; do sleep 5; echo -n .; done Deploy and Configure Cluster Logging and Loki Configure the loki stack to log to Azure Blob\nNote: Only Infrastructure and Application logs are configured to forward by default to reduce storage and traffic. You can add the argument --set clf.audit=true to also forward debug logs.\nhelm upgrade -n \"${NAMESPACE}\" aro-clf-blob \\ --install mobb/aro-clf-blob --version 0.1.1 \\ --set \"azure.storageAccount=${AZR_STORAGE_ACCOUNT_NAME}\" \\ --set \"azure.storageAccountKey=${AZR_STORAGE_KEY}\" \\ --set \"azure.storageContainer=${CLUSTER}-logs\" Wait for the logging stack to come online\nwatch oc -n openshift-logging get pods NAME READY STATUS RESTARTS AGE cluster-logging-operator-8469d5479f-kzh4j 1/1 Running 0 2m10s collector-gbqpr 2/2 Running 0 61s collector-j7f4j 2/2 Running 0 40s collector-ldj2k 2/2 Running 0 58s collector-pc82l 2/2 Running 0 56s collector-qrzlb 2/2 Running 0 58s collector-vsj7z 2/2 Running 0 56s logging-loki-compactor-0 1/1 Running 0 89s logging-loki-distributor-565c84c54f-4f24j 1/1 Running 0 89s logging-loki-gateway-69d68bc47f-rfp8t 2/2 Running 0 88s logging-loki-index-gateway-0 1/1 Running 0 88s logging-loki-ingester-0 0/1 Running 0 89s logging-loki-querier-96c699b7d-fjdrr 1/1 Running 0 89s logging-loki-query-frontend-796d85bf5b-cb4dh 1/1 Running 0 88s logging-view-plugin-98cf668b-dbdkd 1/1 Running 0 107s Sometimes the log collector needs to be restarted for logs to flow correctly into Loki. Wait a few minutes then run the following\noc -n openshift-logging rollout restart daemonset collector Validate Metrics and Logs Now that the Metrics and Log forwarding is set up we can view them in Grafana.\nFetch the Route for Grafana\noc -n \"${NAMESPACE}\" get route grafana-route Browse to the provided route address and login using your OpenShift credentials (username kubeadmin, password echo $OCP_PASS).\nView an existing dashboard such as mobb-aro-obs -\u003e Node Exporter -\u003e USE Method -\u003e Cluster.\nClick the Explore (compass) Icon in the left hand menu, select “Loki (Application)” in the dropdown and search for {kubernetes_namespace_name=\"mobb-aro-obs\"}\nDebugging loki If you don’t see logs in Grafana you can validate that Loki is correctly storing them by querying it directly like so.\nPort forward to the Loki Service\noc port-forward -n openshift-logging svc/logging-loki-gateway-http 8080:8080 Make sure you can curl the Loki service and get a list of labels\nYou can get the bearer token from the login command screen in the OCP Dashboard\ncurl -k -H \"Authorization: Bearer \u003cBEARER TOKEN\u003e\" \\ 'https://localhost:8080/api/logs/v1/infrastructure/loki/api/v1/labels' You can also use the Loki CLI\nlogcli --bearer-token=\"\u003cBEARER TOKEN\u003e\" --tls-skip-verify --addr https://localhost: 8080/api/logs/v1/infrastructure/ labels Cleanup Assuming you didn’t deviate from the guide then you created everything in the Resource Group of the ARO cluster and you can simply destroy our Terraform stack and everything will be cleaned up.\nDelete the ARO cluster\ncd \"${WORKDIR}/terraform-aro\" make delete ","description":"","tags":["ARO","Azure"],"title":"Azure Red Hat Openshift - Shippings logs and metrics to Azure Blob storage","uri":"/docs/aro/shipping-logs-and-metrics-to-azure-blob/"},{"content":"If you prefer a more visual medium, you can watch this video on YouTube.\nThis short video talks about how the STS OIDC flow work in ROSA (Red Hat OpenShift Service on AWS).\n","description":"","tags":["AWS","ROSA","STS"],"title":"STS OIDC in ROSA : How it works!","uri":"/docs/rosa/sts-oidc-flow/"},{"content":"Tyler Stacey\nThe Security Reference Architecture for ROSA is a set of guidelines for deploying Red Hat OpenShift on AWS (ROSA) clusters to support high-security production workloads that align with Red Hat and AWS best practices.\nThis overall architectural guidance compliments detailed, specific recommendations for AWS services and Red Hat OpenShift Container Platform.\nThe Security Reference Architecture (SRA) for ROSA is a living document and is updated periodically based on new feature releases, customer feedback and evolving security best practices.\nThis document is divided into the following sections:\nROSA Day 1 Configuration ROSA Day 2 Security and Operations ROSA Day 1 Configuration ROSA Day 1 configurations are applied to the cluster at the time it is created; they cannot be modified after the cluster has been deployed.\nAWS PrivateLink Networking ROSA provides 3 network deployment patterns: public, private and PrivateLink. Choosing the PrivateLink option provides the most secure configuration and is recommended for customers with sensitive workloads or strict compliance requirements. The PrivateLink option uses AWS PrivateLink to allow Red Hat Site Reliability Engineering (SRE) teams to manage the cluster using a private subnet connected to the cluster’s PrivateLink endpoint in an existing VPC.\nWhen using the PrivateLink model, a VPC with Private Subnets must exist in the AWS account where ROSA will be deployed. The subnets are provided to the installer via CLI flags.\nDetails on the PrivateLink Architecture can be found in the Red Hat and AWS documentation:\nROSA PrivateLink Architecture ROSA PrivateLink Prerequisites Firewall Egress Requirements Deploy a VPC and PrivateLink Cluster AWS Security Token Service (STS) Mode There are two supported methods for providing AWS permissions to ROSA:\nUsing static IAM user credentials with AdministratorAccess policy - “ROSA with IAM Users” (not recommended) Using AWS Security Token Service (STS) with short-lived, dynamic tokens (preferred) - “ROSA with STS” The STS method uses least-privilege predefined roles and policies to grant ROSA minimal permissions in the AWS account for the service to operate and is the recommended option.\nAs stated in the AWS documentation AWS STS “enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users you authenticate (federated users)”. In this case, AWS STS can be used to grant the ROSA service, limited, short-term access, to resources in your AWS account. After these credentials expire (typically an hour after being requested), they are no longer recognized by AWS and they no longer have any kind of account access from API requests made with them.\nDetails on ROSA with STS can be found in Red Hat documentation and blogs:\nROSA with STS Explained AWS prerequisites for ROSA with STS IAM Resources for Clusters that Use STS Customer-Supplied KMS Key By default, ROSA encrypts all Elastic Block Store (EBS) volumes used for node storage and Persistent Volumes (PVs) with an AWS-managed Key Management Service (KMS) key.\nUsing a Customer Managed KMS key allows you to have full control over the KMS key including key policies, key rotation and deletion.\nTo configure a cluster with a custom KMS Key, consider the following references:\nROSA STS Customizations Deploy ROSA with a Custom KMS Key Multi-Availability Zone ROSA clusters that will be used for production workloads should be deployed across multiple availability zones. In this configuration, control plane nodes are distributed across availability zones and at least one worker node is required in each availability zone.\nThis provides the highest level of fault tolerance and protects against the loss of a single availability zone in an AWS region.\nDeploy the Day 1 ROSA SRA via ROSA CLI The Day 1 ROSA SRA can be deployed quickly using the AWS CLI and the ROSA CLI. To deploy the cluster, the following prerequisites must be met:\nAWS Account: Access to an AWS account with sufficient permissions to deploy a ROSA cluster. If using AWS Organizations and Service Control Policies (SCPs), the SCPs must not be more restrictive than the minimum permissions required to operate the service. Sufficient quota to support the cluster deployment. Networking: An AWS VPC, with 3 private subnets across 3 availability zones and outbound internet access. Make note of the AWS VPC Subnet IDs as they will be needed for the installer. Tooling: AWS CLI ROSA CLI v1.2.6 Prepare the ROSA Workload Account Log in to the AWS account with a user that has been assigned AdministratorAccess and run the following command using the aws CLI:\nexport AWS_REGION=\"ca-central-1\" aws iam create-service-linked-role --aws-service-name \"elasticloadbalancing.amazonaws.com\" Create the required ROSA Account Roles Creation of the account roles is a one-time activity, create them with the following command:\nrosa create account-roles --mode auto -y Create the Required KMS Key and Initial Policy The custom KMS key is used to encrypt EC2 EBS node volumes and the EBS volumes that are created by the default StorageClass on OpenShift.\nCreate a new Symmetric KMS Key for EBS Encryption:\nKMS_ARN=$(aws kms create-key --region $AWS_REGION --description 'rosa-ebs-key' --query KeyMetadata.Arn --output text) Generate the necessary key policy to allow the ROSA STS roles to access the key. Use the below command to populate a sample policy, or create your own.\nAWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text); cat \u003c\u003c EOF \u003e rosa-key-policy.json { \"Version\": \"2012-10-17\", \"Id\": \"rosa-key-policy-1\", \"Statement\": [ { \"Sid\": \"Enable IAM User Permissions\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::${AWS_ACCOUNT}:root\" }, \"Action\": \"kms:*\", \"Resource\": \"*\" }, { \"Sid\": \"Allow ROSA use of the key\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-ControlPlane-Role\" ] }, \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": \"*\" }, { \"Sid\": \"Allow attachment of persistent resources\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-ControlPlane-Role\" ] }, \"Action\": [ \"kms:CreateGrant\", \"kms:ListGrants\", \"kms:RevokeGrant\" ], \"Resource\": \"*\", \"Condition\": { \"Bool\": { \"kms:GrantIsForAWSResource\": \"true\" } } } ] } EOF Apply the newly generated key policy to the custom KMS key.\naws kms put-key-policy --key-id $KMS_ARN \\ --policy file://rosa-key-policy.json \\ --policy-name default Deploy a multi-AZ, single subnet, PrivateLink, STS ROSA cluster To deploy the cluster, you must gather the following info:\n--subnet-ids: AWS subnet IDs that the cluster will be deployed in --machine-cidr: The VPC CIDR Deploy the cluster with the following command:\nROSA_CLUSTER_NAME=rosa-ct1 rosa create cluster --cluster-name $ROSA_CLUSTER_NAME --sts --private-link \\ --region ca-central-1 --version 4.11.4 \\ --machine-cidr 10.0.0.0/20 \\ --subnet-ids subnet-058aa558a63da3d51,subnet-058aa558a63da3d52,subnet-058aa558a63da3d53 \\ --enable-customer-managed-key --kms-key-arn $KMS_ARN -y --mode auto To complete the KMS key policy, you must retrieve the Cluster CSI and Machine API operator role names:\nrosa describe cluster -c $ROSA_CLUSTER_NAME The operator role names will be similar to:\narn:aws:iam::${AWS_ACCOUNT}:role/\u003cCLUSTERNAME\u003e-\u003cIDENTIFIER\u003e-openshift-cluster-csi-drivers-ebs-cloud-credenti arn:aws:iam::${AWS_ACCOUNT}:role/\u003cCLUSTERNAME\u003e-\u003cIDENTIFIER\u003e-openshift-machine-api-aws-cloud-credentials Replace the role names in the following script with your EXACT Operator Role names:\nAWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text); cat \u003c\u003c EOF \u003e rosa-key-policy.json { \"Version\": \"2012-10-17\", \"Id\": \"rosa-key-policy-1\", \"Statement\": [ { \"Sid\": \"Enable IAM User Permissions\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::${AWS_ACCOUNT}:root\" }, \"Action\": \"kms:*\", \"Resource\": \"*\" }, { \"Sid\": \"Allow ROSA use of the key\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/\u003cCLUSTERNAME\u003e-\u003cIDENTIFIER\u003e-openshift-cluster-csi-drivers-ebs-cloud-credent\", \"arn:aws:iam::${AWS_ACCOUNT}:role/\u003cCLUSTERNAME\u003e-\u003cIDENTIFIER\u003e-openshift-machine-api-aws-cloud-credentials\" ] }, \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": \"*\" }, { \"Sid\": \"Allow attachment of persistent resources\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-ControlPlane-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/\u003cCLUSTERNAME\u003e-\u003cIDENTIFIER\u003e-openshift-cluster-csi-drivers-ebs-cloud-credent\", \"arn:aws:iam::${AWS_ACCOUNT}:role/\u003cCLUSTERNAME\u003e-\u003cIDENTIFIER\u003e-openshift-machine-api-aws-cloud-credentials\" ] }, \"Action\": [ \"kms:CreateGrant\", \"kms:ListGrants\", \"kms:RevokeGrant\" ], \"Resource\": \"*\", \"Condition\": { \"Bool\": { \"kms:GrantIsForAWSResource\": \"true\" } } } ] } EOF Apply the newly updated key policy to the custom KMS key.\naws kms put-key-policy --key-id $KMS_ARN \\ --policy file://rosa-key-policy.json \\ --policy-name default After creating the operator roles, create the required OIDC provider:\nrosa create oidc-provider --mode auto --cluster $ROSA_CLUSTER_NAME Wait for the cluster deployment to finish.\nROSA Day 2 Security and Operations This section of the SRA describes tasks that are completed once the cluster has been deployed. These configurations enhance the security of the cluster and are often requirements for customers operating in regulated environments.\nConfigure an Identity Provider ROSA provides an easy way to access clusters immediately after deployment through the creation of a cluster-admin user through the ROSA CLI. This method creates an HTPASSWORD identity provider on the cluster. This is good if you need quick access to the cluster, but should not be used for clusters that will host any workloads.\nThe recommended approach is to use a formal identity provider (IDP) to access the cluster (and then grant that user admin privileges, if desired).\nROSA supports several commercially available IDPs and common protocols. The full listing can be found in the ROSA documentation:\nhttps://docs.openshift.com/rosa/rosa_install_access_delete_clusters/rosa-sts-config-identity-providers.html#understanding-idp-supported_rosa-sts-config-identity-providers Some examples of how to configure an IDP can be found on the mobb.ninja website:\nConfigure Azure AD as an identity provider for ROSA/OSD Configure GitLab as an identity provider for ROSA/OSD Configure Azure AD as an identity provider for ROSA with group claims Configure CloudWatch Log Forwarding ROSA does not provide persistent logging by default, but it can be enabled through the cluster-logging operator from the OpenShift Marketplace. This add-on service offers an optional application log-forwarding solution based on AWS CloudWatch. This logging solution can be installed after the ROSA cluster is provisioned.\nTo capture all logging events in AWS CloudWatch, all three log types should be enabled:\nApplications logs: Permits the Operator to collect application logs, which includes everything that is not deployed in the openshift-, kube-, and default namespaces. Infrastructure logs: Permits the Operator to collect logs from OpenShift Container Platform, Kubernetes, and some nodes. Audit logs: Permits the Operator to collect node logs related to security audits. By default, Red Hat stores audit logs outside the cluster through a separate mechanism that does not rely on the Cluster Logging Operator. For more information about default audit logging, see the ROSA Service Definition. After the operator has been enabled the logs can be viewed in the AWS Console, and persistently stored based on the CloudWatch configuration of the AWS Account.\nThe cluster-logging operator has the following limits when configured for CloudWatch log forwarding:\nMessage Size (bytes) Maximum logging rate (messages/second/node) 512 1,000 1,024 650 2,048 450 Details on this configuration can be found at the following links:\nConfiguring the Cluster Log Forwarder for CloudWatch Logs and STS Viewing cluster logs in the AWS Console Configure Custom Ingress TLS Profile By default, ROSA supports multiple versions of TLS on the Ingress COntrollers used for applications to support the broadest set of clients and libraries. To support specific versions of TLS, the tlsSecurityProfile value on cluster ingress controllers can be modified.\nReview the OpenShift Documentation that explains the options for the tlsSecurityProfile to determine which profile meets your organization’s needs. By default, ingress controllers are configured to use the Intermediate profile, which corresponds to the Intermediate Mozilla profile:\nOpenShift documentation on tlsSecurityProfile Intermediate Mozilla Profile The tlsSecurityProfile can be modified by following these instructions:\nConfigure ROSA/OSD to use custom TLS ciphers on the ingress controllers Compliance Operator The Compliance Operator lets ROSA administrators describe the required compliance state of a cluster and provides them with an overview of gaps and ways to remediate them. The Compliance Operator assesses compliance of both the Kubernetes API resources of ROSA, as well as the nodes running the cluster. The Compliance Operator uses OpenSCAP, a NIST-certified tool, to scan and enforce security policies provided by the content.\nThere are several profiles available as part of the Compliance Operator installation. These profiles represent different compliance benchmarks. Each profile has the product name that it applies to added as a prefix to the profile’s name. ocp4-e8 applies the Essential 8 benchmark to the OpenShift Container Platform product, while rhcos4-e8 applies the Essential 8 benchmark to the Red Hat Enterprise Linux CoreOS (RHCOS) product.\nImportant note: The compliance benchmarks are continuously updated and maintained by Red Hat based on each control profile. ROSA-specific benchmarks are under development to account for the managed service components.\nTo understand and install the compliance operator, read the Red Hat documentation:\nInstalling the Compliance Operator Understanding the Compliance Operator Supported compliance profiles OpenShift Service Mesh Red Hat OpenShift Service Mesh addresses a variety of problems in a microservice architecture by creating a centralized point of control in an application. It adds a transparent layer on existing distributed applications without requiring any changes to the application code.\nRed Hat OpenShift Service Mesh provides a number of key capabilities uniformly across a network of services:\nTraffic Management - Control the flow of traffic and API calls between services, make calls more reliable, and make the network more robust in the face of adverse conditions. Service Identity and Security - Provide services in the mesh with a verifiable identity and provide the ability to protect service traffic as it flows over networks of varying degrees of trustworthiness. Policy Enforcement - Apply an organizational policy to the interaction between services, ensure access policies are enforced and resources are fairly distributed among consumers. Policy changes are made by configuring the mesh, not by changing application code. Telemetry - Gain an understanding of the dependencies between services and the nature and flow of traffic between them, providing the ability to quickly identify issues. To learn more about OpenSHift Service Mesh and to install the Service Mesh, read the OpenShift documentation:\nUnderstanding Service Mesh Installing the Service Mesh Operator Adding workloads to the Service Mesh Service Mesh Security Backup and Restore / Disaster Recovery An important part of any platform used to host business and user workloads is data protection. Data protection may include operations including on-demand backup, scheduled backup and restore. These operations allow the objects within a cluster to be backed up to a storage provider, either locally or on a public cloud and restore that cluster from the backup in the event of a failure or scheduled maintenance.\nAs part of the Shared Responsibility Model for ROSA, consumers of the service are responsible for backing up cluster and application data when the STS option is used. To implement a backup and disaster recovery solution, administrators can use OpenShift APIs for Data Protection (OADP). OADP is an operator that Red Hat has created to create backup and restore APIs in the OpenShift cluster. OADP provides the following APIs:\nBackup Restore Schedule BackupStorageLocation VolumeSnapshotLocation You can learn how to install and use OADP from the following resources:\nOADP features and plug-ins Deploying OpenShift Advanced Data Protection on a ROSA cluster Configure AWS WAF and CloudFront for Application Ingress ROSA does not provide advanced firewall or DDoS protection by default, however, this can easily be achieved by combining three AWS services to protect the cluster and applications:\nAWS WAF is a web application firewall that helps protect web applications from attacks by allowing you to configure rules that allow, block, or monitor (count) web requests based on conditions that you define. These conditions include IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and cross-site scripting. Amazon CloudFront is a web service that gives businesses and web application developers an easy and cost effective way to distribute content with low latency and high data transfer speeds. AWS Shield is a managed service that provides protection against Distributed Denial of Service (DDoS) attacks for applications running on AWS. To learn more about these services and how to configure them for ROSA, read the documentation below:\nAWS WAF FAQ Amazon CloudFront FAQ AWS Shield FAQ Using CloudFront + WAF on ROSA Using ALB + WAF on ROSA Use and Store Secrets Securely in AWS Kubernetes Secrets are insecure by default, this is described in the Kubernetes documentation:\nKubernetes Secrets are, by default, stored unencrypted in the API server’s underlying data store (etcd). This design is not unique to ROSA and affects all Kubernetes distributions. Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd. Additionally, anyone who is authorized to create a Pod in a namespace can use that access to read any Secret in that namespace; this includes indirect access such as the ability to create a Deployment.\nCustomers looking for secure ways to manage application secrets often chose to use a third-party tool to manage secrets due to this behavior.\nThe AWS Secrets and Configuration Provider (ASCP) provides a way to expose AWS Secrets as Kubernetes storage volumes. With the ASCP, you can store and manage your secrets in AWS Secrets Manager and then retrieve them through your workloads running on ROSA.\nThis is made even easier / more secure through the use of AWS STS and Kubernetes PodIdentity.\nTo use the AWS Secrets Manager CSI with ROSA and STS, follow this guide:\nUsing AWS Secrets Manager CSI on Red Hat OpenShift on AWS with STS Provide External Persistent Storage to Applications on ROSA ROSA supports both Amazon Elastic Block Storage (EBS) and Elastic File Storage (EFS) for persistent application data.\nWhen applications require ReadWriteMany capabilities, or when multiple applications must read the same data, EFS should be used.\nWith the release of OpenShift 4.10 the EFS CSI Driver is now GA and available.\nTo learn more, or to install the EFS CSI driver, review the following documentation:\nPersistent Storage using EFS Persistent Storage using EBS Enabling the AWS EFS CSI Driver Operator on ROSA ","description":"","tags":["AWS","ROSA"],"title":"Security Reference Architecture for ROSA","uri":"/docs/rosa/security-ra/"},{"content":"Integrating Red Hat OpenShift Service on AWS (ROSA) and AWS Secure Environment Accelerator (ASEA) Landing Zones Tyler Stacey\nLast updated 28 Sep 2022\nThe AWS Secure Environment Accelerator (ASEA) is a tool developed and designed by AWS to help deploy and operate a secure landing zone on AWS. ASEA creates core management and operations accounts, configures networking, identity services, cloud security services, and centralized logging and alerting. In this post, you will learn how to deploy Red Hat OpenShift Service on AWS (ROSA) in an ASEA environment.\nThis guide is validated for ASEA v1.5.3 (using the full configuration) and ROSA v4.11.4.\nPrerequisites AWS CLI Rosa CLI v1.2.6 Knowledge of ASEA operations and troubleshooting Pre-deployed ASEA v1.5.3 ASEA Configuration ASEA is delivered with a sample configuration file that deploys an opinionated and prescriptive architecture designed to help meet the security and operational requirements of many AWS customers around the world. The sample deployment configuration of the ASEA provides a prescriptive architecture that helps customers meet NIST 800-53 and/or Canadian Center for Cyber Security (CCCS) Cloud Medium Control Profile:\nTo best support the automated deployment of ROSA clusters some modifications to the default configuration need to be made:\nAddition of a ROSA Workload Organizational Unit A separate ROSA Organizational Unit (OU) is created in AWS Organizations through the ASEA configuration files to support the configuration required by the service. The ROSA OU has the following characteristics:\nDynamic Virtual Private Cloud (VPC) generation: The CCCS Cloud Medium Profile requires centralized ingress and egress of network traffic in the cloud. To support this the ROSA account Virtual Private Clouds (VPCs) must be attached to the transit gateway and have appropriate routes to direct traffic to the deployed firewall solution. A separate VPC is required for ROSA based on the requirement to create a Route53 Private Hosted Zone (PHZ) during the cluster installation. Currently, shared VPCs do not support the creation of a PHZ in an account separate from the VPC. Support for pre-existing Route53 hosted zones is currently in development: https://github.com/openshift-cs/managed-openshift/issues/70\nPrivate Marketplace (PMP) Access to ROSA: Terms and conditions for the use of ROSA are enabled through the Private Marketplace when the ROSA service is enabled on each account. This is a one-time activity per account.\nAccount Structure: We recommend deploying ROSA into dedicated member accounts based on intended usage: Development, Testing, and Production. This allows for tighter access controls and allows for account resources to be dedicated to each type of cluster.\nTo support this configuration, the following code block must be added to the ASEA config file as a new OU:\n\"ROSA\": { \"type\": \"workload\", \"description\": \"The ROSA OU is used to support the configuration required by the service.\", \"scps\": [ \"Guardrails-Part-0\", \"Guardrails-Part-1\", \"Guardrails-Sensitive\" ], \"default-budgets\": { \"name\": \"Default ROSA Budget\", \"period\": \"Monthly\", \"amount\": 2000, \"include\": [ \"Upfront-reservation-fees\", \"Recurring-reservation-charges\", \"Other-subscription-costs\", \"Taxes\", \"Support-charges\", \"Discounts\" ], \"alerts\": [ { \"type\": \"Actual\", \"threshold-percent\": 50, \"emails\": [ \"MYEMAIL+aseabilling@amazon.com\" ] }, { \"type\": \"Actual\", \"threshold-percent\": 75, \"emails\": [ \"MYEMAIL+aseabilling@amazon.com\" ] }, { \"type\": \"Actual\", \"threshold-percent\": 90, \"emails\": [ \"MYEMAIL+aseabilling@amazon.com\" ] }, { \"type\": \"Actual\", \"threshold-percent\": 100, \"emails\": [ \"MYEMAIL+aseabilling@amazon.com\" ] } ] }, \"vpc\": [ { \"deploy\": \"local\", \"name\": \"${CONFIG::OU_NAME}\", \"description\": \"This VPC is deployed locally in each ROSA account and each VPC will be dynamically assigned unique CIDR ranges and connected to the TGW to enable central ingress/egress.\", \"cidr-src\": \"dynamic\", \"cidr\": [ { \"size\": 20, \"pool\": \"main\" } ], \"region\": \"${HOME_REGION}\", \"use-central-endpoints\": true, \"flow-logs\": \"BOTH\", \"dns-resolver-logging\": true, \"subnets\": [ { \"name\": \"TGW\", \"share-to-ou-accounts\": false, \"share-to-specific-accounts\": [], \"definitions\": [ { \"az\": \"a\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 27 } }, { \"az\": \"b\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 27 } }, { \"az\": \"d\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 27 }, \"disabled\": true } ] }, { \"name\": \"Web\", \"share-to-ou-accounts\": false, \"share-to-specific-accounts\": [], \"definitions\": [ { \"az\": \"a\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 24 } }, { \"az\": \"b\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 24 } }, { \"az\": \"d\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 24 }, \"disabled\": true } ] }, { \"name\": \"App\", \"share-to-ou-accounts\": false, \"share-to-specific-accounts\": [], \"definitions\": [ { \"az\": \"a\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 24 } }, { \"az\": \"b\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 24 } }, { \"az\": \"d\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 24 }, \"disabled\": true } ] }, { \"name\": \"Data\", \"share-to-ou-accounts\": false, \"share-to-specific-accounts\": [], \"definitions\": [ { \"az\": \"a\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 24 } }, { \"az\": \"b\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 24 } }, { \"az\": \"d\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 24 }, \"disabled\": true } ], \"nacls\": [ { \"rule\": 100, \"protocol\": -1, \"ports\": -1, \"rule-action\": \"deny\", \"egress\": true, \"cidr-blocks\": [ { \"vpc\": \"${CONFIG::VPC_NAME}\", \"subnet\": [ \"Web\" ] }, { \"vpc\": \"Central\", \"subnet\": [ \"Data\" ] } ] }, { \"rule\": 32000, \"protocol\": -1, \"ports\": -1, \"rule-action\": \"allow\", \"egress\": true, \"cidr-blocks\": [ \"0.0.0.0/0\" ] }, { \"rule\": 100, \"protocol\": -1, \"ports\": -1, \"rule-action\": \"deny\", \"egress\": false, \"cidr-blocks\": [ { \"vpc\": \"${CONFIG::VPC_NAME}\", \"subnet\": [ \"Web\" ] }, { \"vpc\": \"Central\", \"subnet\": [ \"Data\" ] } ] }, { \"rule\": 32000, \"protocol\": -1, \"ports\": -1, \"rule-action\": \"allow\", \"egress\": false, \"cidr-blocks\": [ \"0.0.0.0/0\" ] } ] }, { \"name\": \"Mgmt\", \"share-to-ou-accounts\": false, \"share-to-specific-accounts\": [], \"definitions\": [ { \"az\": \"a\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 26 } }, { \"az\": \"b\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 26 } }, { \"az\": \"d\", \"route-table\": \"${CONFIG::VPC_NAME}VPC_Common\", \"cidr\": { \"pool\": \"main\", \"size\": 26 }, \"disabled\": true } ] } ], \"gateway-endpoints\": [ \"s3\", \"dynamodb\" ], \"route-tables\": [ { \"name\": \"${CONFIG::VPC_NAME}VPC_Common\", \"routes\": [ { \"destination\": \"0.0.0.0/0\", \"target\": \"TGW\" }, { \"destination\": \"s3\", \"target\": \"s3\" }, { \"destination\": \"DynamoDB\", \"target\": \"DynamoDB\" } ] } ], \"security-groups\": [ { \"name\": \"Mgmt\", \"inbound-rules\": [ { \"description\": \"Mgmt RDP/SSH Traffic Inbound\", \"type\": [ \"RDP\", \"SSH\" ], \"source\": \"${RANGE-RESTRICT}\" }, { \"description\": \"Central VPC Traffic Inbound\", \"type\": [ \"ALL\" ], \"source\": [ { \"vpc\": \"Central\", \"subnet\": [ \"Web\", \"App\", \"Mgmt\", \"App2\" ] } ] } ], \"outbound-rules\": [ { \"description\": \"All Outbound\", \"type\": [ \"ALL\" ], \"source\": [ \"0.0.0.0/0\" ] } ] }, { \"name\": \"Web\", \"inbound-rules\": [ { \"description\": \"World Web Traffic Inbound\", \"type\": [ \"HTTP\", \"HTTPS\" ], \"source\": [ \"0.0.0.0/0\" ] }, { \"description\": \"Central VPC Traffic Inbound\", \"type\": [ \"ALL\" ], \"source\": [ { \"vpc\": \"Central\", \"subnet\": [ \"Web\", \"App\", \"Mgmt\", \"App2\" ] } ] }, { \"description\": \"Local Mgmt Traffic Inbound\", \"type\": [ \"ALL\" ], \"source\": [ { \"security-group\": [ \"Mgmt\" ] } ] } ], \"outbound-rules\": [ { \"description\": \"All Outbound\", \"type\": [ \"ALL\" ], \"source\": [ \"0.0.0.0/0\" ] } ] }, { \"name\": \"App\", \"inbound-rules\": [ { \"description\": \"Central VPC Traffic Inbound\", \"type\": [ \"ALL\" ], \"source\": [ { \"vpc\": \"Central\", \"subnet\": [ \"Web\", \"App\", \"Mgmt\", \"App2\" ] } ] }, { \"description\": \"Local Mgmt Traffic Inbound\", \"type\": [ \"ALL\" ], \"source\": [ { \"security-group\": [ \"Mgmt\" ] } ] }, { \"description\": \"Local Web Tier Inbound\", \"type\": [ \"ALL\" ], \"source\": [ { \"security-group\": [ \"Web\" ] } ] }, { \"description\": \"Allow East/West Communication Inbound\", \"type\": [ \"ALL\" ], \"source\": [ { \"security-group\": [ \"App\" ] } ] } ], \"outbound-rules\": [ { \"description\": \"All Outbound\", \"type\": [ \"ALL\" ], \"source\": [ \"0.0.0.0/0\" ] } ] }, { \"name\": \"Data\", \"inbound-rules\": [ { \"description\": \"Central VPC Traffic Inbound\", \"type\": [ \"ALL\" ], \"source\": [ { \"vpc\": \"Central\", \"subnet\": [ \"Web\", \"App\", \"Mgmt\", \"App2\" ] } ] }, { \"description\": \"Local Mgmt Traffic Inbound\", \"type\": [ \"ALL\" ], \"source\": [ { \"security-group\": [ \"Mgmt\" ] } ] }, { \"description\": \"Local App DB Traffic Inbound\", \"type\": [ \"MSSQL\", \"MYSQL/AURORA\", \"REDSHIFT\", \"POSTGRESQL\", \"ORACLE-RDS\" ], \"source\": [ { \"security-group\": [ \"App\" ] } ] }, { \"description\": \"Allow East/West Communication Inbound\", \"type\": [ \"ALL\" ], \"source\": [ { \"security-group\": [ \"Data\" ] } ] } ], \"outbound-rules\": [ { \"description\": \"All Outbound\", \"type\": [ \"ALL\" ], \"source\": [ \"0.0.0.0/0\" ] } ] } ], \"tgw-attach\": { \"associate-to-tgw\": \"Main\", \"account\": \"shared-network\", \"associate-type\": \"ATTACH\", \"tgw-rt-associate\": [ \"shared\" ], \"tgw-rt-propagate\": [ \"core\", \"shared\", \"segregated\" ], \"blackhole-route\": false, \"attach-subnets\": [ \"TGW\" ], \"options\": [ \"DNS-support\" ] } } ], \"iam\": { \"users\": [], \"policies\": [ { \"policy-name\": \"Default-Boundary-Policy\", \"policy\": \"boundary-policy.txt\" } ], \"roles\": [ { \"role\": \"EC2-Default-SSM-AD-Role\", \"type\": \"ec2\", \"ssm-log-archive-write-access\": true, \"policies\": [ \"AmazonSSMManagedInstanceCore\", \"AmazonSSMDirectoryServiceAccess\", \"CloudWatchAgentServerPolicy\" ], \"boundary-policy\": \"Default-Boundary-Policy\" } ] }, \"ssm-automation\": [ { \"account\": \"operations\", \"regions\": [ \"${HOME_REGION}\" ], \"documents\": [ \"SSM-ELB-Enable-Logging\", \"Put-S3-Encryption\", \"Attach-IAM-Instance-Profile\", \"Attach-IAM-Role-Policy\" ] } ], \"aws-config\": [ { \"excl-regions\": [ \"ap-northeast-3\" ], \"rules\": [ \"EC2-INSTANCE-PROFILE\", \"EC2-INSTANCE-PROFILE-PERMISSIONS\", \"ELB_LOGGING_ENABLED\", \"S3_BUCKET_SERVER_SIDE_ENCRYPTION_ENABLED\", \"ACM_CERTIFICATE_EXPIRATION_CHECK\", \"ALB_WAF_ENABLED\", \"API_GW_CACHE_ENABLED_AND_ENCRYPTED\", \"CLOUD_TRAIL_ENABLED\", \"CLOUDTRAIL_S3_DATAEVENTS_ENABLED\", \"CLOUDTRAIL_SECURITY_TRAIL_ENABLED\", \"CLOUDWATCH_ALARM_ACTION_CHECK\", \"CW_LOGGROUP_RETENTION_PERIOD_CHECK\", \"DB_INSTANCE_BACKUP_ENABLED\", \"DYNAMODB_IN_BACKUP_PLAN\", \"DYNAMODB_TABLE_ENCRYPTED_KMS\", \"EBS_IN_BACKUP_PLAN\", \"EC2_INSTANCE_DETAILED_MONITORING_ENABLED\", \"EC2_MANAGEDINSTANCE_PATCH_COMPLIANCE_STATUS_CHECK\", \"EC2_VOLUME_INUSE_CHECK\", \"ELASTICACHE_REDIS_CLUSTER_AUTOMATIC_BACKUP_CHECK\", \"ELB_ACM_CERTIFICATE_REQUIRED\", \"ELB_CROSS_ZONE_LOAD_BALANCING_ENABLED\", \"EMR_KERBEROS_ENABLED\", \"GUARDDUTY_NON_ARCHIVED_FINDINGS\", \"IAM_GROUP_HAS_USERS_CHECK\", \"IAM_PASSWORD_POLICY\", \"IAM_USER_GROUP_MEMBERSHIP_CHECK\", \"INCOMING_SSH_DISABLED\", \"INSTANCES_IN_VPC\", \"INTERNET_GATEWAY_AUTHORIZED_VPC_ONLY\", \"RDS_IN_BACKUP_PLAN\", \"REDSHIFT_CLUSTER_CONFIGURATION_CHECK\", \"RESTRICTED_INCOMING_TRAFFIC\", \"S3_BUCKET_POLICY_GRANTEE_CHECK\", \"S3_BUCKET_VERSIONING_ENABLED\", \"SAGEMAKER_ENDPOINT_CONFIGURATION_KMS_KEY_CONFIGURED\", \"SAGEMAKER_NOTEBOOK_INSTANCE_KMS_KEY_CONFIGURED\", \"SECURITYHUB_ENABLED\", \"VPC_SG_OPEN_ONLY_TO_AUTHORIZED_PORTS\", \"WAFV2_LOGGING_ENABLED\" ], \"remediate-regions\": [ \"${HOME_REGION}\" ] } ] } Creation of the ROSA Workload Account Following the Development/Testing/Production account structure of the ASEA requires that separate accounts be created for each environment in the ROSA OU; this allows the administrators to restrict account access for each type of environment. It is recommended that account creation happens just-in-time to reduce costs incurred by each account.\nTo add the required accounts, modify the workload-account-configs section of the ASEA configuration to include the following account:\n\"rosa-dev\": { \"account-name\": \"rosa-dev\", \"email\": \"MYEMAIL+rosadev@amazon.com\", \"ou\": \"ROSA\", \"ou-path\": \"ROSA\", \"src-filename\": \"config.json\" } Enabling ROSA One of the key security features of this architecture is the use of highly restrictive Service Control Policies (SCPs) that allow for fine-grained control of access to AWS services and APIs by users and member accounts. Because ROSA automates tasks typically performed by privileged administrators, ASEA deployed SCPs must be temporarily modified to enable ROSA functionality. This is a one time activity per account where ROSA clusters will be deployed.\nTo support the most restrictive SCPs of ASEA we will consider the following SCPs for the installation:\nASEA-Guardrails-Part0-WkldOUs ASEA-Guardrails-Part1 ASEA-Guardrails-Sensitive To prepare your ASEA environment and enable ROSA to perform the following steps:\nCreate the ROSA-TempAdministrator role with AdministratorAccess in the required account.\nIn config.json, only apply Guardrails-Part-0, Guardrails-Part-1 to the ROSA OU.\nRun the main State Machine for ASEA and wait for successful completion.\nEnable the ROSA service in the AWS Console in the required account using the ROSA-TempAdministrator role.\nIn config.json, re-apply Guardrails-Part-0, Guardrails-Part-1, Guardrails-Sensitive.\nRe-run State Machine and wait for successful completion.\nYou can now delete the ROSA-TempAdministrator role in this account.\nDeploy a ROSA cluster To install ROSA in a high-security environment, the custom KMS, PrivateLink and STS cluster patterns should be followed. For more information about these patterns please see the following information:\nhttps://aws.amazon.com/blogs/containers/red-hat-openshift-service-on-aws-private-clusters-with-aws-privatelink/\nhttps://docs.openshift.com/rosa/rosa_getting_started_sts/rosa-sts-getting-started-workflow.html\nPrepare the ROSA Workload Account Log in to the rosa-dev account with a user that has been assigned AdministratorAccess and run the following command using the aws CLI:\nexport AWS_REGION=\"ca-central-1\" aws iam create-service-linked-role --aws-service-name \"elasticloadbalancing.amazonaws.com\" Ensure the account meets the minimum required service quotas for ROSA: https://docs.openshift.com/rosa/rosa_install_access_delete_clusters/rosa_getting_started_iam/rosa-required-aws-service-quotas.html#rosa-required-aws-service-quotas\nVerify that the following egress firewall requirements are met: https://docs.openshift.com/rosa/rosa_install_access_delete_clusters/rosa_getting_started_iam/rosa-aws-prereqs.html#osd-aws-privatelink-firewall-prerequisites_prerequisites\nCreate the required ROSA Account Roles Creation of the account roles is a one-time activity, create them with the following command:\nrosa create account-roles --mode manual --prefix ROSA Using --mode manual generates the aws CLI commands and JSON files needed to create the account-wide roles and policies. Review the roles and policies and run the provided commands from the working directory.\nCreate the Required KMS Key and Initial Policy The custom KMS key is used to encrypt EC2 EBS node volumes and the EBS volumes that are created by the default StorageClass on OpenShift.\nCreate a new Symmetric KMS Key for EBS Encryption:\nKMS_ARN=$(aws kms create-key --region $AWS_REGION --description 'rosa-ebs-key' --query KeyMetadata.Arn --output text) Generate the necessary key policy to allow the ROSA STS roles to access the key. Use the below command to populate a sample policy, or create your own.\nAWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text); cat \u003c\u003c EOF \u003e rosa-key-policy.json { \"Version\": \"2012-10-17\", \"Id\": \"rosa-key-policy-1\", \"Statement\": [ { \"Sid\": \"Enable IAM User Permissions\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::${AWS_ACCOUNT}:root\" }, \"Action\": \"kms:*\", \"Resource\": \"*\" }, { \"Sid\": \"Allow ROSA use of the key\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-ControlPlane-Role\" ] }, \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": \"*\" }, { \"Sid\": \"Allow attachment of persistent resources\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-ControlPlane-Role\" ] }, \"Action\": [ \"kms:CreateGrant\", \"kms:ListGrants\", \"kms:RevokeGrant\" ], \"Resource\": \"*\", \"Condition\": { \"Bool\": { \"kms:GrantIsForAWSResource\": \"true\" } } } ] } EOF Apply the newly generated key policy to the custom KMS key.\naws kms put-key-policy --key-id $KMS_ARN \\ --policy file://rosa-key-policy.json \\ --policy-name default Deploy a single AZ, single subnet, PrivateLink, STS ROSA cluster This deployment configuration should only be used for development. Production clusters should use 3 Availability Zones (AZs) with the --multi-az flag enabled. This requires additional configuration of ASEA and is outside the scope of this post.\nTo deploy the cluster, you must gather the following info:\n--subnet-ids: AWS subnet IDs that the cluster will be deployed in, the App subnets from the ASEA deployment are recommended. --machine-cidr: The VPC CIDR in the rosa-dev account deployed by the ASEA. Deploy the cluster with the following command:\nROSA_CLUSTER_NAME=rosa-ct1 rosa create cluster --cluster-name $ROSA_CLUSTER_NAME --sts --private-link \\ --role-arn arn:aws:iam::$AWS_ACCOUNT:role/ROSA-Installer-Role \\ --support-role-arn arn:aws:iam::$AWS_ACCOUNT:role/ROSA-Support-Role \\ --controlplane-iam-role arn:aws:iam::$AWS_ACCOUNT:role/ROSA-ControlPlane-Role \\ --worker-iam-role arn:aws:iam::$AWS_ACCOUNT:role/ROSA-Worker-Role \\ --operator-roles-prefix ROSA-$ROSA_CLUSTER_NAME --region ca-central-1 --version 4.11.4 \\ --compute-nodes 2 --compute-machine-type m5.xlarge --machine-cidr 10.0.0.0/20 \\ --service-cidr 172.30.0.0/16 --pod-cidr 10.128.0.0/14 --host-prefix 23 \\ --subnet-ids subnet-058aa558a63da3d51 --etcd-encryption --enable-customer-managed-key \\ --kms-key-arn $KMS_ARN --watch --debug While the cluster is creating, create the operator IAM roles:\nrosa create operator-roles --mode manual --cluster $ROSA_CLUSTER_NAME Review the output IAM Role Policy documents and AWS CLI commands for accuracy and run the provided commands.\nAfter creating the Operator Roles, modify the KMS key policy to allow the CSI driver to function correctly:\nAWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text); cat \u003c\u003c EOF \u003e rosa-key-policy.json { \"Version\": \"2012-10-17\", \"Id\": \"rosa-key-policy-1\", \"Statement\": [ { \"Sid\": \"Enable IAM User Permissions\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::${AWS_ACCOUNT}:root\" }, \"Action\": \"kms:*\", \"Resource\": \"*\" }, { \"Sid\": \"Allow ROSA use of the key\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-ControlPlane-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-${ROSA_CLUSTER_NAME}-openshift-cluster-csi-drivers-ebs-cloud-credent\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-${ROSA_CLUSTER_NAME}-openshift-machine-api-aws-cloud-credentials\" ] }, \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": \"*\" }, { \"Sid\": \"Allow attachment of persistent resources\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-ControlPlane-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-${ROSA_CLUSTER_NAME}-openshift-cluster-csi-drivers-ebs-cloud-credent\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ROSA-${ROSA_CLUSTER_NAME}-openshift-machine-api-aws-cloud-credentials\" ] }, \"Action\": [ \"kms:CreateGrant\", \"kms:ListGrants\", \"kms:RevokeGrant\" ], \"Resource\": \"*\", \"Condition\": { \"Bool\": { \"kms:GrantIsForAWSResource\": \"true\" } } } ] } EOF Apply the newly updated key policy to the custom KMS key.\naws kms put-key-policy --key-id $KMS_ARN \\ --policy file://rosa-key-policy.json \\ --policy-name default After creating the operator roles, create the required OIDC provider:\nrosa create oidc-provider --mode auto --cluster $ROSA_CLUSTER_NAME Wait for the cluster deployment to finish.\nAccess the cluster The ROSA cluster can only be accessed from a machine that has routes to the ROSA VPC. One way to gain this access is to use a jumphost in the management subnet and use an AWS Systems Manager terminal session to access the jumphost securely.\nCreate a temporary Admin user with cluster-admin privileges:\nrosa create admin -c $ROSA_CLUSTER_NAME Run the resulting login statement from the output. It may take up to 15 minutes before authentication is fully synced.\nVerify the default persistent volumes in the cluster.\noc get pv Output:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-00dac374-a45e-43fa-a313-ae0491e8edf1 10Gi RWO Delete Bound openshift-monitoring/alertmanager-data-alertmanager-main-1 gp3-customer-kms 26m pvc-7d211496-4ddf-4200-921c-1404b754afa5 10Gi RWO Delete Bound openshift-monitoring/alertmanager-data-alertmanager-main-0 gp3-customer-kms 26m pvc-b5243cef-ec30-4e5c-a348-aeb8136a908c 100Gi RWO Delete Bound openshift-monitoring/prometheus-data-prometheus-k8s-0 gp3-customer-kms 26m pvc-ec60c1cf-72cf-4ac6-ab12-8e9e5afdc15f 100Gi RWO Delete Bound openshift-monitoring/prometheus-data-prometheus-k8s-1 gp3-customer-kms 26m You should see the StorageClass set to gp3-customer-kms. This is the default StorageClass which is encrypted using the customer-provided key.\n","description":"","tags":["AWS","ROSA"],"title":"Integrating ROSA and ASEA Landing Zones","uri":"/docs/rosa/rosa-asea-landing-zone/"},{"content":"Michael McNeill\n23 September 2022\nThis guide demonstrates how to utilize the OpenID Connect group claim functionality implemented in OpenShift 4.10. This functionality allows an identity provider to provide a user’s group membership for use within OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application, configure the necessary Azure AD groups, and configure Red Hat OpenShift Service on AWS (ROSA) to authenticate and manage authorization using Azure AD.\nThis guide will walk through the following steps:\nRegister a new application in Azure AD for authentication. Configure the application registration in Azure AD to include optional and group claims in tokens. Configure the OpenShift cluster to use Azure AD as the identity provider. Grant additional permissions to individual groups. Before you Begin Create a set of security groups and assign users by following the Microsoft documentation.\nIn addition, if you are using zsh as your shell (which is the default shell on macOS) you may need to run set -k to get the below commands to run without errors. This is because zsh disables comments in interactive shells from being used.\n1. Register a new application in Azure AD for authentication Capture the OAuth callback URL First, construct the cluster’s OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variable specified:\nThe “AAD” directory at the end of the the OAuth callback URL should match the OAuth identity provider name you’ll setup later.\nCLUSTER_NAME=example-cluster # Replace this with the name of your ROSA cluster domain=$(rosa describe cluster -c $CLUSTER_NAME | grep \"DNS\" | grep -oE '\\S+.openshiftapps.com') echo \"OAuth callback URL: https://oauth-openshift.apps.$domain/oauth2callback/AAD\" Register a new application in Azure AD Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to App registrations blade, then click on “New registration” to create a new application.\nProvide a name for the application, for example openshift-auth. Select “Web” from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click “Register” to create the application.\nThen, click on the “Certificates \u0026 secrets” sub-blade and select “New client secret”. Fill in the details request and make note of the generated client secret value, as you’ll use it in a later step. You won’t be able to retrieve it again.\nThen, click on the “Overview” sub-blade and make note of the “Application (client) ID” and “Directory (tenant) ID”. You’ll need those values in a later step as well.\n2. Configure optional claims (for optional and group claims) In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically “email” and “preferred_username”, as well as a group claim when a user logs in. For more information on optional claims in Azure AD, see the Microsoft documentation.\nClick on the “Token configuration” sub-blade and select the “Add optional claim” button.\nSelect ID then check the “email” and “preferred_username” claims and click the “Add” button to configure them for your Azure AD application.\nWhen prompted, follow the prompt to enable the necessary Microsoft Graph permissions.\nNext, select the “Add groups claim” button.\nSelect the “Security groups” option and click the “Add” button to configure group claims for your Azure AD application.\nNote: In this example, we are providing all security groups a user is a member of via the group claim. In a real production environment, we highly recommend _scoping the groups provided by the group claim to only those groups which are applicable to OpenShift.\n3. Configure the OpenShift cluster to use Azure AD as the identity provider Finally, we need to configure OpenShift to use Azure AD as its identity provider. While Red Hat OpenShift Service on AWS (ROSA) offers the ability to configure identity providers via the OpenShift Cluster Manager (OCM), that functionality does not currently support group claims. Instead, we’ll configure the cluster’s OAuth provider to use Azure AD as its identity provider via the rosa CLI. To do so, run the following command, making sure to replace the variable specified:\nCLUSTER_NAME=example-cluster # Replace this with the name of your ROSA cluster IDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID CLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID rosa create idp \\ --cluster ${CLUSTER_NAME} \\ --type openid \\ --name ${IDP_NAME} \\ --client-id ${APP_ID} \\ --client-secret ${CLIENT_SECRET} \\ --issuer-url https://login.microsoftonline.com/${TENANT_ID}/v2.0 \\ --email-claims email \\ --name-claims name \\ --username-claims preferred_username \\ --groups-claims groups 4. Grant additional permissions to individual groups Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. In addition, the cluster OAuth provider will automatically create or update the membership of groups the user is a member of (using the group ID).\nOnce you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view. The cluster OAth provider does not automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes.\nOpenShift includes a significant number of pre-configured roles, including the cluster-admin role that grants full access and control over the cluster. To grant an automatically generated group access to the cluster-admin role, you must create a ClusterRoleBinding to the group ID.\nGROUP_ID=wwwwwwww-wwww-wwww-wwww-wwwwwwwwwwww # Replace with your Azure AD Group ID that you would like to have cluster admin permissions oc create clusterrolebinding cluster-admin-group \\ --clusterrole=cluster-admin \\ --group=$GROUP_ID Now, any user in the specified group will automatically be granted cluster-admin access.\nFor more information on how to use RBAC to define and apply permissions in OpenShift, see the OpenShift documentation.\n","description":"","tags":null,"title":"Configure ROSA to use Azure AD Group Claims","uri":"/docs/idp/group-claims/rosa/"},{"content":"Michael McNeill\n24 August 2022\nThis guide demonstrates how to properly patch the cluster ingress controllers, as well as ingress controllers created by the Custom Domain Operator. This functionality allows customers to modify the tlsSecurityProfile value on cluster ingress controllers. This guide will demonstrate how to apply a custom tlsSecurityProfile, a scoped service account (with the associated role and role binding), and a CronJob that the cipher changes are reapplied with 60 minutes (in the event that an ingress controller is recreated or modified).\nBefore you Begin Review the OpenShift Documentation that explains the options for the tlsSecurityProfile. By default, ingress controllers are configured to use the Intermediate profile, which corresponds to the Intermediate Mozilla profile.\n1. Create a service account for the CronJob to use A service account allows our CronJob to directly access the cluster API, without using a regular user’s credentials. To create a service account, run the following command:\noc create sa cron-ingress-patch-sa -n openshift-ingress-operator 2. Create a role and role binding that allows limited access to patch the ingress controllers Role-based access control (RBAC) is critical to ensuring security inside your cluster. Creating a role allows us to provide scoped access to only the API resources we need within the cluster. To create the role, run the following command:\noc create role cron-ingress-patch-role --verb=get,patch,update --resource=ingresscontroller.operator.openshift.io -n openshift-ingress-operator Once the role has been created, you need to bind the role to the service account using a role binding. To create the role binding, run the following command:\noc create rolebinding cron-ingress-patch-rolebinding --role=cron-ingress-patch-role --serviceaccount=openshift-ingress-operator:cron-ingress-patch-sa -n openshift-ingress-operator 3. Patch the ingress controller Important note: The examples provided below add an additional cipher to the ingress controller’s tlsSecurityProfile to allow IE 11 access from Windows Server 2008 R2. You should modify this command to meet your specific business requirements.\nBefore we create the CronJob, we first want to apply the tlsSecurityProfile configuration to validate our changes. This process depends on if you are using the Custom Domain Operator.\nClusters not using the Custom Domain Operator If you are only using the default ingress controller, and not using the Custom Domain Operator, you will run the following command to patch the ingress controller:\noc patch ingresscontroller/default -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}' This patch will add the TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA cipher which allows access from IE 11 on Windows Server 2008 R2 when using RSA certificates.\nOnce you’ve run the command, you’ll receive a response that looks like this:\ningresscontroller.operator.openshift.io/default patched Clusters using the Custom Domain Operator Customers who are using the Custom Domain Operator will need to loop through each of their ingress controllers to patch each one. To patch all of your cluster’s ingress controllers, run the following command:\nfor ic in $(oc get ingresscontroller -o name -n openshift-ingress-operator); do oc patch ${ic} -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}'; done Once you’ve run the command, you’ll receive a response that looks like this:\ningresscontroller.operator.openshift.io/default patched ingresscontroller.operator.openshift.io/custom1 patched ingresscontroller.operator.openshift.io/custom2 patched 4. Create the CronJob to ensure the TLS configuration is not overwritten Occasionally, the cluster’s ingress controller can get recreated. In these cases, the ingress controller will likely not retain the tlsSecurityProfile changes that we’ve made. To ensure this doesn’t happen, we’ll create a CronJob that goes through and updates the cluster’s ingress controller(s). This process depends on if you are using the Custom Domain Operator.\nClusters not using the Custom Domain Operator If you are not using the Custom Domain Operator, creating the CronJob is as simple as running the following command:\ncat \u003c\u003c EOF | oc apply -f - apiVersion: batch/v1 kind: CronJob metadata: name: tls-patch namespace: openshift-ingress-operator spec: schedule: '@hourly' jobTemplate: spec: template: spec: containers: - name: tls-patch image: registry.redhat.io/openshift4/ose-tools-rhel8:latest args: - /bin/sh - '-c' - oc patch ingresscontroller/default -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}' restartPolicy: Never serviceAccountName: cron-ingress-patch-sa EOF Note, this CronJob will run every hour, and will patch the ingress controller, if necessary. It is important that this CronJob does not run constantly, as it can trigger reconciles that could overload the OpenShift Ingress Operator. Most of the time, the logs of the CronJob pod will look something like this, as it will not be changing anything:\ningresscontroller.operator.openshift.io/default patched (no change) Clusters using the Custom Domain Operator If you are using the Custom Domain Operator the CronJob will need to loop through and patch each ingress controller. To create this CronJob, run the following command:\ncat \u003c\u003c EOF | oc apply -f - apiVersion: batch/v1 kind: CronJob metadata: name: tls-patch namespace: openshift-ingress-operator spec: schedule: '@hourly' jobTemplate: spec: template: spec: containers: - name: tls-patch image: registry.redhat.io/openshift4/ose-tools-rhel8:latest args: - /bin/sh - '-c' - for ic in $(oc get ingresscontroller -o name -n openshift-ingress-operator); do oc patch ${ic} -n openshift-ingress-operator --type=merge -p '{\"spec\":{\"tlsSecurityProfile\":{\"type\":\"Custom\",\"custom\":{\"ciphers\":[\"TLS_AES_128_GCM_SHA256\",\"TLS_AES_256_GCM_SHA384\",\"ECDHE-ECDSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\"ECDHE-ECDSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\"ECDHE-ECDSA-CHACHA20-POLY1305\",\"ECDHE-RSA-CHACHA20-POLY1305\",\"DHE-RSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-GCM-SHA384\",\"TLS_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\"],\"minTLSVersion\":\"VersionTLS12\"}}}}'; done restartPolicy: Never serviceAccountName: cron-ingress-patch-sa EOF Note, this CronJob will run every hour, and will patch the ingress controller, if necessary. It is important that this CronJob does not run constantly, as it can trigger reconciles that could overload the OpenShift Ingress Operator. Most of the time, the logs of the CronJob pod will look something like this, as it will not be changing anything:\ningresscontroller.operator.openshift.io/default patched (no change) ingresscontroller.operator.openshift.io/custom1 patched (no change) ingresscontroller.operator.openshift.io/custom2 patched (no change) ","description":"","tags":["ROSA","AWS","OSD"],"title":"Configure ROSA/OSD to use custom TLS ciphers on the ingress controllers","uri":"/docs/misc/tls-cipher-customization/"},{"content":"","description":"","tags":null,"title":"GPU","uri":"/tags/gpu/"},{"content":"Retrieve the login command If you are not logged in via the CLI, access your cluster via the web console, then click on the dropdown arrow next to your name in the top-right and select Copy Login Command.\nA new tab will open and select the authentication method you are using (in our case it’s github)\nClick Display Token\nCopy the command under where it says “Log in with this token”. Then go to your terminal and paste that command and press enter. You will see a similar confirmation message if you successfully logged in.\noc login --token=RYhFlXXXXXXXXXXXX --server=https://api.osd4-demo.abc1.p1.openshiftapps.com:6443 Logged into \"https://api.osd4-demo.abc1.p1.openshiftapps.com:6443\" as \"openshiftuser\" using the token provided. You don't have any projects. You can try to create a new project, by running oc new-project \u003cprojectname\u003e Create new project Create a new project called “notebook-demo” in your cluster by entering the following command:\noc new-project notebook-demo You should receive the following response\nNow using project \"notebook-demo\" on server \"https://api.aro.openshiftdemo.dev:6443\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app rails-postgresql-example to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname Equivalently you can also create this new project using the web console UI by clicking on “Projects” under “Home” on the left menu, and then click “Create Project” button on the right.\nImporting the Minimal Notebook A pre-built version of the minimal notebook which is based on CentOS, can be found at on quay.io at:\nhttps://quay.io/organization/jupyteronopenshift The name of the latest build version of this image is:\nquay.io/jupyteronopenshift/s2i-minimal-notebook-py36:latest Although this image could be imported into an OpenShift cluster using oc import-image, it is recommended instead that you load it using the supplied image stream definition, using:\noc create -f https://raw.githubusercontent.com/jupyter-on-openshift/jupyter-notebooks/master/image-streams/s2i-minimal-notebook.json This is preferred, as it will create an image stream with tag corresponding to the Python version being used, with the underlying image reference referring to a specific version of the image on quay.io, rather than the latest build. This ensures that the version of the image doesn’t change to a newer version of the image which you haven’t tested.\nOnce the image stream definition is loaded, the project it is loaded into should have the tagged image:\ns2i-minimal-notebook:3.6 Deploying the Minimal Notebook To deploy the minimal notebook image run the following commands:\noc new-app s2i-minimal-notebook:3.6 --name minimal-notebook \\ --env JUPYTER_NOTEBOOK_PASSWORD=mypassword The JUPYTER_NOTEBOOK_PASSWORD environment variable will allow you to access the notebook instance with a known password.\nDeployment should be quick if you build the minimal notebook from source code. If you used the image stream, the first deployment may be slow as the image will need to be pulled down from quay.io. You can monitor progress of the deployment if necessary by running:\noc rollout status dc/minimal-notebook Because the notebook instance is not exposed to the public network by default, you will need to expose it. To do this, and ensure that access is over a secure connection run:\noc create route edge minimal-notebook --service minimal-notebook \\ --insecure-policy Redirect To see the hostname which is assigned to the notebook instance, run:\noc get route/minimal-notebook Access the hostname shown using your browser and enter the password you used above.\nTo delete the notebook instance when done, run:\noc delete all --selector app=minimal-notebook Creating Custom Notebook Images To create custom notebooks images, you can use the s2i-minimal-notebook:3.6 image as an S2I builder. This repository contains two examples for extending the minimal notebook. These can be found in:\nscipy-notebook tensorflow-notebook These are intended to mimic the images of the same name available from the Jupyter project.\nIn the directories you will find a requirements.txt file listing the additional Python packages that need to be installed from PyPi. You will also find a .s2i/bin/assemble script which will be triggered by the S2I build process, and which installs further packages and extensions.\nTo use the S2I build process to create a custom image, you can then run the command:\noc new-build --name custom-notebook \\ --image-stream s2i-minimal-notebook:3.6 \\ --code https://github.com/jupyter-on-openshift/jupyter-notebooks \\ --context-dir scipy-notebook If any build of a custom image fails because the default memory limit on builds in your OpenShift cluster is too small, you can increase the limit by running:\noc patch bc/custom-notebook \\ --patch '{\"spec\":{\"resources\":{\"limits\":{\"memory\":\"1Gi\"}}}}' and start a new build by running:\noc start-build bc/custom-notebook If using the custom notebook image with JupyterHub running in OpenShift, you may also need to set the image lookup policy on the image stream created.\noc set image-lookup is/custom-notebook This is necessary so that the image stream reference in the pod definition created by JupyterHub will be able to resolve the name to that of the image stream.\nFor the scipy-notebook and tensorflow-notebook examples provided, if you wish to use the images, instead of running the above commands, after you have loaded the image stream for, or built the minimal notebook image, you can instead run the commands:\noc create -f https://raw.githubusercontent.com/jupyter-on-openshift/jupyter-notebooks/master/build-configs/s2i-scipy-notebook.json oc create -f https://raw.githubusercontent.com/jupyter-on-openshift/jupyter-notebooks/master/build-configs/s2i-tensorflow-notebook.json When creating a custom notebook image, the directory in the Git repository the S2I build is run against can contain a requirements.txt file listing the Python package to be installed in the custom notebook image. Any other files in the directory will also be copied into the image. When the notebook instance is started from the image, those files will then be present in your workspace.\n","description":"","tags":["GPU","OCP"],"title":"How to deploy Jupyter Notebook","uri":"/docs/misc/jup/buildnotebook/"},{"content":"The Open Data Hub operator is available for deployment in the OpenShift OperatorHub as a Community Operators. You can install it from the OpenShift web console:\nFrom the OpenShift web console, log in as a user with cluster-admin privileges. For a developer installation from try.openshift.com including AWS and CRC, the kubeadmin user will work.\nCreate a new project named ‘jph-demo’ for your installation of Open Data Hub Find Open Data Hub in the OperatorHub catalog.\nSelect the new namespace if not already selected. Under Operators, select OperatorHub for a list of operators available for deployment. Filter for Open Data Hub or look under Big Data for the icon for Open Data Hub. Click the Install button and follow the installation instructions to install the Open Data Hub operator.(optional if operator not installed)\nThe subscription creation view will offer a few options including Update Channel, keep the rolling channel selected.\nTo view the status of the Open Data Hub operator installation, find the Open Data Hub Operator under Operators -\u003e Installed Operators (inside the project you created earlier). Once the STATUS field displays InstallSucceeded, you can proceed to create a new Open Data Hub deployment.\nFind the Open Data Hub Operator under Installed Operators (inside the project you created earlier)\nClick on the Open Data Hub Operator to bring up the details for the version that is currently installed.\nClick Create Instance to create a new deployment.\nSelect the YAML View radio button to be presented with a YAML file to customize your deployment. Most of the components available in ODH have been removed, and only components for JupyterHub are required for this example. apiVersion: kfdef.apps.kubeflow.org/v1 kind: KfDef metadata: creationTimestamp: '2022-06-24T18:55:12Z' finalizers: - kfdef-finalizer.kfdef.apps.kubeflow.org generation: 2 managedFields: - apiVersion: kfdef.apps.kubeflow.org/v1 fieldsType: FieldsV1 fieldsV1: 'f:spec': .: {} 'f:applications': {} 'f:repos': {} manager: Mozilla operation: Update time: '2022-06-24T18:55:12Z' - apiVersion: kfdef.apps.kubeflow.org/v1 fieldsType: FieldsV1 fieldsV1: 'f:metadata': 'f:finalizers': .: {} 'v:\"kfdef-finalizer.kfdef.apps.kubeflow.org\"': {} 'f:status': {} manager: opendatahub-operator operation: Update time: '2022-06-24T18:55:12Z' name: opendatahub namespace: jph-demo resourceVersion: '27393048' uid: f54399a6-faa7-4724-bf3d-be04a63d3120 spec: applications: - kustomizeConfig: repoRef: name: manifests path: odh-common name: odh-common - kustomizeConfig: parameters: - name: s3_endpoint_url value: s3.odh.com repoRef: name: manifests path: jupyterhub/jupyterhub name: jupyterhub - kustomizeConfig: overlays: - additional repoRef: name: manifests path: jupyterhub/notebook-images name: notebook-images repos: - name: kf-manifests uri: \u003e- https://github.com/opendatahub-io/manifests/tarball/v1.4.0-rc.2-openshift - name: manifests uri: 'https://github.com/opendatahub-io/odh-manifests/tarball/v1.2' status: {} Update the spec of the resource to match the above and click Create. If you accepted the default name, this will trigger the creation of an Open Data Hub deployment named opendatahub with JupyterHub.\nVerify the installation by viewing the project workload. JupyterHub and traefik-proxy should be running. Click Routes under Networking and url to launch Jupyterhub is created Open JupyterHub on web browser Configure GPU and start server Check for GPU in notebook Reference: Check the blog on Using the NVIDIA GPU Operator to Run Distributed TensorFlow 2.4 GPU Benchmarks in OpenShift 4\n","description":"","tags":["GPU","OCP"],"title":"Installing the Open Data Hub Operator","uri":"/docs/misc/jup/opendatahub-gpu/"},{"content":"You will need the following prerequistes in order to run a basic Jupyter notebook with GPU on OpenShift\n1. A OpenShift Cluster This will assume you have already provisioned a OpenShift cluster succesfully and are able to use it.\nYou will need to log in as cluster admin to deploy GPU Operator.\n2. OpenShift Command Line Interface Please see the OpenShift Command Line section for more information on installing.\nThe following guides through a step by step procedure in deploying Jupyter Notebook in OpenShift.\n3. Reference images You’ll be doing the majority of the labs using the OpenShift CLI, but you can also accomplish them using the OpenShift web console. You can create a minimal Jupyter notebook image using the Source-to-Image (S2I) build process. The image can be built in OpenShift, separately using the s2i tool, or using a docker build. One can deploy a custom notebook image. The Jupyter Project provides a number of images for notebooks on Docker Hub. These are:\nbase-notebook r-notebook minimal-notebook scipy-notebook tensorflow-notebook datascience-notebook pyspark-notebook all-spark-notebook The GitHub repository used to create these is:\nhttps://github.com/jupyter/docker-stacks Basic Concepts Source-To-Image (S2I) Source-to-Image (S2I) is a toolkit and workflow for building reproducible container images from source code. S2I produces ready-to-run images by injecting source code into a container image and letting the container prepare that source code for execution. By creating self-assembling builder images, you can version and control your build environments exactly like you use container images to version your runtime environments.\nS2I Builds Creating Images How it works Start a container from the builder image with the application source injected into a known directory\nThe container process transforms that source code into the appropriate runnable setup - in this case, it will create an image stream with tag corresponding to the Python version being used, with the underlying image reference referring to a specific version of the image on quay.io, rather than the latest build. This ensures that the version of the image doesn’t change to a newer version of the image which you haven’t tested.\nGoals and benefits 1. Reproducibility Allow build environments to be tightly versioned by encapsulating them within a container image and defining a simple interface (injected source code) for callers. Reproducible builds are a key requirement to enabling security updates and continuous integration in containerized infrastructure, and builder images help ensure repeatability as well as the ability to swap runtimes.\n2. Flexibility Any existing build system that can run on Linux can be run inside of a container, and each individual builder can also be part of a larger pipeline. In addition, the scripts that process the application source code can be injected into the builder image, allowing authors to adapt existing images to enable source handling.\n3. Speed Instead of building multiple layers in a single Dockerfile, S2I encourages authors to represent an application in a single image layer. This saves time during creation and deployment, and allows for better control over the output of the final image.\n4. Security Dockerfiles are run without many of the normal operational controls of containers, usually running as root and having access to the container network. S2I can be used to control what permissions and privileges are available to the builder image since the build is launched in a single container. In concert with platforms like OpenShift, source-to-image can enable admins to tightly control what privileges developers have at build time.\nRoutes An OpenShift Route exposes a service at a host name, like www.example.com, so that external clients can reach it by name. When a Route object is created on OpenShift, it gets picked up by the built-in HAProxy load balancer in order to expose the requested service and make it externally available with the given configuration. You might be familiar with the Kubernetes Ingress object and might already be asking “what’s the difference?”. Red Hat created the concept of Route in order to fill this need and then contributed the design principles behind this to the community; which heavily influenced the Ingress design. Though a Route does have some additional features as can be seen in the chart below.\nNOTE: DNS resolution for a host name is handled separately from routing; your administrator may have configured a cloud domain that will always correctly resolve to the router, or if using an unrelated host name you may need to modify its DNS records independently to resolve to the router.\nAlso of note is that an individual route can override some defaults by providing specific configurations in its annotations. See route specific annotations for more details.\nImageStreams An ImageStream stores a mapping of tags to images, metadata overrides that are applied when images are tagged in a stream, and an optional reference to a Docker image repository on a registry.\nWhat are the benefits? Using an ImageStream makes it easy to change a tag for a container image. Otherwise to change a tag you need to download the whole image, change it locally, then push it all back. Also promoting applications by having to do that to change the tag and then update the deployment object entails many steps. With ImageStreams you upload a container image once and then you manage it’s virtual tags internally in OpenShift. In one project you may use the dev tag and only change reference to it internally, in prod you may use a prod tag and also manage it internally. You don’t really have to deal with the registry!\nYou can also use ImageStreams in conjunction with DeploymentConfigs to set a trigger that will start a deployment as soon as a new image appears or a tag changes its reference.\nSee below for more details:\nImage Streams Blog post OpenShift Docs - Understanding containers, images, and image streams Builds A build is the process of transforming input parameters into a resulting object. Most often, the process is used to transform input parameters or source code into a runnable image. A BuildConfig object is the definition of the entire build process.\nOpenShift Container Platform leverages Kubernetes by creating Docker-formatted containers from build images and pushing them to a container image registry.\nBuild objects share common characteristics: inputs for a build, the need to complete a build process, logging the build process, publishing resources from successful builds, and publishing the final status of the build. Builds take advantage of resource restrictions, specifying limitations on resources such as CPU usage, memory usage, and build or pod execution time.\nSee Understanding image builds for more details.\n","description":"","tags":["GPU","OCP"],"title":"Jupyter Notebooks","uri":"/docs/misc/jup/"},{"content":"","description":"","tags":null,"title":"OCP","uri":"/tags/ocp/"},{"content":"","description":"","tags":null,"title":"OSD","uri":"/tags/osd/"},{"content":"OSD and ROSA supports custom domain operator to serve application custom domain, which provisions openshift ingress controller and cloud load balancers. However, when a route with custom domain is created, both default router and custom domain router serve routes. This article describes how to use route labels to stop default router from serving custom domain routes.\nPrerequisites Rosa or OSD Cluster Custom Domain Deployed Problem Demo Deploy A Custom Domain oc create secret tls example-tls --cert=[cert_file] --key=[key_file] cat \u003c\u003c EOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: example spec: domain: example.com scope: External certificate: name: example-tls namespace: default EOF Create a sample application and Route oc new-app --image=openshift/hello-openshift cat \u003c\u003cEOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: hello-openshift app.kubernetes.io/component: hello-openshift app.kubernetes.io/instance: hello-openshift name: helloworld spec: host: helloworld-openshift.example.com port: targetPort: 8080-tcp tls: termination: edge to: kind: \"\" name: hello-openshift EOF Both default router and custom router serve the routes oc get route -o yaml .... status: ingress: - conditions: - lastTransitionTime: \"2022-06-02T20:30:39Z\" status: \"True\" type: Admitted host: helloworld-openshift.example.com routerCanonicalHostname: router-default.apps.mobb-infra-gcp.e8e4.p2.openshiftapps.com routerName: default wildcardPolicy: None - conditions: - lastTransitionTime: \"2022-06-02T20:30:39Z\" status: \"True\" type: Admitted host: helloworld-openshift.example.com routerCanonicalHostname: router-example.example.mobb-infra-gcp.e8e4.p2.openshiftapps.com routerName: example wildcardPolicy: None End user can access the app from both ingress controllers’ cloud load balancer oc get svc -n openshift-ingress NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE router-default LoadBalancer 172.30.39.254 34.73.154.84 80:32108/TCP,443:30332/TCP 39d router-example LoadBalancer 172.30.209.51 34.138.159.7 80:32477/TCP,443:31383/TCP 9m51s curl -k -H \"Host: helloworld-openshift.example.com\" https://34.73.154.84 Hello OpenShift! shading@shading-mac gcp_domain % curl -k -H \"Host: helloworld-openshift.example.com\" https://34.138.159.7 Hello OpenShift! Stop the default router from serving custom domain routes Delete the route oc delete route helloworld Custom Domain only serve routes with corresponding custom domain label oc patch \\ -n openshift-ingress-operator \\ IngressController/example \\ --type='merge' \\ -p '{\"spec\":{\"routeSelector\":{\"matchLabels\": {\"domain\": \"example.com\"}}}}' Exclude default router with corresponding custom domain label oc patch \\ -n openshift-ingress-operator \\ IngressController/default \\ --type='merge' \\ -p '{\"spec\":{\"routeSelector\":{\"matchExpressions\":[{\"key\":\"domain\",\"operator\":\"NotIn\",\"values\":[\"example.com\"]}]}}}' Create route with custom domain label cat \u003c\u003cEOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: hello-openshift domain: example.com app.kubernetes.io/component: hello-openshift app.kubernetes.io/instance: hello-openshift name: helloworld spec: host: helloworld-openshift.example.com port: targetPort: 8080-tcp tls: termination: edge to: kind: \"\" name: hello-openshift EOF Only Custom Domain router route the traffic oc get route -o yaml .... status: ingress: - conditions: - lastTransitionTime: \"2022-06-02T20:30:39Z\" status: \"True\" type: Admitted host: helloworld-openshift.example.com routerCanonicalHostname: router-example.example.mobb-infra-gcp.e8e4.p2.openshiftapps.com routerName: example wildcardPolicy: None oc get svc -n openshift-ingress NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE router-default LoadBalancer 172.30.39.254 34.73.154.84 80:32108/TCP,443:30332/TCP 39d router-example LoadBalancer 172.30.209.51 34.138.159.7 80:32477/TCP,443:31383/TCP 9m51s curl -k -H \"Host: helloworld-openshift.example.com\" https://34.73.154.84 .... The application is currently not serving requests at this endpoint. It may not have been started or is still starting. ... curl -k -H \"Host: helloworld-openshift.example.com\" https://34.138.159.7 Hello OpenShift! ","description":"","tags":["OSD","ROSA"],"title":"Stop default router from serving custom domain routes","uri":"/docs/misc/default-router-custom-domain/"},{"content":"Thatcher Hubbard\n15 July 2022\nThis guide focuses on how to synchronize Identity Provider (IDP) groups and users after configuring authentication in OpenShift Cluster Manager (OCM). For an IDP configuration example, please reference the Configure Okta as an OIDC identity provider for ROSA/OSD guide.\nTo set up group synchronization from Okta to ROSA/OSD you must:\nDefine groups and assign users in Okta Install the Group Sync Operator from the OpenShift Operator Hub Create and configure a new Group Sync instance Set a synchronization schedule Test the synchronization process Define groups and assign users in Okta To synchronize groups and users with ROSA/OSD they must exist in Okta\nCreate groups to syncronize with ROSA/OSD if they do not already exist\nCreate user IDs to synchronize with ROSA/OSD if they do not already exist\nAssign newly created users to the appropriate group\nInstall the Group Sync Operator from the OpenShift Operator Hub In the OpenShift Operator Hub find the Group Sync Operator\nInstall the operator in the group-sync-operator namespace\n(Optional) Create an Okta Group Sync Administrator Tokens are created with whatever permissions the currently logged-in user has, which is typically ‘Super Admin’ for a developer account. This is obviously not good practice for anything other than the most basic testing.\nIdeally, a user would be created inside the Okta organization that was specifically for group synchronizations, which should only need to be able to read groups. Creating a user with ‘Read Administrator’ permissions on the account would be a good place to start for following the principle of “least privilege”. That user can then issue a token that includes only those permissions.\nCreate an Okta API Access Token Login as a user that has minimally has Group and User read permissions (see previous section) and generate an API token in Okta\nCreate and configure a new Group Sync instance Create a new secret named okta-group-sync in the group-sync-operator namespace. This will contain the Okta API key that was just created.\nUsing the OpenShift CLI, create the secret using the following format:\noc create secret generic okta-api-token --from-literal='okta-api-token=${API_TOKEN}' -n group-sync-operator Obtain values from Okta for the AppId and URL. The AppId is the client ID under the application created to support OpenID for the OCP Cluster(s). The URL is the same as the one used to admin Okta, without the -admin in the first term and should look something like this:\nhttps://dev-34278011.okta.com/ Create a new Group Sync instance in the group-sync-operator namespace\nUsing the example below, customize the YAML to match the group names and save the configuration\nSample YAML:\napiVersion: redhatcop.redhat.io/v1alpha1 kind: GroupSync metadata: name: okta-sync spec: schedule: \"*/1 * * * *\" providers: - name: okta okta: credentialsSecret: name: okta-api-token namespace: group-sync-operator groups: - ocp-admins - ocp-restricted-users prune: true url: \"\u003cOkta URL here\u003e\" appId: \u003cOkta AppID here\u003e Set a synchronization schedule The Group Sync Operator provides a cron-based scheduling parameter for specifying how often the groups and users should be synchronized. This can be set in the instance YAML file during initial configuration or at any time after.\nThe schedule setting of schedule: \"* * * * *\" would result in synchronization occuring every minute. It also supports the cron “slash” notation (e.g., “*/5 * * * *”, which would synchronize every five minutes).\nTesting the synchronization process Check to see if the Group Sync process has completed with a Condition: ReconcileSuccess message\nCheck to see that all the groups specified in the configuration YAML file show up in the ROSA/OSD Groups list\nValidate that all users specified in Okta also show up as members of the associated group in ROSA/OSD\nAdd a new user in Okta and assign it to the admin group\nVerify that the user now appears in ROSA/OSD (after the specified synchronization time)\nNow deactivate a user from the Okta admin group\nVerify the user has been deleted from the ROSA/OSD admin group\nBinding Groups to Roles The preceding steps provide a method to get group membership information into OpenShift, but the final step in translating that into user authorization control requires binding each group to a role or roles on the cluster. This can be done via the OCP web console by opening the Group detail, or by applying YAML via the CLI.\nAdditional Okta Config Options There are also other options that are provider-specific that are covered in the operator documentation that should be kept in mind: Pruning groups that cease to exist on Okta A numeric limit on the number of groups to sync A list of groups against which to match If there is a need to have multiple GroupSync configurations against multiple providers, note that there is no “merge” functionality in the operator when it comes to group membership. If a group named ocp-admins is present in two directories with sync jobs, they will effectively overwrite each other each time the sync job runs. It is recommended to name groups intended for use on OCP such that they indicate from which directory they originate (e.g., okta-ocp-admins or something like okta-contoso-ocp-admins in the case of multiple Okta providers). Bind multiple groups with the same permissions needs to the same Role or ClusterRole as needed. ","description":"","tags":null,"title":"Using Group Sync Operator with Okta and ROSA/OSD","uri":"/docs/idp/okta-grp-sync/"},{"content":"","description":"","tags":null,"title":"ACM","uri":"/tags/acm/"},{"content":"","description":"","tags":null,"title":"ACS","uri":"/tags/acs/"},{"content":" ACM Observability on ROSA ","description":"","tags":null,"title":"Advanced Cluster Management - Observability","uri":"/docs/redhat/acm/observability/"},{"content":" This document will take you through deploying ACM Observability on a ROSA cluster. see here for the original documentation.\nPrerequisites An existing ROSA cluster An Advanced Cluster Management (ACM) deployment Set up environment Set environment variables\nexport CLUSTER_NAME=my-cluster export S3_BUCKET=$CLUSTER_NAME-acm-observability export REGION=us-east-2 export NAMESPACE=open-cluster-management-observability export SA=tbd export SCRATCH_DIR=/tmp/scratch export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export AWS_PAGER=\"\" rm -rf $SCRATCH_DIR mkdir -p $SCRATCH_DIR Prepare AWS Account Create an S3 bucket\naws s3 mb s3://$S3_BUCKET Create a Policy for access to S3\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/s3-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Statement\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\", \"s3:GetObject\", \"s3:DeleteObject\", \"s3:PutObject\", \"s3:PutObjectAcl\", \"s3:CreateBucket\", \"s3:DeleteBucket\" ], \"Resource\": [ \"arn:aws:s3:::$S3_BUCKET/*\", \"arn:aws:s3:::$S3_BUCKET\" ] } ] } EOF Apply the Policy\nS3_POLICY=$(aws iam create-policy --policy-name $CLUSTER_NAME-acm-obs \\ --policy-document file://$SCRATCH_DIR/s3-policy.json \\ --query 'Policy.Arn' --output text) echo $S3_POLICY Create service account\naws iam create-user --user-name $CLUSTER_NAME-acm-obs \\ --query User.Arn --output text Attach policy to user\naws iam attach-user-policy --user-name $CLUSTER_NAME-acm-obs \\ --policy-arn ${S3_POLICY} Create Access Keys\nread -r ACCESS_KEY_ID ACCESS_KEY \u003c \u003c(aws iam create-access-key \\ --user-name $CLUSTER_NAME-acm-obs \\ --query 'AccessKey.[AccessKeyId,SecretAccessKey]' --output text) ACM Hub Log into the OpenShift cluster that is running your ACM Hub. We’ll set up Observability here\nCreate a namespace for the observability\noc new-project $NAMESPACE Generate a pull secret (this will check if the pull secret exists, if not, it will create it)\nDOCKER_CONFIG_JSON=`oc extract secret/multiclusterhub-operator-pull-secret -n open-cluster-management --to=-` || \\ DOCKER_CONFIG_JSON=`oc extract secret/pull-secret -n openshift-config --to=-` \u0026\u0026 \\ oc create secret generic multiclusterhub-operator-pull-secret \\ -n open-cluster-management-observability \\ --from-literal=.dockerconfigjson=\"$DOCKER_CONFIG_JSON\" \\ --type=kubernetes.io/dockerconfigjson Create a Secret containing your S3 details\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: thanos-object-storage namespace: open-cluster-management-observability type: Opaque stringData: thanos.yaml: | type: s3 config: bucket: $S3_BUCKET endpoint: s3.$REGION.amazonaws.com signature_version2: false access_key: $ACCESS_KEY_ID secret_key: $ACCESS_KEY EOF Create a CR for MulticlusterHub\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: observability.open-cluster-management.io/v1beta2 kind: MultiClusterObservability metadata: name: observability spec: observabilityAddonSpec: {} storageConfig: metricObjectStorage: name: thanos-object-storage key: thanos.yaml EOF Access ACM Observability Log into Advanced Cluster management and access the new Grafana dashboard ","description":"","tags":["Observability","ROSA","ACM"],"title":"Advanced Cluster Management Observability on ROSA","uri":"/docs/redhat/acm/observability/rosa/"},{"content":"Author: Paul Czarkowski Modified: 10/13/2022\nThis document is adapted from the Azure Key Vault CSI Walkthrough specifically to run with Azure Red Hat OpenShift (ARO).\nPrerequisites An ARO cluster The AZ CLI (logged in) Helm 3.x CLI Environment Variables Run this command to set some environment variables to use throughout\nNote if you created the cluster from the instructions linked above these will re-use the same environment variables, or default them to openshift and eastus.\nexport KEYVAULT_RESOURCE_GROUP=${AZR_RESOURCE_GROUP:-\"openshift\"} export KEYVAULT_LOCATION=${AZR_RESOURCE_LOCATION:-\"eastus\"} export KEYVAULT_NAME=secret-store-$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 10 | head -n 1) export AZ_TENANT_ID=$(az account show -o tsv --query tenantId) Installing the Kubernetes Secret Store CSI Create an OpenShift Project to deploy the CSI into\noc new-project k8s-secrets-store-csi Set SecurityContextConstraints to allow the CSI driver to run (otherwise the DaemonSet will not be able to create Pods)\noc adm policy add-scc-to-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver Add the Secrets Store CSI Driver to your Helm Repositories\nhelm repo add secrets-store-csi-driver \\ https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts Update your Helm Repositories\nhelm repo update Install the secrets store csi driver\nhelm install -n k8s-secrets-store-csi csi-secrets-store \\ secrets-store-csi-driver/secrets-store-csi-driver \\ --version v1.0.1 \\ --set \"linux.providersDir=/var/run/secrets-store-csi-providers\" Check that the Daemonsets is running\nkubectl --namespace=k8s-secrets-store-csi get pods -l \"app=secrets-store-csi-driver\" You should see the following\nNAME READY STATUS RESTARTS AGE csi-secrets-store-secrets-store-csi-driver-cl7dv 3/3 Running 0 57s csi-secrets-store-secrets-store-csi-driver-gbz27 3/3 Running 0 57s Deploy Azure Key Store CSI Add the Azure Helm Repository\nhelm repo add csi-secrets-store-provider-azure \\ https://azure.github.io/secrets-store-csi-driver-provider-azure/charts Update your local Helm Repositories\nhelm repo update Install the Azure Key Vault CSI provider\nhelm install -n k8s-secrets-store-csi azure-csi-provider \\ csi-secrets-store-provider-azure/csi-secrets-store-provider-azure \\ --set linux.privileged=true --set secrets-store-csi-driver.install=false \\ --set \"linux.providersDir=/var/run/secrets-store-csi-providers\" \\ --version=v1.0.1 Set SecurityContextConstraints to allow the CSI driver to run\noc adm policy add-scc-to-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:csi-secrets-store-provider-azure Create Keyvault and a Secret Create a namespace for your application\noc new-project my-application Create an Azure Keyvault in your Resource Group that contains ARO\naz keyvault create -n ${KEYVAULT_NAME} \\ -g ${KEYVAULT_RESOURCE_GROUP} \\ --location ${KEYVAULT_LOCATION} Create a secret in the Keyvault\naz keyvault secret set \\ --vault-name ${KEYVAULT_NAME} \\ --name secret1 --value \"Hello\" Create a Service Principal for the keyvault\nNote: If this gives you an error, you may need upgrade your Azure CLI to the latest version.\nexport SERVICE_PRINCIPAL_CLIENT_SECRET=\"$(az ad sp create-for-rbac --skip-assignment --name http://$KEYVAULT_NAME --query 'password' -otsv)\" export SERVICE_PRINCIPAL_CLIENT_ID=\"$(az ad sp list --display-name http://$KEYVAULT_NAME --query '[0].appId' -otsv)\" Set an Access Policy for the Service Principal\naz keyvault set-policy -n ${KEYVAULT_NAME} \\ --secret-permissions get \\ --spn ${SERVICE_PRINCIPAL_CLIENT_ID} Create and label a secret for Kubernetes to use to access the Key Vault\nkubectl create secret generic secrets-store-creds \\ -n my-application \\ --from-literal clientid=${SERVICE_PRINCIPAL_CLIENT_ID} \\ --from-literal clientsecret=${SERVICE_PRINCIPAL_CLIENT_SECRET} kubectl -n my-application label secret \\ secrets-store-creds secrets-store.csi.k8s.io/used=true Deploy an Application that uses the CSI Create a Secret Provider Class to give access to this secret\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: secrets-store.csi.x-k8s.io/v1alpha1 kind: SecretProviderClass metadata: name: azure-kvname namespace: my-application spec: provider: azure parameters: usePodIdentity: \"false\" useVMManagedIdentity: \"false\" userAssignedIdentityID: \"\" keyvaultName: \"${KEYVAULT_NAME}\" objects: | array: - | objectName: secret1 objectType: secret objectVersion: \"\" tenantId: \"${AZ_TENANT_ID}\" EOF Create a Pod that uses the above Secret Provider Class\ncat \u003c\u003cEOF | kubectl apply -f - kind: Pod apiVersion: v1 metadata: name: busybox-secrets-store-inline namespace: my-application spec: containers: - name: busybox image: k8s.gcr.io/e2e-test-images/busybox:1.29 command: - \"/bin/sleep\" - \"10000\" volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"azure-kvname\" nodePublishSecretRef: name: secrets-store-creds EOF Check the Secret is mounted\nkubectl exec busybox-secrets-store-inline -- ls /mnt/secrets-store/ Output should match:\nsecret1 Print the Secret\nkubectl exec busybox-secrets-store-inline \\ -- cat /mnt/secrets-store/secret1 Output should match:\nHello Cleanup Uninstall Helm\nhelm uninstall -n k8s-secrets-store-csi azure-csi-provider Delete the app\noc delete project my-application Delete the Azure Key Vault\naz keyvault delete -n ${KEYVAULT_NAME} Delete the Service Principal\naz ad sp delete --id ${SERVICE_PRINCIPAL_CLIENT_ID} Uninstalling the Kubernetes Secret Store CSI Delete the secrets store csi driver\nhelm delete -n k8s-secrets-store-csi csi-secrets-store Delete the SecurityContextConstraints\noc adm policy remove-scc-from-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver ","description":"","tags":["Azure","ARO"],"title":"Azure Key Vault CSI on Azure Red Hat OpenShift","uri":"/docs/misc/secrets-store-csi/azure-key-vault/"},{"content":"Common Managed OpenShift References / Tasks Managed OpenShift Overviews Red Hat OpenShift Managed services Microsoft Azure Red Hat OpenShift - ARO Red Hat OpenShift on AWS - ROSA Red Hat OpenShift on IBM Cloud Red Hat OpenShift Dedicated - OSD Managed OpenShift Documentation OpenShift Container Platform v4.7 Azure Red Hat OpenShift v4.x - ARO Red Hat OpenShift on AWS v4.x - ROSA Red Hat OpenShift on IBM Cloud v4.x OpenShift Dedicated v4.x - OSD Common Customer Topics Red Hat OpenShift on AWS - ROSA Creating a ROSA cluster with Private Link enabled ROSA Installation Prerequisites ROSA STS Workflow Shared Responsiblity Matrix (who does what) Red Hat Process and Security for ROSA ROSA Support Azure on Red Hat OpenShift ARO ARO Installation Process ARO Support Azure Compliance Monitoring Authentication Education ARO Getting Started ROSA Getting Started ","description":"","tags":null,"title":"Common Managed OpenShift References / Tasks","uri":"/docs/misc/references/"},{"content":"Michael McNeill, Sohaib Azed\n23 September 2022\nThis guide demonstrates how to configure Azure AD as the cluster identity provider in Azure Red Hat OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application and configure Azure Red Hat OpenShift (ARO) to authenticate using Azure AD.\nThis guide will walk through the following steps:\nRegister a new application in Azure AD for authentication. Configure the application registration in Azure AD to include optional claims in tokens. Configure the Azure Red Hat OpenShift (ARO) cluster to use Azure AD as the identity provider. Grant additional permissions to individual users. Before you Begin If you are using zsh as your shell (which is the default shell on macOS) you may need to run set -k to get the below commands to run without errors. This is because zsh disables comments in interactive shells from being used.\n1. Register a new application in Azure AD for authentication Capture the OAuth callback URL First, construct the cluster’s OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variables specified:\nThe “AAD” directory at the end of the the OAuth callback URL should match the OAuth identity provider name you’ll setup later.\nRESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster echo 'OAuth callback URL: '$(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query consoleProfile.url -o tsv | sed 's/console-openshift-console/oauth-openshift/')'oauth2callback/AAD' Register a new application in Azure AD Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to App registrations blade, then click on “New registration” to create a new application.\nProvide a name for the application, for example openshift-auth. Select “Web” from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click “Register” to create the application.\nThen, click on the “Certificates \u0026 secrets” sub-blade and select “New client secret”. Fill in the details request and make note of the generated client secret value, as you’ll use it in a later step. You won’t be able to retrieve it again.\nThen, click on the “Overview” sub-blade and make note of the “Application (client) ID” and “Directory (tenant) ID”. You’ll need those values in a later step as well.\n2. Configure optional claims In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically “email” and “preferred_username” when a user logs in. For more information on optional claims in Azure AD, see the Microsoft documentation.\nClick on the “Token configuration” sub-blade and select the “Add optional claim” button.\nSelect ID then check the “email” and “preferred_username” claims and click the “Add” button to configure them for your Azure AD application.\nWhen prompted, follow the prompt to enable the necessary Microsoft Graph permissions.\n3. Configure the OpenShift cluster to use Azure AD as the identity provider Finally, we need to configure OpenShift to use Azure AD as its identity provider.\nTo do so, ensure you are logged in to the OpenShift command line interface (oc) by running the following command, making sure to replace the variables specified:\nRESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster oc login \\ $(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query apiserverProfile.url -o tsv) \\ -u $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminUsername -o tsv) \\ -p $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminPassword -o tsv) Next, create a secret that contains the client secret that you captured in step 2 above. To do so, run the following command, making sure to replace the variable specified:\nCLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret oc create secret generic openid-client-secret --from-literal=clientSecret=${CLIENT_SECRET} -n openshift-config Next, generate the necessary YAML for the cluster’s OAuth provider to use Azure AD as its identity provider. To do so, run the following command, making sure to replace the variables specified:\nIDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID cat \u003c\u003c EOF \u003e cluster-oauth-config.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - mappingMethod: claim name: ${IDP_NAME} openID: claims: email: - email name: - name preferredUsername: - preferred_username clientID: ${APP_ID} clientSecret: name: openid-client-secret extraScopes: [] issuer: https://login.microsoftonline.com/${TENANT_ID}/v2.0 type: OpenID EOF Feel free to further modify this output (which is saved in your current directory as cluster-oauth-config.yaml).\nFinally, apply the new configuration to the cluster’s OAuth provider by running the following command:\noc apply -f ./cluster-oauth-config.yaml Note: It is normal to receive an error that says an annotation is missing when you run oc apply for the first time. This can be safely ignored.\n4. Grant additional permissions to individual users Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD.\nOnce you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view.\nOpenShift includes a significant number of pre-configured roles, including the cluster-admin role that grants full access and control over the cluster. To grant your user access to the cluster-admin role, you must create a ClusterRoleBinding to your user account.\nUSERNAME=example@redhat.com # Replace with your Azure AD username oc create clusterrolebinding cluster-admin-user \\ --clusterrole=cluster-admin \\ --user=$USERNAME For more information on how to use RBAC to define and apply permissions in OpenShift, see the OpenShift documentation.\n","description":"","tags":["Azure","ARO"],"title":"Configure ARO to use Azure AD","uri":"/docs/idp/azuread-aro/"},{"content":"Michael McNeill\n23 September 2022\nThis guide demonstrates how to utilize the OpenID Connect group claim functionality implemented in OpenShift 4.10. This functionality allows an identity provider to provide a user’s group membership for use within OpenShift. This guide will walk through the creation of an Azure Active Directory (Azure AD) application, configure the necessary Azure AD groups, and configure Azure Red Hat OpenShift (ARO) to authenticate and manage authorization using Azure AD.\nThis guide will walk through the following steps:\nRegister a new application in Azure AD for authentication. Configure the application registration in Azure AD to include optional and group claims in tokens. Configure the Azure Red Hat OpenShift (ARO) cluster to use Azure AD as the identity provider. Grant additional permissions to individual groups. Before you Begin Create a set of security groups and assign users by following the Microsoft documentation.\nIn addition, if you are using zsh as your shell (which is the default shell on macOS) you may need to run set -k to get the below commands to run without errors. This is because zsh disables comments in interactive shells from being used.\n1. Register a new application in Azure AD for authentication Capture the OAuth callback URL First, construct the cluster’s OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variables specified:\nThe “AAD” directory at the end of the the OAuth callback URL should match the OAuth identity provider name you’ll setup later.\nRESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster echo 'OAuth callback URL: '$(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query consoleProfile.url -o tsv | sed 's/console-openshift-console/oauth-openshift/')'oauth2callback/AAD' Register a new application in Azure AD Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to App registrations blade, then click on “New registration” to create a new application.\nProvide a name for the application, for example openshift-auth. Select “Web” from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click “Register” to create the application.\nThen, click on the “Certificates \u0026 secrets” sub-blade and select “New client secret”. Fill in the details request and make note of the generated client secret value, as you’ll use it in a later step. You won’t be able to retrieve it again.\nThen, click on the “Overview” sub-blade and make note of the “Application (client) ID” and “Directory (tenant) ID”. You’ll need those values in a later step as well.\n2. Configure optional claims (for optional and group claims) In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically “email” and “preferred_username”, as well as a group claim when a user logs in. For more information on optional claims in Azure AD, see the Microsoft documentation.\nClick on the “Token configuration” sub-blade and select the “Add optional claim” button.\nSelect ID then check the “email” and “preferred_username” claims and click the “Add” button to configure them for your Azure AD application.\nWhen prompted, follow the prompt to enable the necessary Microsoft Graph permissions.\nNext, select the “Add groups claim” button.\nSelect the “Security groups” option and click the “Add” button to configure group claims for your Azure AD application.\nNote: In this example, we are providing all security groups a user is a member of via the group claim. In a real production environment, we highly recommend _scoping the groups provided by the group claim to only those groups which are applicable to OpenShift.\nGrant the admin consent for the in the API Permission section\n3. Configure the OpenShift cluster to use Azure AD as the identity provider Finally, we need to configure OpenShift to use Azure AD as its identity provider.\nTo do so, ensure you are logged in to the OpenShift command line interface (oc) by running the following command, making sure to replace the variables specified:\nRESOURCE_GROUP=example-rg # Replace this with the name of your ARO cluster's resource group CLUSTER_NAME=example-cluster # Replace this with the name of your ARO cluster oc login \\ $(az aro show -g $RESOURCE_GROUP -n $CLUSTER_NAME --query apiserverProfile.url -o tsv) \\ -u $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminUsername -o tsv) \\ -p $(az aro list-credentials -g $RESOURCE_GROUP -n $CLUSTER_NAME --query kubeadminPassword -o tsv) Next, create a secret that contains the client secret that you captured in step 2 above. To do so, run the following command, making sure to replace the variable specified:\nCLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret oc create secret generic openid-client-secret --from-literal=clientSecret=${CLIENT_SECRET} -n openshift-config Next, generate the necessary YAML for the cluster’s OAuth provider to use Azure AD as its identity provider. To do so, run the following command, making sure to replace the variables specified:\nIDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID cat \u003c\u003c EOF \u003e cluster-oauth-config.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - mappingMethod: claim name: ${IDP_NAME} openID: claims: email: - email groups: - groups name: - name preferredUsername: - email clientID: ${APP_ID} clientSecret: name: openid-client-secret extraScopes: [] issuer: https://login.microsoftonline.com/${TENANT_ID}/v2.0 type: OpenID EOF Feel free to further modify this output (which is saved in your current directory as cluster-oauth-config.yaml).\nFinally, apply the new configuration to the cluster’s OAuth provider by running the following command:\noc apply -f ./cluster-oauth-config.yaml Note: It is normal to receive an error that says an annotation is missing when you run oc apply for the first time. This can be safely ignored.\nOnce the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. In addition, the cluster OAuth provider will automatically create or update the membership of groups the user is a member of (using the group ID). The provider does not automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes.\n4. Grant additional permissions to individual groups Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD. In addition, the cluster OAuth provider will automatically create or update the membership of groups the user is a member of (using the group ID).\nOnce you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view. The cluster OAth provider does not automatically create RoleBindings and ClusterRoleBindings for the groups that are created, you are responsible for creating those via your own processes.\nOpenShift includes a significant number of pre-configured roles, including the cluster-admin role that grants full access and control over the cluster. To grant an automatically generated group access to the cluster-admin role, you must create a ClusterRoleBinding to the group ID.\nGROUP_ID=wwwwwwww-wwww-wwww-wwww-wwwwwwwwwwww # Replace with your Azure AD Group ID that you would like to have cluster admin permissions oc create clusterrolebinding cluster-admin-group \\ --clusterrole=cluster-admin \\ --group=$GROUP_ID Now, any user in the specified group will automatically be granted cluster-admin access.\nFor more information on how to use RBAC to define and apply permissions in OpenShift, see the OpenShift documentation.\n","description":"","tags":null,"title":"Configure ARO to use Azure AD Group Claims","uri":"/docs/idp/group-claims/aro/"},{"content":"Daniel Moessner\n26 June 2022\nThe steps to add Azure AD as an identity provider for Azure Red Hat OpenShift (ARO) via cli are:\nPrerequisites Have Azure cli installed Login to Azure Azure Define needed variables Get oauthCallbackURL Create manifest.json file to configure the Azure Active Directory application Register/create app Add Service Principal for the new app Make Service Principal an Enterprise Application Create the client secret Update the Azure AD application scope permissions Get Tenant ID OpenShift Login to OpenShift as kubeadmin Create an OpenShift secret### Apply OpenShift OpenID authentication Wait for authentication operator to roll out Verify login through Azure Active Directory Last steps Prerequisites Have Azure cli installed Follow the Microsoft instuctions: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli\nNote This has been written for az cli verion 2.37.0 some commands will not work with previous versions, however, there is a known issue https://github.com/Azure/azure-cli/issues/23027 where we will use an older version via podman run -it mcr.microsoft.com/azure-cli:2.36.0 . In case you’re using docker, just replace podman command by docker . For podman installation on Mac, Windows \u0026 Linux, please refer to https://podman.io/getting-started/installation\nLogin to Azure Login to Azure as follows:\naz login If you’re logging in from a system you have no access to your browser you can authenticate, you can also use\naz login --use-device-code Azure Define needed variables To simplly follow along, first define the following variables according to your set-up:\nRESOURCEGROUP=\u003ccluster-dmoessne-aro01\u003e # replave with your name CLUSTERNAME=\u003crg-dmoessne-aro01\u003e # replave with your name Get oauthCallbackURL To get the oauthCallbackURL for the Azure AD integration, run the following commands:\nDOMAIN=$(az aro show -g $RESOURCEGROUP -n $CLUSTERNAME --query clusterProfile.domain -o tsv) APISERVER=$(az aro show -g $RESOURCEGROUP -n $CLUSTERNAME --query apiserverProfile.url -o tsv) oauthCallbackURL=https://oauth-openshift.apps.$DOMAIN/oauth2callback/AAD echo $oauthCallbackURL Note oauthCallbackURL, in particular AAD can be changed but must match the name in the oauth providerwhen creating the OpenShift OpenID authentication\nCreate manifest.json file to configure the Azure Active Directory application Configure OpenShift to use the email claim and fall back to upn to set the Preferred Username by adding the upn as part of the ID token returned by Azure Active Directory.\nCreate a manifest.json file to configure the Azure Active Directory application.\ncat \u003c\u003c EOF \u003e manifest.json { \"idToken\": [ { \"name\": \"upn\", \"source\": null, \"essential\": false, \"additionalProperties\": [] }, { \"name\": \"email\", \"source\": null, \"essential\": false, \"additionalProperties\": [] } ] } EOF Register/create app Create an Azure AD application and retrieve app id:\nDISPLAYNAME=\u003cauth-dmoessne-aro01\u003e # set you name accordingly az ad app create \\ --display-name $DISPLAYNAME \\ --web-redirect-uris $oauthCallbackURL \\ --sign-in-audience AzureADMyOrg \\ --optional-claims @manifest.json APPID=$(az ad app list --display-name $DISPLAYNAME --query [].appId -o tsv) Add Service Principal for the new app Create Service Principal for the app created:\naz ad sp create --id $APPID Make Service Principal an Enterprise Application We need this Service Principal to be an Enterprise Application to be able to add users and groups, so we add the needed tag (az cli \u003e= 2.38.0)\naz ad sp update --id $APPID --set 'tags=[\"WindowsAzureActiveDirectoryIntegratedApp\"]' Note In case you get a trace back (az cli = 2.37.0) check out https://github.com/Azure/azure-cli/issues/23027 To overcome that issue, we’ll do the following\n# APP_ID=$(az ad app list --display-name $DISPLAYNAME --query [].id -o tsv) # az rest --method PATCH --url https://graph.microsoft.com/v1.0/applications/$APP_ID --body '{\"tags\":[\"WindowsAzureActiveDirectoryIntegratedApp\"]}' Create the client secret The password for the app created is retrieved by resetting the same:\nPASSWD=$(az ad app credential reset --id $APPID --query password -o tsv) Note The password generated with above command is by default valid for one year and you may want to change that by adding either and end date via --end-date or set validity in years with --years. For details consult the documentation\nUpdate the Azure AD application scope permissions To be able to read the user information from Azure Active Directory, we need to add the following Azure Active Directory Graph permissions\nAdd permission for the Azure Active Directory as follows:\nread email az ad app permission add \\ --api 00000003-0000-0000-c000-000000000000 \\ --api-permissions 64a6cdd6-aab1-4aaf-94b8-3cc8405e90d0=Scope \\ --id $APPID read profile az ad app permission add \\ --api 00000003-0000-0000-c000-000000000000 \\ --api-permissions 14dad69e-099b-42c9-810b-d002981feec1=Scope \\ --id $APPID User.Read az ad app permission add \\ --api 00000003-0000-0000-c000-000000000000 \\ --api-permissions e1fe6dd8-ba31-4d61-89e7-88639da4683d=Scope \\ --id $APPID Note If you see a message that you need to grant consent you can safely ignore it, unless you are authenticated as a alobal administrator for this Azure Active Directory. Standard domain users will be asked to grant consent when they first login to the cluster using their AAD credentials.\nGet Tenant ID We do need the Tenant ID for setting up the Oauth provider later on:\nTENANTID=$(az account show --query tenantId -o tsv) Note Now we can switch over to our OpenShift installation and apply the needed configuraion. Please refer to https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/getting-started-cli.html to get the latest oc cli\nOpenShift Login to OpenShift as kubeadmin Fetch kubeadmin password and login to your cluster via oc cli (you can use any other cluster-admin user in case you have already created/added other oauth providers)\nKUBEPW=$(az aro list-credentials \\ --name $CLUSTERNAME \\ --resource-group $RESOURCEGROUP \\ --query kubeadminPassword --output tsv) oc login $APISERVER -u kubeadmin -p $KUBEPW Create an OpenShift secret### Create an OpenShift secret to store the Azure Active Directory application secret from the application password we created/reset earlier:\noc create secret generic openid-client-secret-azuread \\ -n openshift-config \\ --from-literal=clientSecret=$PASSWD Apply OpenShift OpenID authentication As a last step we need to apply the OpenShift OpenID authentication for Azure Active Directory:\ncat \u003c\u003c EOF | oc apply -f - apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: AAD mappingMethod: claim type: OpenID openID: clientID: $APPID clientSecret: name: openid-client-secret-azuread extraScopes: - email - profile extraAuthorizeParameters: include_granted_scopes: \"true\" claims: preferredUsername: - email - upn name: - name email: - email issuer: https://login.microsoftonline.com/$TENANTID EOF Wait for authentication operator to roll out Before we move over to the OpenShift login, let’s wait for the new version of the authentication cluster operator to be rolled out\nwatch -n 5 oc get co authentication Note it may take some time until the rollout starts\nVerify login through Azure Active Directory Get console url to login:\naz aro show --name $CLUSTERNAME --resource-group $RESOURCEGROUP --query \"consoleProfile.url\" -o tsv Opening the url in a browser, we can see the login to Azure AD is available\nAt first login you may have to accept application permissions\nLast steps As a last step you may want to grant a user or group cluster-admin permissions and remove kubeadmin user, see\nhttps://docs.openshift.com/container-platform/4.10/authentication/using-rbac.html#cluster-role-binding-commands_using-rbac https://docs.openshift.com/container-platform/4.10/authentication/remove-kubeadmin.html ","description":"","tags":["Azure","ARO"],"title":"Configure Azure AD as an OIDC identity provider for ARO with cli","uri":"/docs/idp/azuread-aro-cli/"},{"content":"Michael McNeill, Andrea Bozzoni, Steve Mirman\n23 September 2022\nThis guide demonstrates how to configure Azure AD as the cluster identity provider in Red Hat OpenShift Service on AWS (ROSA). This guide will walk through the creation of an Azure Active Directory (Azure AD) application and configure Red Hat OpenShift Service on AWS (ROSA) to authenticate using Azure AD.\nThis guide will walk through the following steps:\nRegister a new application in Azure AD for authentication. Configure the application registration in Azure AD to include optional and group claims in tokens. Configure the OpenShift cluster to use Azure AD as the identity provider. Grant additional permissions to individual groups. Before you Begin Create a set of security groups and assign users by following the Microsoft documentation.\nIn addition, if you are using zsh as your shell (which is the default shell on macOS) you may need to run set -k to get the below commands to run without errors. This is because zsh disables comments in interactive shells from being used.\n1. Register a new application in Azure AD for authentication Capture the OAuth callback URL First, construct the cluster’s OAuth callback URL and make note of it. To do so, run the following command, making sure to replace the variable specified:\nThe “AAD” directory at the end of the the OAuth callback URL should match the OAuth identity provider name you’ll setup later.\nCLUSTER_NAME=example-cluster # Replace this with the name of your ROSA cluster domain=$(rosa describe cluster -c $CLUSTER_NAME | grep \"DNS\" | grep -oE '\\S+.openshiftapps.com') echo \"OAuth callback URL: https://oauth-openshift.apps.$domain/oauth2callback/AAD\" Register a new application in Azure AD Second, you need to create the Azure AD application itself. To do so, login to the Azure portal, and navigate to App registrations blade, then click on “New registration” to create a new application.\nProvide a name for the application, for example openshift-auth. Select “Web” from the Redirect URI dropdown and fill in the Redirect URI using the value of the OAuth callback URL you retrieved in the previous step. Once you fill in the necessary information, click “Register” to create the application.\nThen, click on the “Certificates \u0026 secrets” sub-blade and select “New client secret”. Fill in the details request and make note of the generated client secret value, as you’ll use it in a later step. You won’t be able to retrieve it again.\nThen, click on the “Overview” sub-blade and make note of the “Application (client) ID” and “Directory (tenant) ID”. You’ll need those values in a later step as well.\n2. Configure optional claims In order to provide OpenShift with enough information about the user to create their account, we will configure Azure AD to provide two optional claims, specifically “email” and “preferred_username” when a user logs in. For more information on optional claims in Azure AD, see the Microsoft documentation.\nClick on the “Token configuration” sub-blade and select the “Add optional claim” button.\nSelect ID then check the “email” and “preferred_username” claims and click the “Add” button to configure them for your Azure AD application.\nWhen prompted, follow the prompt to enable the necessary Microsoft Graph permissions.\n3. Configure the OpenShift cluster to use Azure AD as the identity provider Finally, we need to configure OpenShift to use Azure AD as its identity provider. While Red Hat OpenShift Service on AWS (ROSA) offers the ability to configure identity providers via the OpenShift Cluster Manager (OCM), that functionality does not currently support group claims. Instead, we’ll configure the cluster’s OAuth provider to use Azure AD as its identity provider via the rosa CLI. To do so, run the following command, making sure to replace the variable specified:\nCLUSTER_NAME=example-cluster # Replace this with the name of your ROSA cluster IDP_NAME=AAD # Replace this with the name you used in the OAuth callback URL APP_ID=yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy # Replace this with the Application (client) ID CLIENT_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # Replace this with the Client Secret TENANT_ID=zzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz # Replace this with the Directory (tenant) ID rosa create idp \\ --cluster ${CLUSTER_NAME} \\ --type openid \\ --name ${IDP_NAME} \\ --client-id ${APP_ID} \\ --client-secret ${CLIENT_SECRET} \\ --issuer-url https://login.microsoftonline.com/${TENANT_ID}/v2.0 \\ --email-claims email \\ --name-claims name \\ --username-claims preferred_username 4. Grant additional permissions to individual users Once the cluster authentication operator reconciles your changes (generally within a few minutes), you will be able to login to the cluster using Azure AD.\nOnce you login, you will notice that you have very limited permissions. This is because, by default, OpenShift only grants you the ability to create new projects (namespaces) in the cluster. Other projects (namespaces) are restricted from view.\nOpenShift includes a significant number of pre-configured roles, including the cluster-admin role that grants full access and control over the cluster. To grant your user access to the cluster-admin role, run the following command:\nUSERNAME=example@redhat.com # Replace with your Azure AD username rosa grant user cluster-admin \\ --user=${USERNAME} \\ --cluster=${CLUSTER_NAME} For more information on how to use RBAC to define and apply permissions in OpenShift, see the OpenShift documentation.\n","description":"","tags":["Azure","ROSA","OSD"],"title":"Configure Azure AD as an OIDC identity provider for ROSA/OSD","uri":"/docs/idp/azuread/"},{"content":"Steve Mirman\n28 March 2022\nThe following instructions will detail how to configure GitLab as the identity provider for Azure Red Hat OpenShift:\nRegister a new application in GitLab Create OAuth callback URL in ARO Log in and confirm Add administrative users or groups Register a new application in GitLab Log into GitLab and execute the following steps:\nGo to Preferences\nSelect Applications from the left navigation bar\nProvide a Name and enter an OAuth Callback URL as the Redirect URI in GitLab\nNote: the OAuth Callback has the following format: https://oauth-openshift.apps.\u003ccluster-id\u003e.\u003cregion\u003e.aroapp.io/oauth2callback/GitLab\nCheck the openid box and save the application\nAfter saving the GitLab application you will be provided with an Application ID and a Secret\nCopy both the Application ID and Secret for use in the ARO console\nCreate OAuth provider in ARO Log in to the ARO console as an administrator to add a GitLab identity provider\nSelect the ‘Administration’ drop down and click ‘Cluster Settings’\nOn the ‘Configuration’ scroll down and click on ‘OAuth’\nSelect ‘GitLab’ from the Identity Providers drop down\nEnter a Name, the base URL of your GitLab OAuth server, and the Client ID and CLient Secret from the previous step\nClick Add to confirm the configuration\nLog in and confirm Go to the ARO console in a new browser to bring up the OpenShift login page. An option for GitLab should now be available.\nNote: I can take 2-3 minutes for this update to occur\nAfter selecting GitLab for the first time an authorization message will appear. Click Authorize to confirm.\nOnce you have successfully logged in using GitLab, your userid should display under Users in the User Management section of the ARO console\nNote: On initial login users do NOT have elevated access\nAdd administrative users or groups Now that the GitLab identity provider is configured, it is possible to add authenticated users to elevated OpenShift roles. This can be accomplished at the user or group level.\nTo elevate a users permissions, select the user in the OpenShift console and click Create Binding from the RoleBindings tab\nChoose the scope (namespace/cluster), assign a name to the RoleBinding, and choose a role.\nAfter clicking Create the assigned user will have elevated access once they log in.\nTo elevate a groups permissions, create a group in the OpenShift console.\nEdit the group YAML to specify a custom name and initial user set\nCreate a RoleBinding for the group, similar to what was configured previously for an individual user\nAdd additional users to the YAML file as needed and they will assume the elevated access\n","description":"","tags":null,"title":"Configure GitLab as an identity provider for ARO","uri":"/docs/idp/gitlab-aro/"},{"content":"Steve Mirman\n16 February 2022\nThe following instructions will detail how to configure GitLab as the identity provider for Managed OpenShift through the OpenShift Cluster Manager (OCM):\nCreate OAuth callback URL in OCM Register a new application in GitLab Configure the identity provider credentials and URL Add cluster-admin or dedicated-admin users Log in and confirm Create OAuth callback URL in OCM Log in to the OpenShift Cluster Manager (OCM) to add a GitLab identity provider\nSelect your cluster in OCM and then go to the ‘Access control’ tab and select ‘Identity Providers’\nChoose GitLab as identity provider from the identity providers list\nProvide a name for the new identity provider\nCopy the OAuth callback URL. It will be needed later\nNote: the OAuth Callback has the following format:\nhttps://oauth-openshift.apps.\u003ccluster_name\u003e.\u003ccluster_domain\u003e/oauth2callback/\u003cidp_name\u003e At this point, leave the Client ID, Client secret, and URL blank while configuring GitLab\nRegister a new application in GitLab Log into GitLab and execute the following steps:\nGo to Preferences\nSelect Applications from the left navigation bar\nProvide a Name and enter the OAuth Callback URL copied from OCM above and enter it as the Redirect URL in GitLab\nCheck the openid box and save the application\nAfter saving the GitLab application you will be provided with an Application ID and a Secret\nCopy both the Application ID and Secret and return to the OCM console\nConfigure the identity provider credentials and URL Returning to the OCM console, enter the Application ID and Secret obtained from GitLab in the previous step and enter them as Client ID and Client Secret respectively in the OCM console. Additionally, provide the GitLab URL where credentials were obtained and click Add\nThe new GitLab identity provider should display in the IDP list\nAdd cluster-admin or dedicated-admin users Now that the GitLab identity provider is configured, it is possible to add authenticated users to elevated OCM and OpenShift roles. Under Cluster Roles and Access select Add user and enter an existing GitLab user. Then choose to assign dedicated-admin or cluster-admin permissions to the user and click Add user\nThe new user should now display, with proper permissions, in the cluster-admin or dedicated-admin user lists\nLog in and confirm Select the Open console button in OCM to bring up the OpenShift login page. An option for GitLab should now be available.\nNote: I can take 1-2 minutes for this update to occur\nAfter selecting GitLab for the first time an authorization message will appear. Click Authorize to confirm.\nCongratulations!\n","description":"","tags":null,"title":"Configure GitLab as an identity provider for ROSA/OSD","uri":"/docs/idp/gitlab/"},{"content":"Prerequisites AWS CLI Openshift CLI 4.8+ Docker Background Quick Introduction by Ryan Niksch \u0026 Charlotte Fung on YouTube.\nThere are two options to use to authenticate wth Amazon ECR to pull images.\nThe traditional method is to create a pull secret for ecr.\nExample:\noc create secret docker-registry ecr-pull-secret \\ --docker-server=\u003cregistry id\u003e.dkr.ecr.\u003cregion\u003e.amazonaws.com \\ --docker-username=AWS --docker-password=$(aws ecr get-login-password) \\ --namespace=hello-world However Amazon ECR tokens expire every 12 hours which will mean you will need to re-authenticate every 12 hours either through scripting or do so manually.\nA second, and preferred method, is to attach an ECR Policy to your cluster’s worker machine profiles which this guide will walk you through.\nAttach ECR Policy Role You can attach an ECR policy to your cluster giving the cluster permissions to pull images from your registries. ROSA worker machine instances comes with pre-defined IAM roles, named differently depending on whether its a STS cluster or a non-STS cluster.\nSTS Cluster Role ManagedOpenShift-Worker-Role is the IAM role attached to ROSA STS compute instances.\nnon-STS Cluster Role \u003ccluster name\u003e-\u003cidentifier\u003e-worker-role is the IAM role attached to ROSA non-STS compute instances.\nTip: To find the non-STS cluster role run the following command with your cluster name:\naws iam list-roles | grep \u003ccluster_name\u003e ECR Policies ECR has several pre-defined policies that give permissions to interact with the service. In the case of ROSA, we will be pulling images from ECR and will only need to add the AmazonEC2ContainerRegistryReadOnly policy.\nAdd the AmazonEC2ContainerRegistryReadOnly policy to the ManagedOpenShift-Worker-Role for STS clusters or the \u003ccluster name\u003e-\u003cidentifier\u003e-worker-role for non-STS clusters.\nSTS Example:\naws iam attach-role-policy \\ --role-name ManagedOpenShift-Worker-Role \\ --policy-arn \"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\" Test it Out Log into ECR\naws ecr get-login-password --region region | docker login --username AWS \\ --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com Create a repository\naws ecr create-repository \\ --repository-name hello-world \\ --image-scanning-configuration scanOnPush=true \\ --region region Pull an image\ndocker pull openshift/hello-openshift Tag the image for ecr\ndocker tag openshift/hello-openshift:latest \u003cregistry id\u003e.dkr.ecr.\u003cregion\u003e.amazonaws.com/hello-world:latest note: you can find the registry id and URI with the following command\naws ecr describe-repositories Push the image to ECR\ndocker push \u003cregistry id\u003e.dkr.ecr.\u003cregion\u003e.amazonaws.com/hello-world:latest Create a new project\noc new project hello-world Create a new app using the image on ECR\noc new-app --name hello-world --image \u003cregistry id\u003e.dkr.ecr.\u003cregion\u003e.amazonaws.com/hello-world:latest View a list of pods in the namespace you created:\noc get pods Expected output:\nIf you see the hello-world pod running … congratulations! You can now pull images from your ECR repository.\nClean up Simply delete the project you created to test pulling images:\noc delete project hello-world You may also want to remove the arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly policy from the worker nodes if you do no want them to continue to have access to the ECR.\n","description":"","tags":["AWS","ROSA"],"title":"Configuring a ROSA cluster to pull images from AWS Elastic Container Registry (ECR)","uri":"/docs/rosa/ecr/"},{"content":"Red Hat OpenShift on AWS (ROSA) and OpenShift Dedicated (OSD) provide a simple way for the cluster administrator to configure one or more identity providers for their cluster[s] via the OpenShift Cluster Manager (OCM), while Azure Red Hat OpenShift relies on the internal cluster OAuth provider.\nThe identity providers available for use are:\nGitHub GitLab Google LDAP OpenID HTPasswd Configuring Specific Identity Providers ARO GitLab Azure AD Azure AD with Group Claims Azure AD via CLI ROSA/OSD GitLab Azure AD Azure AD with Group Claims (ROSA Only) Configuring Group Synchronization Using Group Sync Operator with Azure Active Directory and ROSA/OSD Using Group Sync Operator with Okta and ROSA/OSD ","description":"","tags":null,"title":"Configuring IDP for ROSA and OSD","uri":"/docs/idp/"},{"content":"","description":"","tags":null,"title":"Cost","uri":"/tags/cost/"},{"content":"Create IAM user and Policy Notes: These are sample commands. Please fill in your own resource parameters E.g. ARN\nCreate the policy cat \u003c\u003cEOF \u003e /tmp/iam_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ecr:GetAuthorizationToken\" ], \"Resource\": \"*\" } ] } EOF aws iam create-policy \\ --policy-name ECRLoginPolicy \\ --policy-document file:///tmp/iam_policy.json Create a user and access key and attach the policy aws iam create-user --user-name ecr-bot aws create-access-key --user-name ecr-bot aws iam attach-user-policy --policy-arn arn:aws:iam::[ACCOUNT_ID]:policy/ECRLoginPolicy --user-name ecr-bot Notes: Save access key id and key for later usage\nSet up a specific ECR repository access cat \u003c\u003cEOF \u003e /tmp/repo_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowPushPull\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::[ACCOUNT_ID]:user/ecr-bot\" ] }, \"Action\": [ \"ecr:BatchGetImage\", \"ecr:BatchCheckLayerAvailability\", \"ecr:CompleteLayerUpload\", \"ecr:GetDownloadUrlForLayer\", \"ecr:InitiateLayerUpload\", \"ecr:PutImage\", \"ecr:UploadLayerPart\" ] } ] } EOF aws ecr set-repository-policy --repository-name test --policy-text file:///tmp/repo_policy.json Create kubernetes Secret with iam user cat \u003c\u003cEOF \u003e /tmp/credentials [default] aws_access_key_id=\"\" aws_secret_access_key=\"\" EOF oc create secret generic aws-ecr-cloud-credentials --from-file=credentials=/tmp/credentials ","description":"","tags":["AWS","ROSA"],"title":"Create IAM user and Policy","uri":"/docs/rosa/ecr-secret-operator/iam_user/"},{"content":"Create STS Assume Role About AWS STS and Assume Role\nNotes: These are sample commands. Please fill in your own resource parameters E.g. ARN\nPrequisites\nAn STS Openshift Cluster\nCreate the policy\ncat \u003c\u003cEOF \u003e /tmp/iam_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ecr:GetAuthorizationToken\" ], \"Resource\": \"*\" } ] } EOF aws iam create-policy \\ --policy-name ECRLoginPolicy \\ --policy-document file:///tmp/iam_policy.json Create the role and attach the policy cat \u003c\u003cEOF \u003e /tmp/trust_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::[ACCOUNT_ID]:oidc-provider/rh-oidc.s3.us-east-1.amazonaws.com/1ou2pbj9v68ghlc63bo0mad059cj1elf\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"rh-oidc.s3.us-east-1.amazonaws.com/1ou2pbj9v68ghlc63bo0mad059cj1elf:sub\": \"system:serviceaccount:ecr-secret-operator:ecr-secret-operator-controller-manager\" } } } ] } EOF aws iam create-role --role-name ECRLogin --assume-role-policy-document file:///tmp/trust_policy.json aws iam attach-role-policy --role-name ECRLogin --policy-arn arn:aws:iam::[ACCOUNT_ID]:policy/ECRLoginPolicy Create the repository policy cat \u003c\u003cEOF \u003e /tmp/repo_policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowPushPull\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::[ACCOUNT_ID]:role/ECRLogin\" ] }, \"Action\": [ \"ecr:BatchGetImage\", \"ecr:BatchCheckLayerAvailability\", \"ecr:CompleteLayerUpload\", \"ecr:GetDownloadUrlForLayer\", \"ecr:InitiateLayerUpload\", \"ecr:PutImage\", \"ecr:UploadLayerPart\" ] } ] } EOF aws ecr set-repository-policy --repository-name test --policy-text file:///tmp/repo_policy.json Create STS kubernetes Secret cat \u003c\u003cEOF \u003e /tmp/credentials [default] role_arn = arn:aws:iam::[ACCOUNT_ID]:role/ECRLogin web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token EOF oc create secret generic aws-ecr-cloud-credentials --from-file=credentials=/tmp/credentials ","description":"","tags":["AWS","ROSA"],"title":"Create STS Assume Role","uri":"/docs/rosa/ecr-secret-operator/iam_assume_role/"},{"content":"Creating a Public/Private BYO VPC for ROSA This is example Terraform to create a single AZ VPC in which to deploy a single AZ ROSA cluster. This is intended to be used as a guide to get started quickly, not to be used in production.\nPre-Requisites Terraform Deploy Download this repo\ngit clone https://github.com/rh-mobb/documentation.git cd documentation/docs/rosa/byo-vpc Modify main.tf as needed, then run\nterraform init terraform plan terraform apply Cleanup To destroy resources terraform destroy ","description":"","tags":["AWS","ROSA"],"title":"Creating a Public/Private BYO VPC for ROSA","uri":"/docs/rosa/byo-vpc/"},{"content":"Byron Miller\nLast updated 4/21/2022\nTip Official Documentation ROSA STS with custom KMS key\nThis guide will walk you through installing ROSA (Red Hat OpenShift Service on AWS) with a customer-provided KMS key that will be used to encrypt both the root volumes of nodes as well as persistent volumes for mounted EBS claims.\nPrerequisites AWS CLI ROSA CLI v1.1.11 or higher OpenShift CLI - rosa download openshift-client Prepare AWS Account for ROSA Configure the AWS CLI by running the following command\naws configure You will be required to enter an AWS Access Key ID and an AWS Secret Access Key along with a default region name and output format\n% aws configure AWS Access Key ID []: AWS Secret Access Key []: Default region name [us-east-2]: Default output format [json]: The AWS Access Key ID and AWS Secret Access Key values can be obtained by logging in to the AWS console and creating an Access Key in the Security Credentials section of the IAM dashboard for your user\nValidate your credentials\naws sts get-caller-identity You should receive output similar to the following\n{ \"UserId\": \u003cyour ID\u003e, \"Account\": \u003cyour account\u003e, \"Arn\": \u003cyour arn\u003e } You will need to save the account ID for adding it to your KMS key to define installer role, so take note.\nIf this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following\naws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\" Set the AWS region you plan to deploy your cluser into. For this example, we will deploy into us-east-2.\nexport AWS_REGION=\"us-east-2\" Create KMS Key For this example, we will create a custom KMS key using the AWS CLI. If you would prefer, you could use an existing key instead.\nCreate a customer-managed KMS key\nKMS_ARN=$(aws kms create-key --region $AWS_REGION --description 'Custom ROSA Encryption Key' --query KeyMetadata.Arn --output text) This command will save the ARN output of this custom key for further steps.\nGenerate the necessary key policy to allow the ROSA STS roles to access the key. Use the below command to populate a sample policy, or create your own.\nImportant note, if you specify a custom STS role prefix, you will need to update that in the command below.\nAWS_ACCOUNT=$(aws sts get-caller-identity --query Account --output text); cat \u003c\u003c EOF \u003e rosa-key-policy.json { \"Version\": \"2012-10-17\", \"Id\": \"key-rosa-policy-1\", \"Statement\": [ { \"Sid\": \"Enable IAM User Permissions\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::${AWS_ACCOUNT}:root\" }, \"Action\": \"kms:*\", \"Resource\": \"*\" }, { \"Sid\": \"Allow ROSA use of the key\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-ControlPlane-Role\" ] }, \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": \"*\" }, { \"Sid\": \"Allow attachment of persistent resources\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Support-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Installer-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-Worker-Role\", \"arn:aws:iam::${AWS_ACCOUNT}:role/ManagedOpenShift-ControlPlane-Role\" ] }, \"Action\": [ \"kms:CreateGrant\", \"kms:ListGrants\", \"kms:RevokeGrant\" ], \"Resource\": \"*\", \"Condition\": { \"Bool\": { \"kms:GrantIsForAWSResource\": \"true\" } } } ] } EOF Apply the newly generated key policy to the custom KMS key.\naws kms put-key-policy --key-id $KMS_ARN \\ --policy file://rosa-key-policy.json \\ --policy-name default Create ROSA Cluster Make sure your ROSA CLI version is at minimum v1.1.11 or higher.\nrosa version Create the ROSA STS Account Roles\nIf you have already installed account-roles into your aws account, you can skip this step.\nrosa create account-roles --mode auto -y Set Environment Variables\nROSA_CLUSTER_NAME=poc-kmskey Using the ROSA CLI, create your cluster.\nWhile this is an example, feel free to customize this command to best suit your needs.\nrosa create cluster --cluster-name $ROSA_CLUSTER_NAME --sts \\ --region $AWS_REGION --compute-nodes 2 --machine-cidr 10.0.0.0/16 \\ --service-cidr 172.30.0.0/16 --pod-cidr 10.128.0.0/14 --host-prefix 23 \\ --kms-key-arn $KMS_ARN Create the operator roles necessary for the cluster to function.\nrosa create operator-roles -c $ROSA_CLUSTER_NAME --mode auto --yes Create the OIDC provider necessary for the cluster to authenticate.\nrosa create oidc-provider -c $ROSA_CLUSTER_NAME --mode auto --yes Validate that the cluster is now installing. Within 5 minutes, the cluster state should move beyond pending and show installing.\nwatch \"rosa describe cluster -c $ROSA_CLUSTER_NAME\" Watch the install logs as the cluster installs.\nrosa logs install -c $ROSA_CLUSTER_NAME --watch --tail 10 Validate the cluster Once the cluster has finished installing we can validate our access to the cluster.\nCreate an Admin user\nrosa create admin -c $ROSA_CLUSTER_NAME Run the resulting login statement from output. May take 2-3 minutes before authentication is fully synced\nVerify the default persistent volumes in the cluster.\noc get pv Output:\nNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-00dac374-a45e-43fa-a313-ae0491e8edf1 10Gi RWO Delete Bound openshift-monitoring/alertmanager-data-alertmanager-main-1 gp2-customer-kms 26m pvc-7d211496-4ddf-4200-921c-1404b754afa5 10Gi RWO Delete Bound openshift-monitoring/alertmanager-data-alertmanager-main-0 gp2-customer-kms 26m pvc-b5243cef-ec30-4e5c-a348-aeb8136a908c 100Gi RWO Delete Bound openshift-monitoring/prometheus-data-prometheus-k8s-0 gp2-customer-kms 26m pvc-ec60c1cf-72cf-4ac6-ab12-8e9e5afdc15f 100Gi RWO Delete Bound openshift-monitoring/prometheus-data-prometheus-k8s-1 gp2-customer-kms 26m You should see the StroageClass set to gp2-customer-kms. This is the default StorageClass which is encrypted using the customer-provided key.\nCleanup Delete the ROSA cluster\nrosa delete cluster -c $ROSA_CLUSTER_NAME Once the cluster is deleted, delete the cluster’s STS roles.\nrosa delete operator-roles -c $ROSA_CLUSTER_NAME --yes --mode auto rosa delete oidc-provider -c $ROSA_CLUSTER_NAME --yes --mode auto ","description":"","tags":["AWS","ROSA"],"title":"Creating a ROSA cluster in STS mode with custom KMS key","uri":"/docs/rosa/kms/"},{"content":"Starting with OpenShift 4.11 it is possible to manage alerting rules for user-defined projects. Similarly, in ROSA clusters the OpenShift Administrator can enable a second AlertManager instance in the user workload monitoring namespace which can be used to create such alerts.\nNote: Currently this is not a managed feature of ROSA. Such an implementation may get overwritten if the User Workload Monitoring functionality is toggled using the OpenShift Cluster Manager (OCM).\nPrerequisites AWS CLI A Red Hat OpenShift for AWS (ROSA) cluster 4.11.0 or higher Create Environment Variables Configure User Workload Monitoring to include AlertManager Edit the user workload config to include AlertManager Note: If you have other modifications to this config, you will need to hand edit the resource rather than brute forcing it like below.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | alertmanager: enabled: true enableAlertmanagerConfig: true EOF Verify that a new Alert Manager instance is defined\noc -n openshift-user-workload-monitoring get alertmanager NAME VERSION REPLICAS AGE user-workload 0.24.0 2 2m If you want non-admin users to be able to define alerts in their own namespaces you can run the following.\noc -n \u003cnamespace\u003e adm policy add-role-to-user alert-routing-edit \u003cuser\u003e Update the Alert Manager Configuration file\nThis will create a basic AlertManager configuration to send alerts to a slack channel. Configuring slack is outside the scope of this document. Update the variables to suit your slack integration.\nSLACK_API_URL=https://hooks.slack.com/services/XXX/XXX/XXX SLACK_CHANNEL='#paultest' cat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: alertmanager-user-workload namespace: openshift-user-workload-monitoring stringData: alertmanager.yaml: | global: slack_api_url: \"${SLACK_API_URL}\" route: receiver: Default group_by: [alertname] receivers: - name: Default slack_configs: - channel: ${SLACK_CHANNEL} send_resolved: true EOF Create an Example Alert Create a Namespace for your custom alert\noc create namespace custom-alert Verify it works by creating a Prometheus Rule that will fire off an alert\ncat \u003c\u003c EOF | kubectl apply -n custom-alert -f - apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: prometheus-example-rules spec: groups: - name: example.rules rules: - alert: ExampleAlert expr: vector(1) EOF Forward a port to the alert manager service\nkubectl port-forward -n openshift-user-workload-monitoring \\ svc/alertmanager-operated 9093:9093 Browse to http://localhost:9093/#/alerts to see the alert “ExampleAlert”\nCheck the Alert was sent to Slack\n","description":"","tags":["AWS","ROSA"],"title":"Custom Alerts in ROSA 4.11.x","uri":"/docs/rosa/custom-alertmanager/"},{"content":"Author: Steve Mirman\nVideo Walkthrough If you prefer a more visual medium, you can watch Steve Mirman walk through this quickstart on YouTube.\nThe purpose of this document is to help you get OpenShift GitOps running in your cluster, including deploying a sample application and demonstrating how ArgoCD ensures environment consistency.\nThis demo assumes you have a Managed OpenShift Cluster available and cluster-admin rights.\nGitHub resources referenced in the demo: BGD Application: gitops-bgd-app OpenShift / ArgoCD configuration: gitops-demo Required command line (CLI) tools GitHub: git OpenShift: oc ArgoCD: argocd Kustomize: kam Environment Set Up Install the OpenShift GitOps operator Install the OpenShift GitOps operator from the Operator Hub\nPull files from GitHub Clone the gitops-demo GitHub repository to your local machine\ngit clone https://github.com/rh-mobb/gitops-demo gitops Export your local path to the GitHub files\nexport GITOPS_HOME=\"$(pwd)/gitops\" cd $GITOPS_HOME Log in to OpenShift via the CLI Retrieve the login command from the OpenShift console Enter the command in your terminal to authenticate with the OpenShift CLI (oc)\nOutput should appear similar to:\nLogged into \"https://\u003cYOUR-INSTANCE\u003e.openshiftapps.com:6443\" as \"\u003cYOUR-ID\u003e\" using the token provided. Deploy the ArgoCD Project Create a new OpenShift project Create a new OpenShift project called gitops oc new-project gitops Edit service account permissions Add cluster-admin rights to the openshift-gitops-argocd-application-controller service account in the openshift-gitops namespace oc adm policy add-cluster-role-to-user cluster-admin -z openshift-gitops-argocd-application-controller -n openshift-gitops Log in to ArgoCD Retrieve ArgoCD URL:\nargoURL=$(oc get route openshift-gitops-server -n openshift-gitops -o jsonpath='{.spec.host}{\"\\n\"}') echo $argoURL Retrieve ArgoCD Password:\nargoPass=$(oc get secret/openshift-gitops-cluster -n openshift-gitops -o jsonpath='{.data.admin\\.password}' | base64 -d) echo $argoPass In a browser, navigate to the ArgoCD console using the $argoURL value returned above Log in with the user name admin and the password returned as $argoPass above Optional step if you prefer CLI access Login to the CLI:\nargocd login --insecure --grpc-web $argoURL --username admin --password $argoPass Deploy the ArgoCD project Use kubectl to apply the bgd-app.yaml file\nkubectl apply -f documentation/modules/ROOT/examples/bgd-app/bgd-app.yaml The bgd-app.yaml file defines several things, including the repo location for the gitops-bgd-app application\nCheck the rollout running the following command:\nkubectl rollout status deploy/bgd -n bgd Once the rollout is complete get the route to the application\noc get route bgd -n bgd -o jsonpath='{.spec.host}{\"\\n\"}' In your browser, paste the route to open the application Go back to your ArgoCD window and verify the configuration shows there as well Exploring the application in ArgoCD, you can see all the components are green (synchronized) Deploy a change to the application In the terminal, enter the following command which will introduce a chance into the bgd application\nkubectl -n bgd patch deploy/bgd --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/env/0/value\", \"value\":\"green\"}]' Go back to your ArgoCD window. The application should no longer be synchronized Refresh the bgd application window and notice the change in box color\nThe new deployment changed the box from blue to green, but only within OpenShift, not in the source code repository\nSynchronize the application In the ArgoCD console, click the SYNC button to re-synchronize the bgd application with the approved configuration in the source code repository Refresh the bgd application window and notice the change in box color\nDetails from GitHub perspective TBD\n","description":"","tags":["GitOps","ROSA","ARO"],"title":"Demonstrate GitOps on Managed OpenShift with ArgoCD","uri":"/docs/redhat/gitops/"},{"content":"Michael McNeill\n26 January 2022\nThis document will take you through deploying 3scale in any OSD or ROSA cluster. Review the official documentation here for more information or how to further customize or use 3scale.\nPrerequisites An existing ROSA or OSD cluster Access to an AWS account with permissions to create S3 buckets, IAM users, and IAM policies A subscription for 3scale API Management A wildcard domain configured with a CNAME to your cluster’s ingress controller Prepare AWS Account Set environment variables (ensuring you update the variables appropriately!)\nexport S3_BUCKET=mobb-3scale-bucket export REGION=us-east-1 export S3_IAM_USER_NAME=mobb-3scale-user export S3_IAM_POLICY_NAME=3scale-s3-access export AWS_PAGER=\"\" export PROJECT_NAME=3scale-example export WILDCARD_DOMAIN=3scale.example.com Create an S3 bucket\naws s3 mb s3://$S3_BUCKET Apply the proper S3 bucket CORS configuration\naws s3api put-bucket-cors --bucket $S3_BUCKET --cors-configuration \\ '{ \"CORSRules\": [{ \"AllowedMethods\": [ \"GET\" ], \"AllowedOrigins\": [ \"https://*\" ] }] }' Create an IAM policy for access to the S3 bucket\nPOLICY_ARN=$(aws iam create-policy --policy-name \"$S3_IAM_POLICY_NAME\" \\ --output text --query \"Policy.Arn\" \\ --policy-document \\ '{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:ListAllMyBuckets\", \"Resource\": \"arn:aws:s3:::*\" }, { \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::'$S3_BUCKET'\", \"arn:aws:s3:::'$S3_BUCKET'/*\" ] } ] }') Create an IAM user to access the S3 bucket\naws iam create-user --user-name $S3_IAM_USER_NAME Generate an access key for the newly created S3 user\nACCESS_CREDS=$(aws iam create-access-key --user-name $S3_IAM_USER_NAME \\ --output text --query \"AccessKey.[AccessKeyId, SecretAccessKey]\") Apply the IAM policy to the newly created S3 user\naws iam attach-user-policy --user-name $S3_IAM_USER_NAME \\ --policy-arn $POLICY_ARN Install the 3Scale API Management Operator Create a new project to install 3Scale API Management into.\noc new-project $PROJECT_NAME Inside of the OpenShift Web Console, navigate to Operators -\u003e OperatorHub.\nSearch for “3scale” and select the “Red Hat Integration - 3scale” Operator.\nClick “Install” and select the project you wish to install the operator into.\nFor this example, I’m deploying into the “3scale-example” project that I have just created.\nOnce the 3Scale operator successfully installs, return to your terminal.\nDeploy 3Scale API Management Create a secret that contains the Amazon S3 configuration.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: creationTimestamp: null name: aws-auth stringData: AWS_ACCESS_KEY_ID: \"$(echo $ACCESS_CREDS | cut -f 1)\" AWS_SECRET_ACCESS_KEY: \"$(echo $ACCESS_CREDS | cut -f 2)\" AWS_BUCKET: \"$S3_BUCKET\" AWS_REGION: \"$REGION\" type: Opaque EOF Create an APIManager custom resource\ncat \u003c\u003c EOF | oc apply -f - echo 'apiVersion: apps.3scale.net/v1alpha1 kind: APIManager metadata: name: example-apimanager spec: wildcardDomain: '$WILDCARD_DOMAIN' system: fileStorage: simpleStorageService: configurationSecretRef: name: aws-auth EOF Once the APIManager instance becomes available, you can login to the 3Scale Admin (located at https://3scale-admin.$WILDCARD_DOMAIN) using the credentials from the below commands:\noc get secret system-seed -o jsonpath={.data.ADMIN_USER} | base64 -d oc get secret system-seed -o jsonpath={.data.ADMIN_PASSWORD} | base64 -d Congratulations! You’ve successfully deployed 3Scale API Management to ROSA/OSD.\n","description":"","tags":["ROSA","OSD"],"title":"Deploying 3scale API Management to ROSA and OSD","uri":"/docs/redhat/3scale/"},{"content":"Prerequisites An STS enabled ROSA cluster Getting Started Create the following environment variables\nChange the cluster name to match your ROSA cluster and ensure you’re logged into the cluster as an Administrator. Ensure all fields are outputted correctly before moving on.\nexport CLUSTER_NAME=my-cluster export ROSA_CLUSTER_ID=$(rosa describe cluster -c ${CLUSTER_NAME} --output json | jq -r .id) export REGION=$(rosa describe cluster -c ${CLUSTER_NAME} --output json | jq -r .region.id) export OIDC_ENDPOINT=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer) export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` export CLUSTER_VERSION=`rosa describe cluster -c ${CLUSTER_NAME} -o json | jq -r .version.raw_id | cut -f -2 -d '.'` export ROLE_NAME=\"${CLUSTER_NAME}-openshift-oadp-aws-cloud-credentials\" export AWS_PAGER=\"\" export SCRATCH=\"/tmp/${CLUSTER_NAME}/oadp\" mkdir -p ${SCRATCH} echo \"Cluster ID: ${ROSA_CLUSTER_ID}, Region: ${REGION}, OIDC Endpoint: ${OIDC_ENDPOINT}, AWS Account ID: ${AWS_ACCOUNT_ID}\" Prepare AWS Account Create an IAM Policy to allow for S3 Access\nPOLICY_ARN=$(aws iam list-policies --query \"Policies[?PolicyName=='RosaOadp'].{ARN:Arn}\" --output text) if [[ -z \"${POLICY_ARN}\" ]]; then cat \u003c\u003c EOF \u003e ${SCRATCH}/policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:CreateBucket\", \"s3:DeleteBucket\", \"s3:PutBucketTagging\", \"s3:GetBucketTagging\", \"s3:PutEncryptionConfiguration\", \"s3:GetEncryptionConfiguration\", \"s3:PutLifecycleConfiguration\", \"s3:GetLifecycleConfiguration\", \"s3:GetBucketLocation\", \"s3:ListBucket\", \"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\", \"s3:ListBucketMultipartUploads\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\", \"ec2:DescribeSnapshots\", \"ec2:CreateTags\", \"ec2:CreateVolume\", \"ec2:CreateSnapshot\", \"ec2:DeleteSnapshot\" ], \"Resource\": \"*\" } ]} EOF POLICY_ARN=$(aws iam create-policy --policy-name \"RosaOadp\" \\ --policy-document file:///${SCRATCH}/policy.json --query Policy.Arn \\ --tags Key=rosa_openshift_version,Value=4.9 Key=rosa_role_prefix,Value=ManagedOpenShift Key=operator_namespace,Value=openshift-oadp Key=operator_name,Value=openshift-oadp \\ --output text) fi echo ${POLICY_ARN} Create an IAM Role trust policy for the cluster\ncat \u003c\u003cEOF \u003e ${SCRATCH}/trust-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/rh-oidc.s3.us-east-1.amazonaws.com/${ROSA_CLUSTER_ID}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"rh-oidc.s3.us-east-1.amazonaws.com/${ROSA_CLUSTER_ID}:sub\": [ \"system:serviceaccount:openshift-adp:openshift-adp-controller-manager\", \"system:serviceaccount:openshift-adp:velero\"] } } }] } EOF ROLE_ARN=$(aws iam create-role --role-name \\ \"${ROLE_NAME}\" \\ --assume-role-policy-document file://${SCRATCH}/trust-policy.json \\ --tags Key=rosa_cluster_id,Value=${ROSA_CLUSTER_ID} Key=rosa_openshift_version,Value=${CLUSTER_VERSION} Key=rosa_role_prefix,Value=ManagedOpenShift Key=operator_namespace,Value=openshift-adp Key=operator_name,Value=openshift-oadp \\ --query Role.Arn --output text) echo ${ROLE_ARN} Attach the IAM Policy to the IAM Role\naws iam attach-role-policy --role-name \"${ROLE_NAME}\" \\ --policy-arn ${POLICY_ARN} Deploy OADP on cluster Create a namespace for OADP\noc create namespace openshift-adp Create a credentials secret\ncat \u003c\u003cEOF \u003e ${SCRATCH}/credentials [default] role_arn = ${ROLE_ARN} web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token EOF oc -n openshift-adp create secret generic cloud-credentials \\ --from-file=${SCRATCH}/credentials Deploy OADP Operator\ncat \u003c\u003c EOF | oc create -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-adp- namespace: openshift-adp name: oadp spec: targetNamespaces: - openshift-adp --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/oadp-operator.openshift-adp: \"\" name: oadp-operator namespace: openshift-adp spec: channel: stable installPlanApproval: Automatic name: oadp-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: oadp-operator.v0.5.6 EOF Wait for the operator to be ready\nwatch oc -n openshift-adp get pods NAME READY STATUS RESTARTS AGE openshift-adp-controller-manager-546684844f-qqjhn 1/1 Running 0 22s Create Cloud Storage\ncat \u003c\u003c EOF | oc create -f - apiVersion: oadp.openshift.io/v1alpha1 kind: CloudStorage metadata: name: ${CLUSTER_NAME}-oadp namespace: openshift-adp spec: creationSecret: key: credentials name: cloud-credentials enableSharedConfig: true name: ${CLUSTER_NAME}-oadp provider: aws region: $REGION EOF Deploy a Data Protection Application\ncat \u003c\u003c EOF | oc create -f - apiVersion: oadp.openshift.io/v1alpha1 kind: DataProtectionApplication metadata: name: ${CLUSTER_NAME}-dpa namespace: openshift-adp spec: backupLocations: - bucket: cloudStorageRef: name: ${CLUSTER_NAME}-oadp credential: key: credentials name: cloud-credentials default: true configuration: velero: defaultPlugins: - openshift - aws restic: enable: false volumeSnapshots: - velero: config: credentialsFile: /tmp/credentials/openshift-adp/cloud-credentials-credentials enableSharedConfig: \"true\" region: ${REGION} provider: aws EOF Perform a backup Create a workload to backup\noc create namespace hello-world oc new-app -n hello-world --docker-image=docker.io/openshift/hello-openshift Backup workload\ncat \u003c\u003c EOF | oc create -f - apiVersion: velero.io/v1 kind: Backup metadata: name: hello-world namespace: openshift-adp spec: includedNamespaces: - hello-world storageLocation: ${CLUSTER_NAME}-dpa-1 ttl: 720h0m0s EOF Wait until backup is done\nwatch \"oc -n openshift-adp get backup hello-world -o json | jq .status\" { \"completionTimestamp\": \"2022-09-07T22:20:44Z\", \"expiration\": \"2022-10-07T22:20:22Z\", \"formatVersion\": \"1.1.0\", \"phase\": \"Completed\", \"progress\": { \"itemsBackedUp\": 58, \"totalItems\": 58 }, \"startTimestamp\": \"2022-09-07T22:20:22Z\", \"version\": 1 } Delete the demo workload\noc delete ns hello-world Restore from the backup\ncat \u003c\u003c EOF | oc create -f - apiVersion: velero.io/v1 kind: Restore metadata: name: hello-world namespace: openshift-adp spec: backupName: hello-world EOF Wait for the Restore to finish\nwatch \"oc -n openshift-adp get restore hello-world -o json | jq .status\" { \"completionTimestamp\": \"2022-09-07T22:25:47Z\", \"phase\": \"Completed\", \"progress\": { \"itemsRestored\": 38, \"totalItems\": 38 }, \"startTimestamp\": \"2022-09-07T22:25:28Z\", \"warnings\": 9 } Check the workload is restored\noc -n hello-world get pods NAME READY STATUS RESTARTS AGE hello-openshift-9f885f7c6-kdjpj 1/1 Running 0 90s Cleanup Delete the workload\noc delete ns hello-world Delete the Data Protection Application\noc delete dpa ${CLUSTER_NAME}-dpa Delete the Cloud Storage\noc delete cloudstorage ${CLUSTER_NAME}-oadp «««\u003c HEAD:content/docs/misc/oadp/rosa-sts/_index.md Delete the AWS S3 Bucket\naws s3 rm s3://${CLUSTER_NAME}-oadp --recursive aws s3api delete-bucket --bucket ${CLUSTER_NAME}-oadp Detach the Policy from the role\naws iam detach-role-policy --role-name \"${ROLE_NAME}\" \\ --policy-arn \"${POLICY_ARN}\" Delete the role\naws iam delete-role --role-name \"${ROLE_NAME}\" main:docs/misc/oadp/rosa-sts/README.md\n","description":"","tags":["ROSA","AWS","STS"],"title":"Deploying OpenShift Advanced Data Protection on a ROSA cluster","uri":"/docs/misc/oadp/rosa-sts/"},{"content":"Author: Roberto Carratalá\nUpdated: 10/06/2022\nThis document is based in the RHACS workshop and in the RHACS official documentation.\nPrerequisites An ARO cluster or a ROSA cluster. Set up the OpenShift CLI (oc) Download the OS specific OpenShift CLI from Red Hat\nUnzip the downloaded file on your local machine\nPlace the extracted oc executable in your OS path or local directory\nLogin to ARO / ROSA Login to your ARO / ROSA clusters with user with cluster-admin privileges. Installing Red Hat Advanced Cluster Security in ARO/ROSA For install RHACS in ARO/ROSA you have two options:\nOption 1 - Manual Installation Option 2 - Automated Installation using Ansible Option 1 - Manual Installation For install RHACS using the Option 1 - Manual installation:\nFollow the steps within the RHACS Operator Installation Workshop to install the RHACS Operator.\nFollow the steps within the RHACS Central Cluster Installation Workshop to install the RHACS Central Cluster.\nFollow the steps within the RHACS Secured Cluster Configuration, to import the ARO/ROSA cluster into RHACS.\nOption 2 - Automated Installation using Ansible For install the RHACS in ROSA/ARO you can use the rhacs-demo repository that will install RH-ACS using Ansible playbooks:\nClone the rhacm-demo repo and install the galaxy collection: ansible-galaxy collection install kubernetes.core pip3 install kubernetes jmespath git clone https://github.com/rh-mobb/rhacs-demo cd rhacs-demo Deploy the RHACS with the ansible-playbook command: ansible-playbook rhacs-install.yaml This will install RHACS and also a couple of example Apps to demo. If you want just the plain RHACS installation, use the rhacs-only-install.yaml playbook.\nDeploying Example Apps for demo RHACS Deploy some example apps for demo RHACS policies and violations: oc new-project test oc run shell --labels=app=shellshock,team=test-team \\ --image=vulnerables/cve-2014-6271 -n test oc run samba --labels=app=rce \\ --image=vulnerables/cve-2017-7494 -n test ","description":"","tags":["ACS","ARO","ROSA"],"title":"Deploying Red Hat Advanced Cluster Security in ARO/ROSA","uri":"/docs/redhat/rhacs/"},{"content":"Amazon Elastic Container Registry Private Registry Authentication provides a temporary token that is valid only for 12 hours. It is a challenge for automating container image build process to refresh the token or secret in a timely manner.\nThis operators frequently talks with AWS ECR GetAuthroization Token and create/update the secret, so that the service account can perform docker image build.\nHow to use this operator Prerequisites Create an ECR private repository Provide AWS Authentication to the operator. Two Options: IAM User STS Assume Role Install the operator Install the operator from operator hub community Create the ECR Secret CRD echo \u003c\u003c EOF | oc apply -f - apiVersion: ecr.mobb.redhat.com/v1alpha1 kind: Secret metadata: name: ecr-secret namespace: test-ecr-secret-operator spec: generated_secret_name: ecr-docker-secret ecr_registry: [ACCOUNT_ID].dkr.ecr.us-east-2.amazonaws.com frequency: 10h region: us-east-2 EOF A docker registry secret is created by the operator momentally and the token is patched every 10 hours\noc get secret ecr-docker-secret NAME TYPE DATA AGE ecr-docker-secret kubernetes.io/dockerconfigjson 1 16h A sample build process with generated secret Link the secret to builder\noc secrets link builder ecr-docker-secret Configure build config to point to your ECR Container repository\noc create imagestream ruby oc tag openshift/ruby:2.5-ubi8 ruby:2.5 echo \u003c\u003c EOF | oc apply -f - kind: BuildConfig apiVersion: build.openshift.io/v1 metadata: name: ruby-sample-build namespace: test-ecr-secret-operator spec: runPolicy: Serial source: git: uri: \"https://github.com/openshift/ruby-hello-world\" strategy: sourceStrategy: from: kind: \"ImageStreamTag\" name: \"ruby:2.5\" incremental: true output: to: kind: \"DockerImage\" name: \"[ACCOUNT_ID].dkr.ecr.us-east-2.amazonaws.com/test:latest\" postCommit: script: \"bundle exec rake test\" EOF oc start-build ruby-sample-build --wait Build should succeed and push the image to the the private ECR Container repository\n","description":"","tags":["AWS","ROSA"],"title":"ECR Secret Operator","uri":"/docs/rosa/ecr-secret-operator/"},{"content":"Author: Paul Czarkowski Modified: 07/11/2022\nThe Amazon Web Services Elastic File System (AWS EFS) is a Network File System (NFS) that can be provisioned on Red Hat OpenShift Service on AWS clusters. With the release of OpenShift 4.10 the EFS CSI Driver is now GA and available.\nThis is a guide to quickly enable the EFS Operator on ROSA to a Red Hat OpenShift on AWS (ROSA) cluster with STS enabled.\nNote: The official supported installation instructions for the EFS CSI Driver on ROSA are available here.\nPrerequisites A Red Hat OpenShift on AWS (ROSA) 4.10 cluster The OC CLI The AWS CLI JQ Set up environment export some environment variables\nexport CLUSTER_NAME=\"sts-cluster\" export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json \\ | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export SCRATCH_DIR=/tmp/scratch export AWS_PAGER=\"\" mkdir -p $SCRATCH_DIR Prepare AWS Account In order to use the AWS EFS CSI Driver we need to create IAM roles and policies that can be attached to the Operator.\nCreate an IAM Policy\ncat \u003c\u003c EOF \u003e $SCRATCH_DIR/efs-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"elasticfilesystem:DescribeAccessPoints\", \"elasticfilesystem:DescribeFileSystems\", \"elasticfilesystem:DescribeMountTargets\", \"ec2:DescribeAvailabilityZones\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"elasticfilesystem:CreateAccessPoint\" ], \"Resource\": \"*\", \"Condition\": { \"StringLike\": { \"aws:RequestTag/efs.csi.aws.com/cluster\": \"true\" } } }, { \"Effect\": \"Allow\", \"Action\": \"elasticfilesystem:DeleteAccessPoint\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/efs.csi.aws.com/cluster\": \"true\" } } } ] } EOF Create the Policy\nThis creates a named policy for the cluster, you could use a generic policy for multiple clusters to keep things simpler.\nPOLICY=$(aws iam create-policy --policy-name \"${CLUSTER_NAME}-rosa-efs-csi\" \\ --policy-document file://$SCRATCH_DIR/efs-policy.json \\ --query 'Policy.Arn' --output text) || \\ POLICY=$(aws iam list-policies \\ --query 'Policies[?PolicyName==`rosa-efs-csi`].Arn' \\ --output text) echo $POLICY Create a Trust Policy\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/TrustPolicy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": [ \"system:serviceaccount:openshift-cluster-csi-drivers:aws-efs-csi-driver-operator\", \"system:serviceaccount:openshift-cluster-csi-drivers:aws-efs-csi-driver-controller-sa\" ] } } } ] } EOF Create Role for the EFS CSI Driver Operator\n```bash ROLE=$(aws iam create-role \\ --role-name \"${CLUSTER_NAME}-aws-efs-csi-operator\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $ROLE Attach the Policies to the Role\naws iam attach-role-policy \\ --role-name \"${CLUSTER_NAME}-aws-efs-csi-operator\" \\ --policy-arn $POLICY Deploy and test the AWS EFS Operator Create a Secret to tell the AWS EFS Operator which IAM role to request.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: aws-efs-cloud-credentials namespace: openshift-cluster-csi-drivers stringData: credentials: |- [default] role_arn = $ROLE web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token EOF Install the EFS Operator\ncat \u003c\u003cEOF | oc create -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-cluster-csi-drivers- namespace: openshift-cluster-csi-drivers --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/aws-efs-csi-driver-operator.openshift-cluster-csi-drivers: \"\" name: aws-efs-csi-driver-operator namespace: openshift-cluster-csi-drivers spec: channel: stable installPlanApproval: Automatic name: aws-efs-csi-driver-operator source: redhat-operators sourceNamespace: openshift-marketplace EOF Wait until the Operator is running\nwatch oc get deployment aws-efs-csi-driver-operator -n openshift-cluster-csi-drivers Install the AWS EFS CSI Driver\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operator.openshift.io/v1 kind: ClusterCSIDriver metadata: name: efs.csi.aws.com spec: managementState: Managed EOF Wait until the CSI driver is running\nwatch oc get daemonset aws-efs-csi-driver-node -n openshift-cluster-csi-drivers Create a storage class\ncat \u003c\u003cEOF | oc apply -f - allowVolumeExpansion: true apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: efs-csi provisioner: efs.csi.aws.com parameters: reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer EOF Prepare an AWS EFS Volume Run this set of commands to update the VPC to allow EFS access\nNODE=$(oc get nodes --selector=node-role.kubernetes.io/worker \\ -o jsonpath='{.items[0].metadata.name}') VPC=$(aws ec2 describe-instances \\ --filters \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{VpcId:VpcId}' \\ | jq -r '.[0][0].VpcId') SUBNET=$(aws ec2 describe-subnets \\ --filters Name=vpc-id,Values=$VPC Name=tag:Name,Values='*-private*' \\ --query 'Subnets[*].{SubnetId:SubnetId}' \\ | jq -r '.[0].SubnetId') CIDR=$(aws ec2 describe-vpcs \\ --filters \"Name=vpc-id,Values=$VPC\" \\ --query 'Vpcs[*].CidrBlock' \\ | jq -r '.[0]') SG=$(aws ec2 describe-instances --filters \\ \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{SecurityGroups:SecurityGroups}' \\ | jq -r '.[0][0].SecurityGroups[0].GroupId') echo \"CIDR - $CIDR, SG - $SG\" Assuming the CIDR and SG are correct, update the security group\naws ec2 authorize-security-group-ingress \\ --group-id $SG \\ --protocol tcp \\ --port 2049 \\ --cidr $CIDR | jq . Create EFS File System\nEFS=$(aws efs create-file-system --creation-token efs-token-1 \\ --encrypted | jq -r '.FileSystemId') echo $EFS Configure Mount Target for EFS\nMOUNT_TARGET=$(aws efs create-mount-target --file-system-id $EFS \\ --subnet-id $SUBNET --security-groups $SG \\ | jq -r '.MountTargetId') echo $MOUNT_TARGET Create a Storage Class for the EFS volume\ncat \u003c\u003cEOF | oc apply -f - kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: efs-sc provisioner: efs.csi.aws.com parameters: provisioningMode: efs-ap fileSystemId: $EFS directoryPerms: \"700\" gidRangeStart: \"1000\" gidRangeEnd: \"2000\" basePath: \"/dynamic_provisioning\" EOF Test Create a namespace\noc new-project efs-demo Create a PVC\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-efs-volume spec: storageClassName: efs-sc accessModes: - ReadWriteMany resources: requests: storage: 5Gi EOF Create a Pod to write to the EFS Volume\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-efs spec: volumes: - name: efs-storage-vol persistentVolumeClaim: claimName: pvc-efs-volume containers: - name: test-efs image: centos:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"while true; do echo 'hello efs' | tee -a /mnt/efs-data/verify-efs \u0026\u0026 sleep 5; done;\" ] volumeMounts: - mountPath: \"/mnt/efs-data\" name: efs-storage-vol EOF It may take a few minutes for the pod to be ready. If you see errors such as Output: Failed to resolve \"fs-XXXX.efs.us-east-2.amazonaws.com\" it likely means its still setting up the EFS volume, just wait longer.\nWait for the Pod to be ready\nwatch oc get pod test-efs Create a Pod to read from the EFS Volume\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-efs-read spec: volumes: - name: efs-storage-vol persistentVolumeClaim: claimName: pvc-efs-volume containers: - name: test-efs-read image: centos:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"tail -f /mnt/efs-data/verify-efs\" ] volumeMounts: - mountPath: \"/mnt/efs-data\" name: efs-storage-vol EOF Verify the second POD can read the EFS Volume\noc logs test-efs-read You should see a stream of “hello efs”\nhello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs Cleanup Delete the Pods\noc delete pod -n efs-demo test-efs test-efs-read Delete the Volume\noc delete -n efs-demo pvc pvc-efs-volume Delete the Namespace\noc delete project efs-demo Delete the storage class\noc delete storageclass efs-sc Delete the EFS Shared Volume via AWS\naws efs delete-mount-target --mount-target-id $MOUNT_TARGET aws efs delete-file-system --file-system-id $EFS Note if you receive the error An error occurred (FileSystemInUse) wait a few minutes and try again.\nDetach the Policies to the Role\naws iam detach-role-policy \\ --role-name \"${CLUSTER_NAME}-aws-efs-csi-operator\" \\ --policy-arn $POLICY Delete the Role\naws iam delete-role --role-name \\ ${CLUSTER_NAME}-aws-efs-csi-operator Delete the Policy\naws iam delete-policy --policy-arn \\ $POLICY ","description":"","tags":["AWS","ROSA"],"title":"Enabling the AWS EFS CSI Driver Operator on ROSA","uri":"/docs/rosa/aws-efs-csi-operator-sts/"},{"content":"Problem Statement Operator requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA)\nOperator does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF\nQuick Introduction by Paul Czarkowski \u0026 Ryan Niksch on YouTube\nSolutions Cloud Front -\u003e WAF -\u003e CustomDomain -\u003e $APP This is the preferred method and can also work with most third party WAF systems that act as a reverse proxy\nUses a custom domain, custom route, LE cert. CloudFront and WAF\nUsing Cloud Front Application Load Balancer -\u003e ALB Operator -\u003e $APP Installs the ALB Operator, and uses the ALB to route via WAF, one ALB per app though!\nApplication Load Balancer ","description":"","tags":["AWS","ROSA","OSD","OCP"],"title":"Examples of using a WAF in front of ROSA / OSD on AWS / OCP on AWS","uri":"/docs/rosa/waf/"},{"content":"Extending ROSA STS to include authentication with AWS Services In this example we will deploy the Amazon Ingress Controller that uses ALBs, and configure it to use STS authentication.\nDeployment Configure STS Make sure your cluster has the pod identity webhook\nkubectl get mutatingwebhookconfigurations.admissionregistration.k8s.io pod-identity-webhook Download the IAM Policy for the AWS Load Balancer Hooks\nwget https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.0/docs/install/iam_policy.json Create AWS Role with inline policy\naws iam create-role \\ --role-name AWSLoadBalancerController --query Policy.Arn --output text Create AWS Policy and Service Account\nPOLICY_ARN=$(aws iam create-policy --policy-name \"AWSLoadBalancerControllerIAMPolicy\" --policy-document file://iam_policy.json --query Policy.Arn --output text) echo $POLICY_ARN Create service account\nNote I had issues with the policy, and for now just gave this user admin creds. Need to revisit and figure out.\nSA_ARN=$(aws iam create-user --user-name aws-lb-controller --permissions-boundary=$POLICY_ARN --query User.Arn --output text) Create access key\nACCESS_KEY=$(aws iam create-access-key --user-name aws-lb-controller) Attach policy to user\nPaste the AccessKeyId and SecretAccessKey into values.yaml\ntag your public subnet with ``\nCreate a namespace for the controller\nkubectl create ns aws-load-balancer-controller Apply CRDs\nkubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Add the helm repo and install the controller (install helm3 if not already)\nhelm repo add eks https://aws.github.io/eks-charts helm install -n aws-load-balancer-controller \\ aws-load-balancer-controller eks/aws-load-balancer-controller \\ --values=./helm/values.yaml --create-namespace Deploy Sample Application oc new-project demo oc new-app https://github.com/sclorg/django-ex.git kubectl -n demo patch service django-ex -p '{\"spec\":{\"type\":\"NodePort\"}}' kubectl apply -f ingress.yaml ","description":"","tags":["AWS","ROSA","STS"],"title":"Extending ROSA STS to include authentication with AWS Services","uri":"/docs/rosa/using-sts-with-aws-services/"},{"content":"Red Hat Openshift for AWS (ROSA) comes with two built-in monitoring stacks. ClusterMonitoring and User Workload Monitoring. They are both based on Prometheus, the first targets the Cluster Operator (Red Hat SRE) and the latter targets the Cluster user (you!).\nBoth provide amazing metrics insights inside the Cluster’s web console, showing overall cluster metrics as well as namespace specific workload metrics, all integrated with your configured IDP.\nHowever the Alert Manager instance is locked down and used to send alerts to the Red Hat SRE team. This means that the customer cannot create alerts for either the cluster resources, or their own workloads. This is being worked on and future versions of ROSA will provide a way for the end user to create alerts for their own workloads.\nUntil that work is done, the ROSA cluster administrator can deploy a Prometheus instance and configure it to send alerts to themselves. Thankfully with Prometheus’ federated metrics feature and the Prometheus Operator, this can be done in a few simple steps.\nThis guide is heavily influenced by Tommer Amber’s guide for OCP 4.x.\nPre-requisites Make sure the following pre-requisites are met: Helm 3 A Red Hat OpenShift for AWS (ROSA) cluster 4.8 or higher Prepare Environment Set the following environment variables\nexport NAMESPACE=federated-metrics Create the namespace\noc new-project $NAMESPACE Add the MOBB chart repository to your Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories\nhelm repo update Use the mobb/operatorhub chart to deploy the needed operators\nhelm upgrade -n $NAMESPACE federated-metrics-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/rosa-federated-prometheus/files/operatorhub.yaml Wait until the two operators are running\nwatch kubectl get pods -n $NAMESPACE NAME READY STATUS RESTARTS AGE grafana-operator-controller-manager-775f8d98c9-822h7 2/2 Running 0 7m33s operatorhubio-dtb2v 1/1 Running 0 8m32s prometheus-operator-5cb6844699-t7wfd 1/1 Running 0 7m29s Deploy the monitoring stack Install the mobb/rosa-federated-prometheus Helm Chart\nhelm upgrade --install -n $NAMESPACE monitoring \\ --set grafana-cr.basicAuthPassword='mypassword' \\ --set fullnameOverride='monitoring' \\ --version 0.5.3 \\ mobb/rosa-federated-prometheus Validate Prometheus Ensure the new Prometheus instance’s Pods are running\nkubectl get pods -n ${NAMESPACE} -l app=prometheus -o wide You should see the following:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES prometheus-federation-prometheus-0 3/3 Running 1 7m58s 10.131.0.104 ip-10-0-215-84.us-east-2.compute.internal \u003cnone\u003e \u003cnone\u003e prometheus-federation-prometheus-1 3/3 Running 1 7m58s 10.128.2.21 ip-10-0-146-85.us-east-2.compute.internal \u003cnone\u003e \u003cnone\u003e Log into the new Prometheus instance\nFetch the Route:\nkubectl -n ${NAMESPACE} get route prometheus-route You should see the following:\nNAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD prometheus-route prometheus-route-custom-prometheus.apps.mycluster.jnmf.p1.openshiftapps.com monitoring-prometheus-cr web-proxy reencrypt None\nOpen the Prometheus Route in your browser (the `HOST/PATH` field from above) It should take you through authorization and then you should see the Prometheus UI. 1. add `/targets` to the end of the URL to see the list of available targets ![screenshot of prometheus targets screen](./prom-targets.png) 1. Switch out the trailing path to be `graph?g0.range_input=1h\u0026g0.expr=kubelet_running_containers\u0026g0.tab=0` to see the graph of the number of running containers fetched from cluster monitoring. ![screenshot of prometheus graph screen](./prom-graph.png) 1. click on **Alerts** in the menu to see our example Alert ### Validate Alert Manager 1. forward a port to Alert Manager ```bash kubectl -n ${NAMESPACE} port-forward svc/monitoring-alertmanager-cr 9093:9093 Browse to http://localhost:9093/#/alerts to see the alert “ExampleAlert”\nValidate Grafana and Dashboards Find the Grafana Route\nkubectl get route grafana-route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD grafana-route grafana-route-federated-metrics.apps.metrics.9l1z.p1.openshiftapps.com / grafana-service grafana-proxy reencrypt None\n1. Log into grafana using your cluster's idp 1. Click login and login to Grafana as `admin` with the password you set when doing `helm install`. 1. Click on **Configuration** -\u003e **Datasources** and check that the prometheus data source is loaded. Sometimes due to Kubernetes resource ordering the Data Source may not be loaded. We can force the Operator to reload it by running `kubectl annotate -n $NAMESPACE grafanadatasources.integreatly.org federated reroll=true` 1. Click on **Dashboards** -\u003e **Manage** and click on the \"Use Method / Cluster\" dashboard. ![Screenshot of Grafana USE Dashboard](./grafana-use.png) ## Cleanup 1. Delete the helm release ```bash helm -n $NAMESPACE delete monitoring Delete the namespace\nkubectl delete namespace $NAMESPACE ","description":"","tags":["AWS","ROSA"],"title":"Federating Metrics to a centralized Prometheus Cluster","uri":"/docs/rosa/federated-metrics-prometheus/"},{"content":"","description":"","tags":null,"title":"GitOps","uri":"/tags/gitops/"},{"content":"The HashiCorp Vault Secret CSI Driver allows you to access secrets stored in HashiCorp Vault as Kubernetes Volumes.\nPrerequisites An OpenShift Cluster (ROSA, ARO, OSD, and OCP 4.x all work) kubectl helm v3 Installing the Kubernetes Secret Store CSI Create an OpenShift Project to deploy the CSI into\noc new-project k8s-secrets-store-csi Set SecurityContextConstraints to allow the CSI driver to run (otherwise the DaemonSet will not be able to create Pods)\noc adm policy add-scc-to-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver Add the Secrets Store CSI Driver to your Helm Repositories\nhelm repo add secrets-store-csi-driver \\ https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts Update your Helm Repositories\nhelm repo update Install the secrets store csi driver\nhelm install -n k8s-secrets-store-csi csi-secrets-store \\ secrets-store-csi-driver/secrets-store-csi-driver \\ --version v1.0.1 \\ --set \"linux.providersDir=/var/run/secrets-store-csi-providers\" Check that the Daemonsets is running\nkubectl --namespace=k8s-secrets-store-csi get pods -l \"app=secrets-store-csi-driver\" You should see the following\nNAME READY STATUS RESTARTS AGE csi-secrets-store-secrets-store-csi-driver-cl7dv 3/3 Running 0 57s csi-secrets-store-secrets-store-csi-driver-gbz27 3/3 Running 0 57s Install HashiCorp Vault with CSI driver enabled Add the HashiCorp Helm Repository\nhelm repo add hashicorp https://helm.releases.hashicorp.com Update your Helm Repositories\nhelm repo update Create a namespace for Vault\noc new-project hashicorp-vault Create a SCC for the CSI driver\noc adm policy add-scc-to-user privileged \\ system:serviceaccount:hashicorp-vault:vault-csi-provider Create a values file for Helm to use\ncat \u003c\u003c EOF \u003e values.yaml global: openshift: true csi: enabled: true daemonSet: providersDir: /var/run/secrets-store-csi-providers injector: enabled: false server: image: repository: \"registry.connect.redhat.com/hashicorp/vault\" tag: \"1.8.0-ubi\" dev: enabled: true EOF Install Hashicorp Vault with CSI enabled\nhelm install -n hashicorp-vault vault \\ hashicorp/vault --values values.yaml Patch the CSI daemonset\nCurrently the CSI has a bug in its manifest which we need to patch\nkubectl patch daemonset vault-csi-provider --type='json' \\ -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/securityContext\", \"value\": {\"privileged\": true} }]' Configure Hashicorp Vault Get a bash prompt inside the Vault pod\noc exec -it vault-0 -- bash Create a Secret in Vault\nvault kv put secret/db-pass password=\"hunter2\" Configure Vault to use Kubernetes Auth\nvault auth enable kubernetes Check your Cluster’s token issuer\noc get authentication.config cluster \\ -o json | jq -r .spec.serviceAccountIssuer Configure Kubernetes auth method\nIf the issuer here does not match the above, update it.\nvault write auth/kubernetes/config \\ issuer=\"https://kubernetes.default.svc.cluster.local\" \\ token_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\ kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" \\ kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt Create a policy for our app\nvault policy write internal-app - \u003c\u003cEOF path \"secret/data/db-pass\" { capabilities = [\"read\"] } EOF Create an auth role to access it\nvault write auth/kubernetes/role/database \\ bound_service_account_names=webapp-sa \\ bound_service_account_namespaces=default \\ policies=internal-app \\ ttl=20m exit from the vault-0 pod\nexit Deploy a sample application Create a SecretProviderClass in the default namespace\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: secrets-store.csi.x-k8s.io/v1alpha1 kind: SecretProviderClass metadata: name: vault-database namespace: default spec: provider: vault parameters: vaultAddress: \"http://vault.hashicorp-vault:8200\" roleName: \"database\" objects: | - objectName: \"db-password\" secretPath: \"secret/data/db-pass\" secretKey: \"password\" EOF Create a service account webapp-sa\nkubectl create serviceaccount -n default webapp-sa Create a Pod to use the secret\ncat \u003c\u003c EOF | kubectl apply -f - kind: Pod apiVersion: v1 metadata: name: webapp namespace: default spec: serviceAccountName: webapp-sa containers: - image: jweissig/app:0.0.1 name: webapp volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"vault-database\" EOF Check the Pod has the secret\nkubectl -n default exec webapp \\ -- cat /mnt/secrets-store/db-password The output should match\nhunter2 Uninstall HashiCorp Vault with CSI driver enabled Delete the pod and\nkubectl delete -n default pod webapp kubectl delete -n default secretproviderclass vault-database kubectl delete -n default serviceaccount webapp-sa Delete the Hashicorp Vault Helm\nhelm delete -n hashicorp-vault vault Delete the SCC for Hashicorp Vault\noc adm policy remove-scc-from-user privileged \\ system:serviceaccount:hashicorp-vault:vault-csi-provider Delete the Hashicorp vault project\noc delete project hashicorp-vault Uninstalling the Kubernetes Secret Store CSI Delete the secrets store csi driver\nhelm delete -n k8s-secrets-store-csi csi-secrets-store Delete the SecurityContextConstraints\noc adm policy remove-scc-from-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver ","description":"","tags":["ROSA","ARO","OSD","OCP"],"title":"Installing the HashiCorp Vault Secret CSI Driver","uri":"/docs/misc/secrets-store-csi/hashicorp-vault/"},{"content":"The Kubernetes Secret Store CSI is a storage driver that allows you to mount secrets from external secret management systems like HashiCorp Vault and AWS Secrets.\nIt comes in two parts, the Secret Store CSI, and a Secret provider driver. This document covers just the CSI itself.\nPrerequisites An OpenShift Cluster (ROSA, ARO, OSD, and OCP 4.x all work) kubectl helm v3 Installing the Kubernetes Secret Store CSI Create an OpenShift Project to deploy the CSI into\noc new-project k8s-secrets-store-csi Set SecurityContextConstraints to allow the CSI driver to run (otherwise the DaemonSet will not be able to create Pods)\noc adm policy add-scc-to-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver Add the Secrets Store CSI Driver to your Helm Repositories\nhelm repo add secrets-store-csi-driver \\ https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts Update your Helm Repositories\nhelm repo update Install the secrets store csi driver\nhelm install -n k8s-secrets-store-csi csi-secrets-store \\ secrets-store-csi-driver/secrets-store-csi-driver \\ --version v1.0.1 \\ --set \"linux.providersDir=/var/run/secrets-store-csi-providers\" Check that the Daemonsets is running\nkubectl --namespace=k8s-secrets-store-csi get pods -l \"app=secrets-store-csi-driver\" You should see the following\nNAME READY STATUS RESTARTS AGE csi-secrets-store-secrets-store-csi-driver-cl7dv 3/3 Running 0 57s csi-secrets-store-secrets-store-csi-driver-gbz27 3/3 Running 0 57s Uninstalling the Kubernetes Secret Store CSI Delete the secrets store csi driver\nhelm delete -n k8s-secrets-store-csi csi-secrets-store Delete the SecurityContextConstraints\noc adm policy remove-scc-from-user privileged \\ system:serviceaccount:k8s-secrets-store-csi:secrets-store-csi-driver Provider Specifics HashiCorp Vault\n","description":"","tags":["ARO","ROSA","OSD","OCP"],"title":"Installing the Kubernetes Secret Store CSI on OpenShift","uri":"/docs/misc/secrets-store-csi/"},{"content":"Integrating with AWS resources using Pod Identity Prerequisites ROSA CLI AWS CLI ROSA Cluster with STS ","description":"","tags":["AWS","ROSA","STS"],"title":"Integrating with AWS resources using Pod Identity","uri":"/docs/rosa/sts-and-pod-identity/"},{"content":"7/29/2021\nby Paul Czarkowski and JJ Asghar\nIntroduction Occasionally when you’re moving between major version of Kubernetes or Red Hat OpenShift, you’ll want to migrate your applications between clusters. Or if you’re moving between two clouds, you’ll want an easy way to migrate your workloads from one platform to another.\nThe Crane operator from the open source Konveyer project automates this migration process for you. The Konveyer site offers a selection of helpful projects to administer your cluster. Crane is designed to automate migration from one cluster to another and is surprisingly easy to get working.\nThis article shows you how we moved a default sample application from a Red Hat OpenShift on AWS (ROSA) to a Red Hat OpenShift on IBM Cloud (ROIC) cluster. To see how it’s done, watch the video or read the steps below.\nInstall the Crane operator First, log into the cluster console where your original application is hosted and also log into the console of the destination where you want to migrate your application. In our example, we logged into the OpenShift Service on AWS as our origin console and, in another tab, logged into to the Red Hat OpenShift on IBM Cloud console as our destination console.\nFrom the Operator Hub in both consoles, search for “Crane Operator” and follow the default prompts to install the operator.\nSet up your sample application From your origin cluster, choose the Developer profile and then click +Add to add a project where you will deploy your application into.\nChoose a sample app to play with. In our example, we chose the Python application. Name it and then click “Create”. It will pull the source information from GitHub and build an image, deploy the image, and expose it as a PHP endpoint.\nYou can change back from a Developer profile to the Admin profile in order to see if the operator has been installed correctly.\nCreate a migration (MIG) controller Now it’s time to create your migration controller.\nGo to your m migration cluster (in this example, our IBM console), select the Crane operator, and select Create migration controller. Do the same on the origin cluster (in our example, AWS).\nSwitch to the openshift-migration namespace.\nUpdate the host MIG controller.\napiVersion: migration.openshift.io/v1alpha1 kind: MigCluster metadata: name: host namespace: openshift-migration spec: isHostCluster: true Then you can apply your migration cluster.\nkubectl apply -f origin-migcluster.yaml NOTE: Run only this following command on the remote cluster Save your service account secret for the destination cluster.\noc sa get-token migration-controller -n openshift-migration | base64 -w 0 Write this into sa-secret-remote.yaml on your origin cluster:\napiVersion: v1 kind: Secret metadata: name: sa-token-remote namespace: openshift-config type: Opaque data: # [!] Change saToken to contain a base64 encoded SA token with cluster-admin # privileges on the remote cluster. # `oc sa get-token migration-controller -n openshift-migration | base64 -w 0` saToken: \u003cyour-base64-encoded-aws-sa-token-here\u003e kubectl apply -f sa-secret-remote.yaml Add your destination cluster:\napiVersion: migration.openshift.io/v1alpha1 kind: MigCluster metadata: name: src-ocp-3-cluster namespace: openshift-migration spec: insecure: true isHostCluster: false serviceAccountSecretRef: name: sa-token-remote namespace: openshift-config url: 'https://master.ocp3.mycluster.com/' kubectl apply -f dest-migcluster.yaml Configure s3 credentials to host migration storage. Included here is the correct access key for my files. You’ll need to have that handy.\napiVersion: v1 kind: Secret metadata: namespace: openshift-config name: migstorage-creds type: Opaque data: aws-access-key-id: aGVsbG8K aws-secret-access-key: aGVsbG8K kubectl apply -f mig-storage-creds.yaml Configure MIG storage to use s3\napiVersion: migration.openshift.io/v1alpha1 kind: MigStorage metadata: name: aws-s3 namespace: openshift-migration spec: backupStorageConfig: awsBucketName: konveyer-jj-migration # You need to change this for your s3 bucket credsSecretRef: name: migstorage-creds namespace: openshift-config backupStorageProvider: aws volumeSnapshotConfig: credsSecretRef: name: migstorage-creds namespace: openshift-config volumeSnapshotProvider: aws kubectl apply -f migstorage.yaml Create the migration plan. THe plan is essentially saying: “This is what I want”. The plan says what namespace you want to move. In this example, it’s project-a as referenced below.\napiVersion: migration.openshift.io/v1alpha1 kind: MigPlan metadata: name: migrate-project-a namespace: openshift-migration spec: destMigClusterRef: name: destination namespace: openshift-migration indirectImageMigration: true indirectVolumeMigration: true srcMigClusterRef: name: host namespace: openshift-migration migStorageRef: name: aws-s3 namespace: openshift-migration namespaces: - project-a # notice this is where you'd add other projects persistentVolumes: [] kubectl apply -f migplan.yaml Execute your migration plan.\napiVersion: migration.openshift.io/v1alpha1 kind: MigMigration metadata: name: migrate-project-a-execute namespace: openshift-migration spec: migPlanRef: name: migrate-project-a namespace: openshift-migration quiescePods: true stage: false kubectl apply -f migplanexecute.yaml Watch the magic Back in your original console, you can find the correct namespace and see that things have moved.\nGo back to the origin OpenShift console (AWS in our example). Bring up the migration GUI from the openshift-migration namespace. Check through the migration plans which show you the migration or watch the logs to see the migration happening.\nOr, watch the progress via the CLI if you prefer.\nkubectl logs -f migration-log-reader-\u003chash\u003e color You can also open your destination console (Red Hat OpenShift on IBM Cloud in our example) and see if the new cluster has migrated.\nConclusion Hopefully walking through these steps has helped you understand the power that Crane can offer you when migrating workloads between clusters. This was only a sample application. With a little work and testing, you should be able to leverage Crane for your applications. If you have any questions or thoughts, come around to #konveyer on the Kubernetes public Slack channel, and the team would me more then willing to help advise you.\nHappy migrating your apps!\n","description":"","tags":[],"title":"Migrate Kubernetes Applications with Konveyer Crane","uri":"/docs/redhat/crane/"},{"content":"","description":"","tags":null,"title":"Observability","uri":"/tags/observability/"},{"content":"OpenShift - Sharing Common images Paul Czarkowski\n21 June 2021\nIn OpenShift images (stored in the in-cluster registry) are protected by Kubernetes RBAC and by default only the namespace in which the image was built can access it.\nFor example if you build an image in project-a only project-a can use that image, or build from it. If you wanted the default service account in project-b to have access to the images in project-a you would run the following.\noc policy add-role-to-user \\ system:image-puller system:serviceaccount:project-b:default \\ --namespace=project-a However if you had to do this for every namespace it could become quite combersome. Instead if you choose to have a set of common images in a common-images namespace you could make them available to all authenticated users like so.\noc adm policy add-cluster-role-to-group system:image-puller \\ system:authenticated --namespace=common-images oc adm policy add-role-to-group view system:authenticated \\ -n common-images Note: It’s important to understand and accept the security implications that come with this. If any Pod in the cluster is compromised it will have access to pull any images in this namespace.\nSee Global Image Puller for an example Kubernetes Controller that may allow for a more surgical (but still automated) way to grant access to images.\n","description":"","tags":["OCP"],"title":"OpenShift - Sharing Common images","uri":"/docs/misc/common-images-namespace/"},{"content":"Author: Charlotte Fung\nLast edited: 09/05/2022\nAdopted from Official Documentation for Cost Management Service\nRed Hat Cost Management is a software as a service (SaaS) offering available free of charge as part of your Red Hat subscriptions. Cost management helps you monitor and analyze your OpenShift Container Platform and Public cloud costs in order to improve the management of your business.\nSome capabilities of cost management are :\nVisualize costs across hybrid cloud infrastructure Track cost trends Map charges to projects and organizations Normalize data and add markups with cost models Generate showback and chargeback information In this document, I will show you how to connect your OpenShift and Cloud provider sources to Cost Management in order to collect cost and usage.\nPrerequisites A Public Cloud subscritption (Azure Subscription) An OpenShift Cluster (to create an Azure Red Hat OpenShift (ARO) cluster, click here) Adding your OpenShift source to Cost Management Installing the Cost Management Metric Operator Log into the Openshift cluster web console with cluster-admin credentials\nOn the left navigation pane under Administator perspective, select Operators –\u003e OperatorHub\nSearch for and locate cost management metrics operator. Click on the displayed Cost Management Metrics Operator\nWhen the Install Operator window appears, you must select the costmanagement-metrics-operator namespace for installation. If it does not exist, it will be created for you. Click on install button.\nAfter a short wait, Cost Management Metrics Operator appears in the Installed Operators tab under Project: all Projects or Project: costmanagement-metrics-operator\nConfiguring the Operator instance for a new installation Once installed, click on the Cost Management\nIn the detail window, click + Create Instance\nA Cost Management Metrics Operator \u003e Create CostManagementMetricsConfig window appears\nClick the YAML view radio button to view and modify the contents of the YAML configuration file\nModify the following two lines in the YAML file to look like the following\nChange SOURCE-NAME to the new name of your source (ex. my-openshift-cost-source)\ncreate_source: true name: \u003cSOURCE-NAME\u003e Click the Create button. This creates a new source for cost management that will appear in the console.redhat.com Cost Management applications\nAdding your Microsoft Azure source to Cost Management 1. Creating a Microsoft Azure Source in your Red Hat account In the console.redhat.com click on All apps and services tab in the left top corner of the screen to navigate to this window. Click on Sources under Settings\nOn Sources page, click on Cloud sources tab and then click Add a source. This opens up the Sources Wizard\nEnter a name for your source and click next\nSelect cost management as the application and Microsoft Azure as the source type.\nClick Next. We will create the storage account and resource group in Azure account before proceeding. Keep this window open.\n2. Configuring your Microsoft Azure The following steps are required to configure your Azure account to be a cost management source\nCreating a storage account and resource group Configuring a storage account contributor and reader roles for access Scheduling daily exports 2.1 Creating an Azure resource group and storage account using Azure CLI First create a new resource group\naz group create \\ --name storage-resource-group \\ --location eastus If you’re not sure which region to specify for the --location parameter, you can retrieve a list of supported regions for your subscription with the az account list-locations command.\naz account list-locations \\ --query \"[].{Region:name}\" \\ --out table Next, create a standard general-purpose v2 storage account with read-access geo-redundant storage. Ensure the name of your storage account is unique across Azure\naz storage account create \\ --name \u003caccount-name\u003e \\ --resource-group storage-resource-group \\ --location eastus \\ --sku Standard_RAGRS \\ --kind StorageV2 Make note of the resource group and storage account. We will need them in the subsequent steps\nReturn to Sources wizard in console.redhat.com, enter the Resource group name and Storage account name and click Next. Leave this window for now and proceed to next step below.\n2.2 Configuring Azure roles using Azure CLI We need to grant cost management read-only access to Azure cost data by configuring a Storage Account Contributor and Reader role in Azure\nRun the following command to obtain your Subscription ID:\nSUBSCRIPTION=$(az account show --query \"{subscription_id: id}\" -o tsv) Return to the console.redhat.com Sources wizard, enter your Subscription ID. Click Next to move to the next screen\nIn Azure CLI, create a cost management Storage Account Contributor role, an obtain your tenant ID, client (application) ID, and client secret\naz ad sp create-for-rbac -n \"CostManagement\" \\ --role \"Storage Account Contributor\" \\ --query '{\"tenant\": tenant, \"client_id\": appId, \"secret\": password}' Return to Sources wizard in console.redhat.com, enter your Azure Tenant ID, Client ID, and Client Secret.\nRun the following command to create cost management Reader role with your subscription ID. Copy the full command from the Sources wizard, which will automatically substitute your Azure subscription ID obtained earlier.\naz role assignment create --role \"Cost Management Reader\" \\ --assignee http://CostManagement --subscription ${SUBSCRIPTION} Click Next in Sources wizard.\n2.3 Configuring a Daily Azure data export schedule using Azure Portal Cost management requires a data export from a Subscription level scope\nIn the Azure Portal home page, click on Subscriptions\nSelect the Subscription you want to track from the list, and then select Cost Analysis in the menu. At the top of the Cost analysis page, select configure subscription\nClick on the Export tab, and then Schedule export\nIn the Exports wizard, fill out the Export details\nFor Export Type, select Daily export of billing-period-to-date costs For Storage account, select the account you created earlier Enter any value for the Container name and Directory path for the export. These values provide the tree structure in the storage account where report files are stored.\nClick Create to start exporting data to the Azure storage container.\nReturn to Sources wizard after creating the export schedule and click Next. Review the source details\nClick Finish to complete adding the Azure source to cost management\nCost management will begin polling Azure for cost data, which will appear on the cost management dashboard (console.redhat.com/openshift/cost-management/).\nManaging your Costs After adding your Openshift Container Platform and Cloud Provider sources, Cost management will show cost data by\nSource\nCloud provider cost and usage related to running your OpenShift Container Platform clusters on their platform\nSee the following video for a quick overview of Cost Management for OpenShift followed by a demo of the product on YouTube\nNext steps for managing your costs Limiting access to cost management resources - Use role-based access control to limit visibility of resources in cost management reports.\nManaging cost data using tagging - Tags allow you to organize your resources by cost and allocate the costs to different parts of your cloud infrastructure\nUsing cost models - Configure cost models to associate prices to metrics and usage.\nVisualizing your costs using Cost Explorer - Allows you to see your costs through time.\n","description":"","tags":["Cost","Azure","ARO"],"title":"Red Hat Cost Management for Cloud Services","uri":"/docs/misc/cost-management/"},{"content":"Federating Metrics from ROSA/OSD is a bit tricky as the cluster metrics require pulling from its /federated endpoint while the user workload metrics require using the prometheus remoteWrite configuration.\nThis guide will walk you through using the MOBB Helm Chart to deploy the necessary agents to federate the metrics into AWS Prometheus and then use Grafana to visualize those metrics.\nAs a bonus it will set up a CloudWatch datasource to view any metrics or logs you have in Cloud Watch.\nMake sure to use a region where Amazon Prometheus service is supported\nPrerequisites A ROSA cluster deployed with STS aws CLI jq Set up environment Create environment variables\nexport CLUSTER=my-cluster export REGION=us-east-2 export PROM_NAMESPACE=custom-metrics export PROM_SA=aws-prometheus-proxy export SCRATCH_DIR=/tmp/scratch export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export AWS_PAGER=\"\" mkdir -p $SCRATCH_DIR Create namespace\noc new-project $PROM_NAMESPACE Deploy Operators Add the MOBB chart repository to your Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories\nhelm repo update Use the mobb/operatorhub chart to deploy the needed operators\nhelm upgrade -n $PROM_NAMESPACE custom-metrics-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/rosa-aws-prometheus/files/operatorhub.yaml Deploy and Configure the AWS Sigv4 Proxy and the Grafana Agent Create a Policy for access to AWS Prometheus\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/PermissionPolicyIngest.json { \"Version\": \"2012-10-17\", \"Statement\": [ {\"Effect\": \"Allow\", \"Action\": [ \"aps:RemoteWrite\", \"aps:GetSeries\", \"aps:GetLabels\", \"aps:GetMetricMetadata\" ], \"Resource\": \"*\" } ] } EOF Apply the Policy\nPROM_POLICY=$(aws iam create-policy --policy-name $PROM_SA-prom \\ --policy-document file://$SCRATCH_DIR/PermissionPolicyIngest.json \\ --query 'Policy.Arn' --output text) echo $PROM_POLICY Create a Policy for access to AWS CloudWatch\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/PermissionPolicyCloudWatch.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowReadingMetricsFromCloudWatch\", \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:DescribeAlarmsForMetric\", \"cloudwatch:DescribeAlarmHistory\", \"cloudwatch:DescribeAlarms\", \"cloudwatch:ListMetrics\", \"cloudwatch:GetMetricStatistics\", \"cloudwatch:GetMetricData\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowReadingLogsFromCloudWatch\", \"Effect\": \"Allow\", \"Action\": [ \"logs:DescribeLogGroups\", \"logs:GetLogGroupFields\", \"logs:StartQuery\", \"logs:StopQuery\", \"logs:GetQueryResults\", \"logs:GetLogEvents\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowReadingTagsInstancesRegionsFromEC2\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:DescribeTags\", \"ec2:DescribeInstances\", \"ec2:DescribeRegions\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowReadingResourcesForTags\", \"Effect\": \"Allow\", \"Action\": \"tag:GetResources\", \"Resource\": \"*\" } ] } EOF Apply the Policy\nCW_POLICY=$(aws iam create-policy --policy-name $PROM_SA-cw \\ --policy-document file://$SCRATCH_DIR/PermissionPolicyCloudWatch.json \\ --query 'Policy.Arn' --output text) echo $CW_POLICY Create a Trust Policy\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/TrustPolicy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_PROVIDER}:sub\": [ \"system:serviceaccount:${PROM_NAMESPACE}:${PROM_SA}\", \"system:serviceaccount:${PROM_NAMESPACE}:grafana-serviceaccount\" ] } } } ] } EOF Create Role for AWS Prometheus and CloudWatch\nPROM_ROLE=$(aws iam create-role \\ --role-name \"prometheus-$CLUSTER\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $PROM_ROLE Attach the Policies to the Role\naws iam attach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn arn:aws:iam::aws:policy/AmazonPrometheusQueryAccess aws iam attach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn $CW_POLICY Create an AWS Prometheus Workspace\nPROM_WS=$(aws amp create-workspace --alias $CLUSTER \\ --query \"workspaceId\" --output text) echo $PROM_WS Deploy AWS Prometheus Proxy Helm Chart\nhelm upgrade --install -n $PROM_NAMESPACE --set \"aws.region=$REGION\" \\ --set \"aws.roleArn=$PROM_ROLE\" --set \"fullnameOverride=$PROM_SA\" \\ --set \"aws.workspaceId=$PROM_WS\" \\ --set \"grafana-cr.serviceAccountAnnotations.eks\\.amazonaws\\.com/role-arn=$PROM_ROLE\" \\ aws-prometheus-proxy mobb/rosa-aws-prometheus Configure remoteWrite for user workloads\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: remoteWrite: - url: \"http://aws-prometheus-proxy.$PROM_NAMESPACE.svc.cluster.local:8005/workspaces/$PROM_WS/api/v1/remote_write\" EOF ## Verify Metrics are being collected 1. Access Grafana and check for metrics ```bash oc get route -n custom-metrics grafana-route -o jsonpath='{.status.ingress[0].host}' ``` 1. Browse to the URL provided in the above command and log in with your OpenShift Credentials 1. Enable Admin by hitting sign in and user `admin` and `password` 1. Browse to `/datasources` and verify that `cloudwatch` and `prometheus` are present If not, you may have hit a race condition that can be fixed by running the following then trying again ```bash kubectl delete grafanadatasources.integreatly.org aws-prometheus-proxy-prometheus helm upgrade --install -n $PROM_NAMESPACE --set \"aws.region=$REGION\" \\ --set \"aws.roleArn=$PROM_ROLE\" --set \"fullnameOverride=$PROM_SA\" \\ --set \"aws.workspaceId=$PROM_WS\" \\ --set \"grafana-cr.serviceAccountAnnotations.eks\\.amazonaws\\.com/role-arn=$PROM_ROLE\" \\ aws-prometheus-proxy mobb/rosa-aws-prometheus ``` 1. Browse to `/dashboards` and select the **custom-metrics**-\u003e**NodeExporter / Use Method / Cluster** dashboard ![example cluster metrics dashboard](./dashboard.png) ## Cleanup 1. Delete the `aws-prometheus-proxy` Helm Release ```bash helm delete -n custom-metrics aws-prometheus-proxy ``` 1. Delete the `custom-metrics-operators` Helm Release ```bash helm delete -n custom-metrics custom-metrics-operators ``` 1. Delete the `custom-metrics` namespace ```bash kubectl delete namespace custom-metrics ``` 1. Detach AWS Role Policies ```bash aws iam detach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn arn:aws:iam::aws:policy/AmazonPrometheusQueryAccess aws iam detach-role-policy \\ --role-name \"prometheus-$CLUSTER\" \\ --policy-arn $CW_POLICY ``` 1. Delete the custom Cloud Watch Policy ```bash aws iam delete-policy --policy-arn $CW_POLICY ``` 1. Delete the AWS Prometheus Role ```bash aws iam delete-role --role-name \"prometheus-$CLUSTER\" ``` 1. Delete AWS Prometheus Workspace ```bash aws amp delete-workspace --workspace-id $PROM_WS ``` ","description":"","tags":["AWS","ROSA"],"title":"ROSA - Federating Metrics to AWS Prometheus","uri":"/docs/rosa/cluster-metrics-to-aws-prometheus/"},{"content":"Note: It is recommended that you use the Cloud Front based guide unless you absolutely must use an ALB based solution.\nHere’s a good overview of AWS LB types and what they support\nProblem Statement Operator requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA)\nOperator does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF\nProposed Solution Loosely based off EKS instructions here - https://aws.amazon.com/premiumsupport/knowledge-center/eks-alb-ingress-aws-waf/\nDeploy secondary Ingress solution (+TLS +DNS) that uses an AWS ALB\nTodo Configure TLS + DNS for that Ingress (Lets Encrypt + WildCard DNS) Pre Requisites A ROSA / OSD on AWS cluster Helm 3 cli oc / kubectl AWS cli Disable AWS cli output paging\nexport AWS_PAGER=\"\" Set the ALB Controller version\nexport ALB_VERSION=\"v2.2.0\" Set the name of your cluster for lookup\nexport CLUSTER_NAME=\"waf-demo\" Deployment Create a new public ROSA cluster called waf-demo and make sure to set it to be multi-AZ enabled, or replace the cluster name variable with your own cluster name.\nAWS Load Balancer Controller AWS Load Balancer controller manages the following AWS resources\nApplication Load Balancers to satisfy Kubernetes ingress objects Network Load Balancers in IP mode to satisfy Kubernetes service objects of type LoadBalancer with NLB IP mode annotation\nCreate AWS Policy and Service Account\ncurl -so iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/${ALB_VERSION}/docs/install/iam_policy.json POLICY_ARN=$(aws iam create-policy --policy-name \"AWSLoadBalancerControllerIAMPolicy\" --policy-document file://iam-policy.json --query Policy.Arn --output text) echo $POLICY_ARN Create service account\naws iam create-user --user-name aws-lb-controller \\ --query User.Arn --output text Attach policy to user\naws iam attach-user-policy --user-name aws-lb-controller \\ --policy-arn ${POLICY_ARN} Create access key and save the output (Paste the AccessKeyId and SecretAccessKey into values.yaml)\naws iam create-access-key --user-name aws-lb-controller export AWS_ID=\u003cfrom above\u003e export AWS_KEY=\u003cfrom above\u003e Modify the VPC ID and cluster name in the values.yaml with the output from (replace poc-waf with your cluster name):\nVPC_ID=$(aws ec2 describe-vpcs --output json --filters \\ Name=tag-value,Values=\"${CLUSTER_NAME}*\" \\ --query \"Vpcs[].VpcId\" --output text) echo ${VPC_ID} Modify the subnet list in ingress.yaml with the output from: (replace poc-waf with your cluster name)\nSUBNET_IDS=$(aws ec2 describe-subnets --output json \\ --filters Name=tag-value,Values=\"${CLUSTER_NAME}-*public*\" \\ --query \"Subnets[].SubnetId\" --output text | sed 's/\\t/ /g') echo ${SUBNET_IDS} Add tags to those subnets (change the subnet ids in the resources line)\naws ec2 create-tags \\ --resources $(echo ${SUBNET_IDS}) \\ --tags Key=kubernetes.io/role/elb,Value= Key=kubernetes.io/cluster/${CLUSTER_NAME},Value=shared Create a namespace for the controller\nkubectl create ns aws-load-balancer-controller Apply CRDs\nkubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Add the helm repo and install the controller (install helm3 if not already)\nhelm repo add eks https://aws.github.io/eks-charts helm install -n aws-load-balancer-controller \\ aws-load-balancer-controller eks/aws-load-balancer-controller \\ --set \"env.AWS_ACCESS_KEY_ID=${AWS_ID}\" \\ --set \"env.AWS_SECRET_ACCESS_KEY=${AWS_KEY}\" \\ --set \"vpcID=${VPC_ID}\" \\ --set \"clusterName=${CLUSTER_NAME}\" \\ --set \"image.tag=${ALB_VERSION}\" \\ --create-namespace Deploy Sample Application Create a new application in OpenShift\noc new-project demo oc new-app https://github.com/sclorg/django-ex.git kubectl -n demo patch service django-ex -p '{\"spec\":{\"type\":\"NodePort\"}}' Create an Ingress to trigger an ALB\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: django-ex namespace: demo annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance # alb.ingress.kubernetes.io/subnets: subnet-0982bb73ca67d61de,subnet-0aa9967e8767d792f,subnet-0fd57669a80eb7596 alb.ingress.kubernetes.io/shield-advanced-protection: \"true\" # wafv2 arn to use # alb.ingress.kubernetes.io/wafv2-acl-arn: arn:aws:wafv2:us-east-2:660250927410:regional/webacl/waf-demo/6565d2a1-6d26-4b6b-b56f-1e996c7e9e8f labels: app: django-ex spec: rules: - host: foo.bar http: paths: - pathType: Prefix path: /* backend: service: name: django-ex port: number: 8080 Check the logs of the ALB controller\nkubectl logs -f deployment/aws-load-balancer-controller use the second address from the ingress to browse to the app\nkubectl -n demo get ingress curl -s --header \"Host: foo.bar\" k8s-demo-djangoex-49f31c1921-782305710.us-east-2.elb.amazonaws.com | head WAF time Create a WAF rule here https://console.aws.amazon.com/wafv2/homev2/web-acls/new and use the Core and SQL Injection rules. (make sure region matches us-east-2)\nView your WAF\naws wafv2 list-web-acls --scope REGIONAL --region us-east-2 | jq . set the waf annotation to match the ARN provided above (and uncomment it) then re-apply the ingress\nkubectl apply -f ingress.yaml test the app still works\ncurl -s --header \"Host: foo.bar\" --location \"k8s-demo-djangoex-49f31c1921-782305710.us-east-2.elb.amazonaws.com\" test the WAF denies a bad request\nYou should get a 403 Forbidden error\ncurl -X POST http://k8s-demo-djangoex-49f31c1921-782305710.us-east-2.elb.amazonaws.com -F \"user='\u003cscript\u003e\u003calert\u003eHello\u003e\u003c/alert\u003e\u003c/script\u003e'\" ","description":"","tags":["AWS","ROSA","OSD"],"title":"This is a POC of ROSA with a AWS WAF service","uri":"/docs/rosa/waf/alb/"},{"content":"This is a POC of ROSA with a AWS WAF service Non working (yet) instructions for using STS to manage the creds for ALB\nSee https://issues.redhat.com/browse/CTONET-858 for a similar request\nHere’s a good overview of AWS LB types and what they support\nProblem Statement Customer requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA)\nCustomer does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF\nProposed Solution Loosely based off EKS instructions here - https://aws.amazon.com/premiumsupport/knowledge-center/eks-alb-ingress-aws-waf/\nDeploy secondary Ingress solution (+TLS +DNS) that uses an AWS ALB\nConfigure TLS + DNS for that Ingress\nLets Encrypt + WildCard DNS Deployment AWS Load Balancer Controller AWS Load Balancer controller manages the following AWS resources\nApplication Load Balancers to satisfy Kubernetes ingress objects Network Load Balancers in IP mode to satisfy Kubernetes service objects of type LoadBalancer with NLB IP mode annotation\nConfigure STS Make sure your cluster has the pod identity webhook\nkubectl get mutatingwebhookconfigurations.admissionregistration.k8s.io pod-identity-webhook Create AWS Policy and Service Account\nPOLICY_ARN=$(aws iam create-policy --policy-name \"AWSLoadBalancerControllerIAMPolicy\" --policy-document file://iam-policy.json --query Policy.Arn --output text) echo $POLICY_ARN Create service account\nNote I had issues with the policy, and for now just gave this user admin creds. Need to revisit and figure out.\nSA_ARN=$(aws iam create-user --user-name aws-lb-controller --permissions-boundary=$POLICY_ARN --query User.Arn --output text) Create access key\nACCESS_KEY=$(aws iam create-access-key --user-name aws-lb-controller) Attach policy to user\nPaste the AccessKeyId and SecretAccessKey into values.yaml\ntag your public subnet with ``\nCreate a namespace for the controller\nkubectl create ns aws-load-balancer-controller Apply CRDs\nkubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Add the helm repo and install the controller (install helm3 if not already)\nhelm repo add eks https://aws.github.io/eks-charts helm install -n aws-load-balancer-controller \\ aws-load-balancer-controller eks/aws-load-balancer-controller \\ --values=./helm/values.yaml --create-namespace Deploy Sample Application oc new-project demo oc new-app https://github.com/sclorg/django-ex.git kubectl -n demo patch service django-ex -p '{\"spec\":{\"type\":\"NodePort\"}}' kubectl apply -f ingress.yaml ","description":"","tags":["AWS","ROSA"],"title":"This is a POC of ROSA with a AWS WAF service","uri":"/docs/rosa/waf/readme-complex/"},{"content":"Author Paul Czarkowski\nlast modified 2021-08-17\nThe AWS Secrets and Configuration Provider (ASCP) provides a way to expose AWS Secrets as Kubernetes storage volumes. With the ASCP, you can store and manage your secrets in Secrets Manager and then retrieve them through your workloads running on ROSA or OSD.\nThis is made even easier / more secure through the use of AWS STS and Kubernetes PodIdentity.\nPrerequisites A ROSA cluster deployed with STS Helm 3 aws CLI jq Preparing Environment Validate that your cluster has STS\noc get authentication.config.openshift.io cluster -o json \\ | jq .spec.serviceAccountIssuer You should see something like the following, if not you should not proceed, instead look to the Red Hat documentation on creating an STS cluster.\n\"https://rh-oidc.s3.us-east-1.amazonaws.com/xxxxxx\" Set SecurityContextConstraints to allow the CSI driver to run\noc new-project csi-secrets-store oc adm policy add-scc-to-user privileged \\ system:serviceaccount:csi-secrets-store:secrets-store-csi-driver oc adm policy add-scc-to-user privileged \\ system:serviceaccount:csi-secrets-store:csi-secrets-store-provider-aws Create some environment variables to refer to later\nexport ROSA_CLUSTER_NAME=my-cluster export ROSA_CLUSTER_ID=$(rosa describe cluster -c $ROSA_CLUSTER_NAME --output json | jq -r .id) export REGION=us-east-2 export OIDC_ENDPOINT=$(oc get authentication.config.openshift.io cluster -o json | jq .spec.serviceAccountIssuer) export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` export AWS_PAGER=\"\" Deploy the AWS Secrets and Configuration Provider Use Helm to register the secrets store csi driver\nhelm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts Update your Helm Repositories\nhelm repo update Install the secrets store csi driver\nhelm upgrade --install -n csi-secrets-store csi-secrets-store-driver secrets-store-csi-driver/secrets-store-csi-driver Deploy the AWS provider\nkubectl -n csi-secrets-store apply -f \\ https://raw.githubusercontent.com/rh-mobb/documentation/main/docs/security/secrets-store-csi/aws-provider-installer.yaml Check that both Daemonsets are running\nkubectl -n csi-secrets-store get ds \\ csi-secrets-store-provider-aws \\ csi-secrets-store-driver-secrets-store-csi-driver Creating a Secret and IAM Access Policies Create a secret in Secrets Manager\nSECRET_ARN=$(aws --region \"$REGION\" secretsmanager create-secret \\ --name MySecret --secret-string \\ '{\"username\":\"shadowman\", \"password\":\"hunter2\"}' \\ --query ARN --output text) echo $SECRET_ARN Create IAM Access Policy document\ncat \u003c\u003c EOF \u003e policy.json { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Action\": [ \"secretsmanager:GetSecretValue\", \"secretsmanager:DescribeSecret\" ], \"Resource\": [\"$SECRET_ARN\"] }] } EOF Create an IAM Access Policy\nPOLICY_ARN=$(aws --region \"$REGION\" --query Policy.Arn \\ --output text iam create-policy \\ --policy-name openshift-access-to-mysecret-policy \\ --policy-document file://policy.json) echo $POLICY_ARN Create IAM Role trust policy document\nNote you can use Conditions to lock down to a specific namespace or service account here. But for simplicity we’re keeping it open.\ncat \u003c\u003cEOF \u003e trust-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::$AWS_ACCOUNT_ID:oidc-provider/rh-oidc.s3.us-east-1.amazonaws.com/$ROSA_CLUSTER_ID\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\" } ] } EOF Create IAM Role\nROLE_ARN=$(aws iam create-role --role-name openshift-access-to-mysecret \\ --assume-role-policy-document file://trust-policy.json \\ --query Role.Arn --output text) echo $ROLE_ARN Attach Role to the Policy\naws iam attach-role-policy --role-name openshift-access-to-mysecret --policy-arn $POLICY_ARN Create an Application to use this secret Create an OpenShift project\noc new-project my-application Annotate the default service account to use the STS Role\noc annotate -n my-application serviceaccount default \\ eks.amazonaws.com/role-arn=$ROLE_ARN Create a secret provider class to access our secret\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: my-application-aws-secrets spec: provider: aws parameters: objects: | - objectName: \"MySecret\" objectType: \"secretsmanager\" EOF Create a Deployment using our secret\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: my-application labels: app: my-application spec: volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"my-application-aws-secrets\" containers: - name: my-application-deployment image: k8s.gcr.io/e2e-test-images/busybox:1.29 command: - \"/bin/sleep\" - \"10000\" volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true EOF Verify the Pod has the secret mounted\nkubectl exec -it my-application -- cat /mnt/secrets-store/MySecret Cleanup Delete application\noc delete project my-application Delete the secrets store csi driver\nhelm delete -n kube-system csi-secrets-store Delete the AWS provider\nkubectl -n kube-system delete -f \\ https://raw.githubusercontent.com/aws/secrets-store-csi-driver-provider-aws/main/deployment/aws-provider-installer.yaml Delete Security Context Constraints\noc adm policy remove-scc-from-user privileged \\ system:serviceaccount:kube-system:secrets-store-csi-driver oc adm policy remove-scc-from-user privileged \\ system:serviceaccount:kube-system:csi-secrets-store-provider-aws Delete AWS Roles and Policies\naws iam detach-role-policy --role-name openshift-access-to-mysecret --policy-arn $POLICY_ARN aws iam delete-role --role-name openshift-access-to-mysecret aws iam delete-policy --policy-arn $POLICY_ARN ","description":"","tags":["AWS","ROSA"],"title":"Using AWS Secrets Manager CSI on Red Hat OpenShift on AWS with STS","uri":"/docs/rosa/aws-secrets-manager-csi/"},{"content":"Problem Statement Operator requires WAF (Web Application Firewall) in front of their workloads running on OpenShift (ROSA)\nOperator does not want WAF running on OpenShift to ensure that OCP resources do not experience Denial of Service through handling the WAF\nProposed Solution Add a CustomDomain resource to the cluster using a wildcard DNS and TLS certificate.\nSet the Wildcard DNS CNAME’s to CloudFront and enable the CloudFront + WAF services to reverse proxy and inspect the traffic before sending it to the cluster.\nPreparation Create a cluster\nrosa create cluster --cluster-name poc-waf --multi-az \\ --region us-east-2 --version 4.7.9 --compute-nodes 3 \\ --machine-cidr 10.0.0.0/16 --service-cidr 172.30.0.0/16 \\ --pod-cidr 10.128.0.0/14 --host-prefix 23 When its ready create a admin user and follow the instructions to log in\nrosa create admin -c poc-waf Set some environment variables\nEMAIL=username.taken@gmail.com DOMAIN=waf.mobb.ninja Certificate and DNS Use certbot to create a wildcard cert\ncertbot certonly --manual \\ --preferred-challenges=dns \\ --email $EMAIL \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d \"*.$DOMAIN\" Follow Certbot’s instructions to create a DNS TXT record. certificate records will be saved on your system, in my case in /etc/letsencrypt/live/waf.mobb.ninja/. set that as an enviroment variable.\nCERTS=/etc/letsencrypt/live/waf.mobb.ninja Custom OpenShift Domain Create a project and add the certs as a secret\noc new-project my-custom-route oc create secret tls acme-tls --cert=$CERTS/fullchain1.pem --key=$CERTS/privkey1.pem Create a Custom Domain resource\ncat \u003c\u003c EOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: acme spec: domain: $DOMAIN certificate: name: acme-tls namespace: my-custom-route EOF Wait until your Custom Domain has an Endpoint\nwatch oc get customdomains AWS WAF + CloudFront Create a WAF rule here https://console.aws.amazon.com/wafv2/homev2/web-acls/new?region=us-east-2 and use the Core and SQL Injection rules and set it as a CloudFront distribution resource type.\nView your WAF\naws wafv2 list-web-acls --scope REGIONAL --region us-east-2 | jq . Add a certificate to ACM - https://us-east-2.console.aws.amazon.com/acm/home?region=us-east-1#/importwizard/, Paste in the cert, key, certchain from the files certbot game you.\nMake sure you create it in the US-EAST-1 region (otherwise cloud front can’t use it)\nLog into the AWS console and Create a Cloud Front distribution (make sure its the same region as your cluster).\nOrigin Domain Name: Origin Protocol Policy: HTTPS only Viewer Protocol Policy: Redirect HTTP to HTTPS Allowed HTTP Methods: GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE AWS WAF Web ACL: demo-waf-acl Alternate Domain Names: *. Custom SSL Certificate: Origin Request Policy: create a new policy whitelist: Origin, user-agent, referer, host (IMPORTANT) Hit Create then wait until the Status is Ready.\nDNS CNAME Create a CNAME in your DNS provider for *.\u003c$DOMAIN\u003e that points at the endpoint from the above status page. It should look something like d1vm7mfs9sc24l.cloudfront.net. Deploy an Application Create a new application\noc new-app --docker-image=docker.io/openshift/hello-openshift Create a route for the application\noc create route edge --service=hello-openshift hello-openshift-tls \\ --hostname hello.waf.mobb.ninja Test the WAF Make sure you can access your application with curl\ncurl https://hello.waf.mobb.ninja You should get a simple hello response\nHello OpenShift! Try do a XSS injection\ncurl -X POST https://hello.waf.mobb.ninja \\ -F \"user='\u003cscript\u003e\u003calert\u003eHello\u003e\u003c/alert\u003e\u003c/script\u003e'\" you should see this\n\u003c!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"\u003e \u003cHTML\u003e\u003cHEAD\u003e\u003cMETA HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=iso-8859-1\"\u003e \u003cTITLE\u003eERROR: The request could not be satisfied\u003c/TITLE\u003e \u003c/HEAD\u003e\u003cBODY\u003e \u003cH1\u003e403 ERROR\u003c/H1\u003e ","description":"","tags":["AWS","ROSA"],"title":"Using CloudFront + WAF","uri":"/docs/rosa/waf/cloud-front/"},{"content":"Steve Mirman\n8 November 2021\nThis guide focuses on how to synchronize Identity Provider (IDP) groups and users after configuring authentication in OpenShift Cluster Manager (OCM). For an IDP configuration example, please reference the Configure Azure AD as an OIDC identity provider for ROSA/OSD guide.\nTo set up group synchronization from Azure Active Directory (AD) to ROSA/OSD you must:\nDefine groups and assign users in Azure AD Add the required API permissions to the app registration in Azure AD Install the Group Sync Operator from the OpenShift Operator Hub Create and configure a new Group Sync instance Set a synchronization schedule Test the synchronization process Define groups and assign users in Azure AD To synchronize groups and users with ROSA/OSD they must exist in Azure AD\nCreate groups to syncronize with ROSA/OSD if they do not already exist\nCreate user IDs to synchronize with ROSA/OSD if they do not already exist\nAssign newly created users to the appropriate group\nAdd API Permissions to Azure AD App Registration The GroupSync job requires permissions on the Azure AD tenant beyond those of the OIDC IdP. For it to work, add the these entries:\nGroup.ReadAll GroupMember.ReadAll User.ReadAll ..under the ‘API Permissions’ menu item. These three should all be ‘Application’ rather than ‘Delegated’ and this will require clicking on ‘Grant admin consent’ button above the permissions list. When done, the screen should look like this:\n![API permissions](./images/grp-sync-api-perm.png) Install the Group Sync Operator from the OpenShift Operator Hub In the OpenShift Operator Hub find the Group Sync Operator\nInstall the operator in the group-sync-operator namespace\nCreate and configure a new Group Sync instance Create a new secret named azure-group-sync in the group-sync-operator namespace. For this you will need the following values:\nAZURE_TENANT_ID AZURE_CLIENT_ID AZURE_CLIENT_SECRET Using the OpenShift CLI, create the secret using the following format:\noc create secret generic azure-group-sync \\ --from-literal=AZURE_TENANT_ID=\u003cinsert-id\u003e \\ --from-literal=AZURE_CLIENT_ID=\u003cinsert-id\u003e \\ --from-literal=AZURE_CLIENT_SECRET=\u003cinsert-secret\u003e Create a new Group Sync instance in the group-sync-operator namespace\nSelect all the default YAML and replace is with a modified version of the the example below, customizingthe YAML to match the group names and save the configuration.\nSample YAML:\napiVersion: redhatcop.redhat.io/v1alpha1 kind: GroupSync metadata: name: azure-groupsync namespace: group-sync-operator spec: providers: - name: azure azure: credentialsSecret: name: azure-group-sync namespace: group-sync-operator groups: - rosa_admin - rosa_project_owner - rosa_viewer prune: false schedule: '* * * * *' Set a synchronization schedule The Group Sync Operator provides a cron-based scheduling parameter for specifying how often the groups and users should be synchronized. This can be set in the instance YAML file during initial configuration or at any time after.\nThe schedule setting of schedule: * * * * * would result in synchronization occuring every minute. It also supports the cron “slash” notation (e.g., “*/5 * * * *”, which would synchronize every five minutes).\nTesting the synchronization process Check to see if the Group Sync process has completed with a Condition: ReconcileSuccess message\nCheck to see that all the groups specified in the configuration YAML file show up in the ROSA/OSD Groups list\nValidate that all users specified in Azure AD also show up as members of the associated group in ROSA/OSD\nAdd a new user in Azure AD and assign it to the admin group\nVerify that the user now appears in ROSA/OSD (after the specified synchronization time)\nNow delete a user from the Azure AD admin group\nVerify the user has been deleted from the ROSA/OSD admin group\nBinding Groups to Roles The preceding steps provide a method to get group membership information into OpenShift, but the final step in translating that into user authorization control requires binding each group to a role or roles on the cluster. This can be done via the OCP web console by opening the Group detail, or by applying YAML via the CLI.\nAdditional Notes The prune key in the YAML controls how the sync handles groups that are removed from Azure AD. If they key isn’t present, the default value is false, which means that if a group is removed from Azure AD, it will still persist in OpenShift. If it is set to true, removal of a group from Azure AD will also remove the corresponding OpenShift Group.\nIf there is a need to have multiple GroupSync configurations against multiple providers, note that there is no “merge” functionality in the operator when it comes to group membership. If a group named ocp-admins is present in two directories with sync jobs, they will effectively overwrite each other each time the sync job runs. It is recommended to name groups intended for use on OCP such that they indicate from which directory they originate (e.g., azure-ocp-admins or something like contoso_ocp_admins in the case of multiple Azure AD providers). Bind multiple groups with the same permissions needs to the same Role or ClusterRole as needed.\n","description":"","tags":["Azure","ROSA"],"title":"Using Group Sync Operator with Azure Active Directory and ROSA","uri":"/docs/idp/az-ad-grp-sync/"},{"content":"This document shows how you can use the AWS Cloud Watch agent to scrape Prometheus endpoints and publish metrics to CloudWatch in a Red Hat OpenShift Container Platform (ROSA) cluster.\nIt pulls from The AWS documentation for installing the CloudWatch agent to Kubernetes and collections and publishes metrics for the Kubernetes API Server and provides a simple Dashboard to view the results.\nCurrently the AWS Cloud Watch Agent does not support pulling all metrics from the Prometheus federated endpoint, but the hope is that when it does we can ship all Cluster and User Workload metrics to CloudWatch.\nPrerequisites AWS CLI jq A ROSA Cluster Prepare AWS Account Turn off AWS CLI Paging\nexport AWS_PAGER=\"\" Set some environment variables\nChange these to suit your environment.\nexport CLUSTER_NAME=metrics export CLUSTER_REGION=us-east-2 export SCRATCH_DIR=/tmp/scratch mkdir -p $SCRATCH_DIR Create an AWS IAM User for Cloud Watch\naws iam create-user \\ --user-name $CLUSTER_NAME-cloud-watch \\ \u003e $SCRATCH_DIR/aws-user.json Fetch Access and Secret Keys for IAM User\naws iam create-access-key \\ --user-name $CLUSTER_NAME-cloud-watch \\ \u003e $SCRATCH_DIR/aws-access-key.json Attach Policy to AWS IAM User\naws iam attach-user-policy \\ --user-name $CLUSTER_NAME-cloud-watch \\ --policy-arn \"arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\" Deploy Cloud Watch Prometheus Agent Create a namespace for Cloud Watch\noc create namespace amazon-cloudwatch Download the Cloud Watch Agent Kubernetes manifests\nwget -O $SCRATCH_DIR/cloud-watch.yaml https://raw.githubusercontent.com/rh-mobb/documentation/main/docs/rosa/metrics-to-cloudwatch-agent/cloud-watch.yaml Update the Cloud Watch Agent Kubernetes manifests\nsed -i \"s/__cluster_name__/$CLUSTER_NAME/g\" $SCRATCH_DIR/cloud-watch.yaml sed -i \"s/__cluster_region__/$CLUSTER_REGION/g\" $SCRATCH_DIR/cloud-watch.yaml Provide AWS Creds to the Cloud Watch Agent\nAWS_ID=`cat $SCRATCH_DIR/aws-access-key.json | jq -r '.AccessKey.AccessKeyId'` AWS_KEY=`cat $SCRATCH_DIR/aws-access-key.json | jq -r '.AccessKey.SecretAccessKey'` echo \"[AmazonCloudWatchAgent]\\naws_access_key_id = $AWS_ID\\naws_secret_access_key = $AWS_KEY\" \\ \u003e $SCRATCH_DIR/credentials oc --namespace amazon-cloudwatch \\ create secret generic aws-credentials \\ --from-file=credentials=$SCRATCH_DIR/credentials Allow Cloud Watch Agent to run as Root user (inside the container)\noc -n amazon-cloudwatch adm policy \\ add-scc-to-user anyuid -z cwagent-prometheus Apply the Cloud Watch Agent Kubernetes manifests\noc apply -f $SCRATCH_DIR/cloud-watch.yaml Check the Pod is running\noc get pods -n amazon-cloudwatch You should see:\nNAME READY STATUS RESTARTS AGE cwagent-prometheus-54cd498c9c-btmjm 1/1 Running 0 60m Create Sample Dashboard Download the Sample Dashboard\nwget -O $SCRATCH_DIR/dashboard.json https://raw.githubusercontent.com/rh-mobb/documentation/main/docs/rosa/metrics-to-cloudwatch-agent/dashboard.json Update the Sample Dashboard\nsed -i \"s/__CLUSTER_NAME__/$CLUSTER_NAME/g\" $SCRATCH_DIR/dashboard.json sed -i \"s/__REGION_NAME__/$CLUSTER_REGION/g\" $SCRATCH_DIR/dashboard.json Browse to https://us-east-2.console.aws.amazon.com/cloudwatch\nCreate a Dashboard, call it “Kubernetes API Server”\nClick Actions-\u003eView/edit source\nPaste the JSON contents from $SCRATCH_DIR/dashboard.json into the text area\nView the dashboard\n","description":"","tags":["AWS","ROSA"],"title":"Using the AWS Cloud Watch agent to publish metrics to CloudWatch in ROSA","uri":"/docs/rosa/metrics-to-cloudwatch-agent/"},{"content":"Currently, the logging-addon is not working on ROSA STS clusters. This is due to permissions missing from the Operator itself. This is a work around to provide credentials to the addon.\nNote: Please see the official Red Hat KCS for more information.\nPrerequisites An STS based ROSA Cluster Workaround Uninstall the logging-addon from the cluster\nrosa uninstall addon -c \u003cmycluster\u003e cluster-logging-operator -y Create a IAM Trust Policy document\ncat \u003c\u003c EOF \u003e /tmp/trust-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:DescribeLogGroups\", \"logs:DescribeLogStreams\", \"logs:PutLogEvents\", \"logs:GetLogEvents\", \"logs:PutRetentionPolicy\", \"logs:GetLogRecord\" ], \"Resource\": \"arn:aws:logs:*:*:*\" } ] } EOF Create IAM Policy\nPOLICY_ARN=$(aws iam create-policy --policy-name \"RosaCloudWatchAddon\" --policy-document file:///tmp/trust-policy.json --query Policy.Arn --output text) echo $POLICY_ARN Create service account\naws iam create-user --user-name RosaCloudWatchAddon \\ --query User.Arn --output text Attach policy to user\naws iam attach-user-policy --user-name RosaCloudWatchAddon \\ --policy-arn ${POLICY_ARN} Create access key and save the output (Paste the AccessKeyId and SecretAccessKey into values.yaml)\naws iam create-access-key --user-name RosaCloudWatchAddon export AWS_ID=\u003cfrom above\u003e export AWS_KEY=\u003cfrom above\u003e Create a secret for the addon to use\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: instance namespace: openshift-logging stringData: aws_access_key_id: ${AWS_ID} aws_secret_access_key: ${AWS_KEY} EOF Install the logging-addon from the cluster\nrosa install addon -c \u003cmycluster\u003e cluster-logging-operator -y Accept the defaults (or change them as appropriate)\n? Use AWS CloudWatch: Yes ? Collect Applications logs: Yes ? Collect Infrastructure logs: Yes ? Collect Audit logs (optional): No ? CloudWatch region (optional): I: Add-on 'cluster-logging-operator' is now installing. To check the status run 'rosa list addons -c mycluster' ","description":"","tags":["AWS","ROSA","STS"],"title":"Work Around to fix the issue with the logging-addon on ROSA STS Clusters","uri":"/docs/rosa/sts-cluster-logging-addon/"},{"content":"Kevin Collins\n06/28/2022\nOne of the advantages of using OpenShift is the internal registry that comes with OpenShfit to build, deploy and manage container images locally. By default, access to the registry is limited to the cluster ( by design ) but can be extended to usage outside of the cluster. This guide will go through the steps required to access the OpenShift Registry on an ARO cluster outside of the cluster.\nPrerequisites an ARO Cluster oc cli podman or docker cli Expose the Registry Expose the registry service\noc create route reencrypt --service=image-registry -n openshift-image-registry Annotate the route\noc annotate route image-registry haproxy.router.openshift.io/balance=source -n openshift-image-registry Get the route host name\nHOST=$(oc get route image-registry -n openshift-image-registry --template='{{ .spec.host }}') Log into the image registry\npodman docker login -u $(oc whoami) -p $(oc whoami -t) $HOST Test it out podman pull openshift/hello-openshift podman images expected output\nopenshift/hello-openshift latest 7af3297a3fb4 4 years ago 6.09MB ","description":"","tags":["ARO","Azure"],"title":"Accessing the Internal Registry from ARO","uri":"/docs/aro/registry/"},{"content":"The is an example guide for creating a public ingress endpoint for a ROSA Private-Link cluster. Be aware of the security implications of creating a public subnet in your ROSA VPC this way.\nPrerequisites AWS CLI Rosa CLI v1.0.8 jq A ROSA PL cluster Getting Started Set some environment variables Set the following environment variables, changing them to suit your cluster.\nexport ROSA_CLUSTER_NAME=private-link # this should be a free CIDR inside your VPC export PUBLIC_CIDR=10.0.2.0/24 export AWS_PAGER=\"\" export EMAIL=username.taken@gmail.com export DOMAIN=public.aws.mobb.ninja export SCRATCH_DIR=/tmp/scratch mkdir -p $SCRATCH_DIR Create a public subnet If you followed the above instructions to create the ROSA Private-Link cluster, you should already have a public subnet in your VPC and can skip to tagging the subnet.\nGet a Private Subnet ID from the cluster.\nPRIVATE_SUBNET_ID=$(rosa describe cluster -c $ROSA_CLUSTER_NAME -o json \\ | jq -r '.aws.subnet_ids[0]') echo $PRIVATE_SUBNET_ID Get the VPC ID from the subnet ID.\nVPC_ID=$(aws ec2 describe-subnets --subnet-ids $PRIVATE_SUBNET_ID \\ --query 'Subnets[0].VpcId' --output text) echo $VPC_ID Get the Cluster Tag from the subnet\nTAG=$(aws ec2 describe-subnets --subnet-ids $PRIVATE_SUBNET_ID \\ --query 'Subnets[0].Tags[?Value == `shared`]' | jq -r '.[0].Key') echo $TAG Create a public subnet\nPUBLIC_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block $PUBLIC_CIDR \\ --query 'Subnet.SubnetId' --output text` echo $PUBLIC_SUBNET Tag the public subnet for the cluster\naws ec2 create-tags --resources $PUBLIC_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-public \\ Key=$TAG,Value=\"shared\" Key=kubernetes.io/role/elb,Value=\"true\" Create a Custom Domain Create TLS Key Pair for custom domain using certbot:\nSkip this if you already have a key pair.\ncertbot certonly --manual \\ --preferred-challenges=dns \\ --email $EMAIL \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --config-dir \"$SCRATCH_DIR/config\" \\ --work-dir \"$SCRATCH_DIR/work\" \\ --logs-dir \"$SCRATCH_DIR/logs\" \\ -d \"*.$DOMAIN\" Create TLS secret for custom domain:\nNote use your own keypair paths if not using certbot.\nCERTS=/tmp/scratch/config/live/$DOMAIN oc new-project my-custom-route oc create secret tls acme-tls --cert=$CERTS/fullchain.pem --key=$CERTS/privkey.pem Create Custom Domain resource:\ncat \u003c\u003c EOF | oc apply -f - apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: acme spec: domain: $DOMAIN certificate: name: acme-tls namespace: my-custom-route EOF Wait for the domain to be ready:\nwatch oc get customdomains Once its ready grab the CLB name:\nCDO_NAME=acme CLB_NAME=$(oc get svc -n openshift-ingress -o jsonpath='{range .items[?(@.metadata.labels.ingresscontroller\\.operator\\.openshift\\.io\\/owning-ingresscontroller==\"'$CDO_NAME'\")]}{.status.loadBalancer.ingress[].hostname}{\"\\n\"}{end}') echo $CLB_NAME Create a CNAME in your DNS provider for *.\u003c$DOMAIN\u003e that points at the CLB NAME from the above command.\nDeploy a public application Create a new project\noc new-project my-public-app Create a new application\noc new-app --docker-image=docker.io/openshift/hello-openshift Create a route for the application\noc create route edge --service=hello-openshift hello-openshift-tls \\ --hostname hello.$DOMAIN Check that you can access the application:\ncurl https://hello.$DOMAIN You should see the output\nHello OpenShift! ","description":"","tags":["AWS","ROSA","Private Link"],"title":"Adding a Public Ingress endpoint to a ROSA Private-Link Cluster","uri":"/docs/rosa/private-link/public-ingress/"},{"content":"Paul Czarkowski, Stuart Kirk, Anton Nesterov\n03/30/2022 (updated on 12/09/2022)\nPrerequisites an Azure Red Hat OpenShift cluster a DNS zone that you can easily modify Get Started Create some environment variables\nDOMAIN=custom.azure.mobb.ninja EMAIL=example@email.com SCRATCH_DIR=/tmp/aro Create a certificate for the ingress controller\ncertbot certonly --manual \\ --preferred-challenges=dns \\ --email $EMAIL \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d \"*.$DOMAIN\" \\ --config-dir \"$SCRATCH_DIR/config\" \\ --work-dir \"$SCRATCH_DIR/work\" \\ --logs-dir \"$SCRATCH_DIR/logs\" Create a secret for the certificate\noc create secret tls custom-tls \\ -n openshift-ingress \\ --cert=$SCRATCH_DIR/config/live/$DOMAIN/fullchain.pem \\ --key=$SCRATCH_DIR/config/live/$DOMAIN/privkey.pem Create an ingress controller\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: custom namespace: openshift-ingress-operator spec: domain: $DOMAIN nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker: \"\" routeSelector: matchLabels: type: custom defaultCertificate: name: custom-tls httpEmptyRequestsPolicy: Respond httpErrorCodePages: name: \"\" replicas: 3 EOF NOTE: By default the ingress controller is created with external scope. This means that the corresponding Azure Load Balancer will have a public frontend IP. If you wish to deploy a privately visible ingress controller add the following lines to the spec:\nspec: ... endpointPublishingStrategy: loadBalancer: scope: Internal type: LoadBalancerService ... Wait a few moments then get the EXTERNAL-IP of the new ingress controller\noc get -n openshift-ingress svc router-custom In case of an Externally (publicly) scoped ingress controller the output should look like:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE router-custom LoadBalancer 172.30.90.84 20.120.48.78 80:32160/TCP,443:32511/TCP 49s In case of an Internal (private) one:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE router-custom LoadBalancer 172.30.55.36 10.0.2.4 80:30475/TCP,443:30249/TCP 10s Optionally verify in the Azure portal or using CLI that the Load Balancer Service has gotten the new Frontend IP and two Load Balancing Rules - one for port 80 and another one for port 443. In case of an Internally scoped Ingress Controller the changes are to be observed within the Load Balancer that has the -internal suffix.\nCreate a wildcard DNS record pointing at the EXTERNAL-IP\nTest that the Ingress is working\nNOTE: For the Internal ingress controller, make sure that the test host has the necessary reachability to the VPC/subnet as well as the DNS resolver.\ncurl -s https://test.$DOMAIN | head \u003chtml\u003e \u003chead\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e Create a new project to deploy an application to\noc new-project demo Create a new application\noc new-app --docker-image=docker.io/openshift/hello-openshift Expose\ncat \u003c\u003c EOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: hello-openshift app.kubernetes.io/component: hello-openshift app.kubernetes.io/instance: hello-openshift type: custom name: hello-openshift-tls spec: host: hello.$DOMAIN port: targetPort: 8080-tcp tls: termination: edge insecureEdgeTerminationPolicy: Redirect to: kind: Service name: hello-openshift EOF Verify it works\ncurl https://hello.custom.azure.mobb.ninja Hello OpenShift! ","description":"","tags":["ARO","Azure"],"title":"Adding an additional ingress controller to an ARO cluster","uri":"/docs/aro/additional-ingress-controller/"},{"content":"Paul Czarkowski\n08/17/2022\nThis document shows how to set up infrastructure nodes in an ARO cluster and move infrastructure related workloads to them. This can help with larger clusters that have resource contention between user workloads and infrastructure workloads such as Prometheus.\nImportant note: Infrastructure nodes are billed at the same rates as your existing ARO worker nodes.\nYou can find the original (and more detailed) document describing the process for a self-managed OpenShift Container Platform cluster here\nPrerequisites Azure Red Hat OpenShift cluster Helm CLI Create Infra Nodes We’ll use the MOBB Helm Chart for adding ARO machinesets which defaults to creating infra nodes, it looks up an existing machineset to collect cluster specific settings and then creates a new machineset specific for infra nodes with the same settings.\nAdd the MOBB chart repository to your Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories\nhelm repo update Install the mobb/aro-machinesets Chart to create infra nodes\nhelm upgrade --install -n openshift-machine-api \\ infra mobb/aro-machinesets Wait for the new nodes to be available\nwatch oc get machines Moving Infra workloads Ingress You may choose this for any additional Ingress controllers you may have in the cluster, however if you application has very high Ingress resource requirements it may make sense to allow them to spread across the worker nodes, or even a dedicated MachineSet.\nSet the nodePlacement on the ingresscontroller to node-role.kubernetes.io/infra and increase the replicas to match the number of infra nodes\noc patch -n openshift-ingress-operator ingresscontroller default --type=merge \\ -p='{\"spec\":{\"replicas\":3,\"nodePlacement\":{\"nodeSelector\":{\"matchLabels\":{\"node-role.kubernetes.io/infra\":\"\"}},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/infra\",\"operator\":\"Exists\"}]}}}' Check the Ingress Controller Operator is starting pods on the new infra nodes\noc -n openshift-ingress get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES router-default-69f58645b7-6xkvh 1/1 Running 0 66s 10.129.6.6 cz-cluster-hsmtw-infra-aro-machinesets-eastus-3-l6dqw \u003cnone\u003e \u003cnone\u003e router-default-69f58645b7-vttqz 1/1 Running 0 66s 10.131.4.6 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r \u003cnone\u003e \u003cnone\u003e router-default-6cb5ccf9f5-xjgcp 1/1 Terminating 0 23h 10.131.0.11 cz-cluster-hsmtw-worker-eastus2-xj9qx \u003cnone\u003e \u003cnone\u003e Registry Set the nodePlacement on the registry to node-role.kubernetes.io/infra\noc patch configs.imageregistry.operator.openshift.io/cluster --type=merge \\ -p='{\"spec\":{\"affinity\":{\"podAntiAffinity\":{\"preferredDuringSchedulingIgnoredDuringExecution\":[{\"podAffinityTerm\":{\"namespaces\":[\"openshift-image-registry\"],\"topologyKey\":\"kubernetes.io/hostname\"},\"weight\":100}]}},\"logLevel\":\"Normal\",\"managementState\":\"Managed\",\"nodeSelector\":{\"node-role.kubernetes.io/infra\":\"\"},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/infra\",\"operator\":\"Exists\"}]}}' Check the Registry Operator is starting pods on the new infra nodes\noc -n openshift-image-registry get pods -l \"docker-registry\" -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES image-registry-84cbd76d5d-cfsw7 1/1 Running 0 3h46m 10.128.6.7 cz-cluster-hsmtw-infra-aro-machinesets-eastus-2-kljml \u003cnone\u003e \u003cnone\u003e image-registry-84cbd76d5d-p2jf9 1/1 Running 0 3h46m 10.129.6.7 cz-cluster-hsmtw-infra-aro-machinesets-eastus-3-l6dqw \u003cnone\u003e \u003cnone\u003e Cluster Monitoring Configure the cluster monitoring stack to use the infra nodes\nNote: This will override any other customizations to the cluster monitoring stack, so you may want to merge your existing customizations into this before running the command.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" prometheusOperator: {} grafana: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" telemeterClient: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" openshiftStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" thanosQuerier: nodeSelector: node-role.kubernetes.io/infra: \"\" tolerations: - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/infra\" operator: \"Exists\" EOF Check the OpenShift Monitoring Operator is starting pods on the new infra nodes\nsome Pods like prometheus-operator will remain on master nodes.\noc -n openshift-monitoring get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES alertmanager-main-0 6/6 Running 0 2m14s 10.128.6.11 cz-cluster-hsmtw-infra-aro-machinesets-eastus-2-kljml \u003cnone\u003e \u003cnone\u003e alertmanager-main-1 6/6 Running 0 2m46s 10.131.4.11 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r \u003cnone\u003e \u003cnone\u003e cluster-monitoring-operator-5bbfd998c6-m9w62 2/2 Running 0 28h 10.128.0.23 cz-cluster-hsmtw-master-1 \u003cnone\u003e \u003cnone\u003e grafana-599d4b948c-btlp2 3/3 Running 0 2m48s 10.131.4.10 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r \u003cnone\u003e \u003cnone\u003e kube-state-metrics-574c5bfdd7-f7fjk 3/3 Running 0 2m49s 10.131.4.8 cz-cluster-hsmtw-infra-aro-machinesets-eastus-1-vr56r \u003cnone\u003e \u003cnone\u003e ... ... ","description":"","tags":["ARO","Azure"],"title":"Adding infrastructure nodes to an ARO cluster","uri":"/docs/aro/add-infra-nodes/"},{"content":"Azure Policy helps to enforce organizational standards and to assess compliance at-scale. Azure Policy supports arc enabled kubernetes cluster with both build-in and custom policies to ensure kubernetes resources are compliant. This article demonstrates how to make Azure Redhat Openshift cluster compliant with azure policy.\nPrerequisites Azure CLI Openshift CLI Azure Openshift Cluster (ARO Cluster) Deploy Azure Policy Deploy Azure Arc and Enable Azure Policy Add-on az connectedk8s connect -n [Cluster_Name] -g [Resource_Group_Name] az k8s-extension create --cluster-type connectedClusters --cluster-name [Cluster_Name] --resource-group [Resource_Group_Name] --extension-type Microsoft.PolicyInsights --name azurepolicy Verify Azure Arc and Azure Policy Add-on oc get pod -n azure-arc NAME READY STATUS RESTARTS AGE cluster-metadata-operator-6d4b957d65-5ts9b 2/2 Running 0 3h31m clusterconnect-agent-d5d6c6848-kbmfc 3/3 Running 0 3h31m clusteridentityoperator-6f5bf5c94-6qxlm 2/2 Running 0 3h31m config-agent-54b48fb5d9-wll42 2/2 Running 0 3h31m controller-manager-69fd59cf7-lf2mf 2/2 Running 0 3h31m extension-manager-695f99c94d-q6zmw 2/2 Running 0 3h31m flux-logs-agent-88588c88-j9xf8 1/1 Running 0 3h31m kube-aad-proxy-74d5747967-jhxq2 2/2 Running 0 3h31m metrics-agent-854dfbdc74-948bn 2/2 Running 0 3h31m resource-sync-agent-77f8bb95d4-94dpm 2/2 Running 0 3h31m oc get pod -n gatekeeper-system NAME READY STATUS RESTARTS AGE gatekeeper-audit-bbdd45779-lbsmf 1/1 Running 0 3h25m gatekeeper-controller-manager-59864c84f4-25pt2 1/1 Running 0 3h25m gatekeeper-controller-manager-59864c84f4-sztck 1/1 Running 0 3h25m Demo a simple policy This policy will allow only images from a specific registry.\nOpen Azure Portal Policy Services Click on Assign Policy Select the subscription and ARO cluster resource group as the scope Select “Kubernetes cluster containers should only use allowed images” in the “policy definition” field Click Next -\u003e fill out namespace inclusion as [“test-policy”] -\u003e Allowed Registry Regex as “index.docker.io.+$” Save the result. The policy will take effect after around 30 minutes. oc get K8sAzureContainerAllowedImages #Get the latest policy oc get K8sAzureContainerAllowedImages azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636 -o yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sAzureContainerAllowedImages metadata: annotations: azure-policy-assignment-id: /subscriptions/${subscription_id}/resourceGroups/shaozhen-tf-rg/providers/Microsoft.Authorization/policyAssignments/9f9d73056d5f422bb3bbbc5f azure-policy-definition-id: /providers/Microsoft.Authorization/policyDefinitions/febd0533-8e55-448f-b837-bd0e06f16469 azure-policy-definition-reference-id: \"\" azure-policy-setdefinition-id: \"\" constraint-installed-by: azure-policy-addon creationTimestamp: \"2022-07-25T16:19:12Z\" generation: 2 labels: managed-by: azure-policy-addon name: azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636 resourceVersion: \"169521\" uid: 0e25efc6-0099-4e3c-86a9-a223dd01e13d spec: enforcementAction: deny match: excludedNamespaces: - kube-system - gatekeeper-system - azure-arc kinds: - apiGroups: - \"\" kinds: - Pod namespaces: - test-policy parameters: excludedContainers: [] imageRegex: index.docker.io.+$ Policy Engine denies images from quay.io oc run -ti --image quay.io/alpine test -- /bin/sh Error from server ([azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636] Container image quay.io/alpine for container test has not been allowed.): admission webhook \"validation.gatekeeper.sh\" denied the request: [azurepolicy-k8sazurecontainerallowedimages-5a0572fab674dfdfb636] Container image quay.io/alpine for container test has not been allowed. References Azure Policy Overview Azure Arc-enabled kubernetes Understand Azure Policy for Kubernetes Azure Arc-enabled kubernetes built-in policy ","description":"","tags":["ARO","Azure"],"title":"Apply Azure Policy to Azure Policy","uri":"/docs/aro/azure-policy/"},{"content":"This is a high level overview of disaster recovery options for Azure Red Hat OpenShift. It is not a detailed design, but rather a starting point for a more detailed design.\nWhat is Disaster Recovery (DR) Disaster Recovery is an umbrella term that includes the following:\nBackup (and restore!) Failover (and failback!) High Availability Disaster Avoidence The most important part of Disaster Recovery is the “Recovery”. Whatever your DR plan it must be tested and ideally performed on a semi-regular basis.\nYou can use RTO (Recovery Time Objective) and RPO (Recovery Point Objective) to help determine what level of DR is right for your company. These Objectives are often application dependent and may mean choosing full HA for one application, and Backup/Restore for another even if they’re both on the same OpenShift cluster.\nRecovery Time Objective (RTO) How long can your application be down without causing significant issues for your business. This can differ from application to application. Some applications may only affect internal staff while others may affect customers and revenue.\nIn general you will categorize your applications by priority and potential damage to the business and match your DR plans accordingly.\nRecovery Point Objective (RPO) How much data can you lose before significant damage is done to your business. The traditional backup strategy is Daily. If you can survive a loss of 24 hours of data, or you have an alternative way to restore that data then this is often good enough.\nCombined RTO / RPO When combined you will account for “how long can the application be offline” and “how much data can I lose”. If the answer zero or approaching zero for both then your DR strategy must be focussed around High Availabily and real time data replication.\nBackup In OpenShift it is not necessary to back up the cluster itself, but instead you back up the “active state” of your resources, any Persistent Volumes, and any backing services.\nAzure provides documentation on the basic Backup and Restore of the applications running on your ARO cluster.\nAzure also provides documentation on Backing up the various PaaS backing services that you may have connected to your applications such as Azure PostgreSQL.\nFailover An ARO cluster can be deployed into Multiple Availability Zones (AZs) in a single region. To protect your applications from region failure you must deploy your application into multiple ARO clusters across different regions. Here are some considerations:\nStart with a solid and tested Backup/Restore manual cutover DR solution Decide on RPO/RTO (Recovery Point Objective / Recovery Time Objective) for DR Decide whether your regions should be hot/hot, hot/warm, or hot/cold. Choose regions close to your consumers. choose two “paired” regions. Use global virtual network peering to connect your networks together. Use Front Door or Traffic Manager to route public traffic to the correct region. Enable geo-replication for container images (If using ACR). Remove service state from inside containers. Create a storage migration plan. Backup and Restore - Manual Cutover ( Hot / Cold ) Do you currently have the ability to do a point in time restore of Backups of your applications?\nCreate a backup of your Kubernetes cluster\nIf you restore these backups to a new cluster and manually cutover the DNS, will your applications be full functional?\nCreate backups of any regionally co-located resources (like Redis, Postgres, etc.).\nSome Azure PaaS services such as Azure Container Registry can replicate to another region which may assist in performing backups or restore. This replication is often one way, therefore a new replication relationship must be created from the new region to another for the next DR event.\nIf using DNS based failover, make sure TTLs are set to a suitable value.\nDetermine if Non-regionally co-located resources (such as SaaS products) have appropriate failover plans and ensure that any special networking arrangements are available at the DR region.\nFailover to an existing cluster in the DR region (Hot / Warm) In a Hot / Warm situation the destination cluster should be similar to the the source cluster, but for financial reasons may be smaller, or be single AZ. If this is the case you may either run the DR cluster with lower expectations on performance and resiliance with the idea of failing back to the original cluster ASAP, or you will expand the DR cluster to match the original cluster and turn the original cluster into the next DR site.\nIdeally Your applications and data should be replicated to the DR site and should be ready to switch over within a very short window.\nHigh Availability ( Hot / Hot ) In a Hot / Hot scenario you have two-way synchronous replication of your data. The end user can access the application in either site and have the exact same experience.\nARO Specific Example This following example will create two ARO clusters, each in its own Region. Virtual Network Peering is used to make it easier for resources to communicate for replication.\nCreate a Primary Cluster Set the following environment variables:\nAZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=ARO-DR-1 AZR_CLUSTER=ARODR1 AZR_PULL_SECRET=~/Downloads/pull-secret.txt NETWORK_SUBNET=10.0.0.0/20 CONTROL_SUBNET=10.0.0.0/24 MACHINE_SUBNET=10.0.1.0/24 FIREWALL_SUBNET=10.0.2.0/24 JUMPHOST_SUBNET=10.0.3.0/24 Complete the rest of the step to create networks and cluster following the Private ARO cluster\nCreate a Secondary Cluster Set the following environment variables:\nAZR_RESOURCE_LOCATION=centralus AZR_RESOURCE_GROUP=ARO-DR-2 AZR_CLUSTER=ARODR2 AZR_PULL_SECRET=~/Downloads/pull-secret.txt NETWORK_SUBNET=10.1.0.0/20 CONTROL_SUBNET=10.1.0.0/24 MACHINE_SUBNET=10.1.1.0/24 FIREWALL_SUBNET=10.1.2.0/24 JUMPHOST_SUBNET=10.1.3.0/24 Complete the rest of the step to create networks and cluster following the Private ARO cluster\nConnect the clusters via Virtual Network Peering Virtual network peering allows two Azure regions to connect to each other via a virtual network. Ideally you will use a Hub-Spoke topology and create appropriate firewalling in the Hub network but that is an excercise left for the reader and here we’re creating a simple open peering between the two networks.\nGet the ID of the two networks you created in the previous step.\nDR1_VNET=$(az network vnet show \\ --resource-group ARO-DR-1 \\ --name ARODR1-aro-vnet-eastus \\ --query id --out tsv) echo $DR1_VNET DR2_VNET=$(az network vnet show \\ --resource-group ARO-DR-2 \\ --name ARODR2-aro-vnet-centralus \\ --query id --out tsv) echo $DR2_VNET Create peering from the Primary network to the Secondary network.\naz network vnet peering create \\ --name primary-to-secondary \\ --resource-group ARO-DR-1 \\ --vnet-name ARODR1-aro-vnet-eastus \\ --remote-vnet $DR2_VNET \\ --allow-vnet-access Create peering from the Secondary network to the Primary network.\naz network vnet peering create \\ --name secondary-to-primary \\ --resource-group ARO-DR-2 \\ --vnet-name ARODR2-aro-vnet-centralus \\ --remote-vnet $DR1_VNET \\ --allow-vnet-access Verify that the Jump Host in the Primary region is able to reach the Jump Host in the Secondary region.\nssh -i $HOME/.ssh/id_rsa aro@$JUMP_IP ping 10.1.3.4 PING 10.1.3.4 (10.1.3.4) 56(84) bytes of data. 64 bytes from 10.1.3.4: icmp_seq=1 ttl=64 time=23.8 ms 64 bytes from 10.1.3.4: icmp_seq=2 ttl=64 time=23.10 ms ssh to jump host forwarding port 1337 as a socks proxy.\nssh -D 1337 -C -i $HOME/.ssh/id_rsa aro@$JUMP_IP configure localhost:1337 as a socks proxy in your browser and access the two consoles.\nFrom here the two clusters are visible to each other via their frontends. This means they can access eachother’s ingress endpoints, routes and Load Balancers, but not pod-to-pod. A PostgreSQL pod in the primary cluster could replicate to a PostgreSQL pod in the secondary cluster via a service of type LoadBalancer.\nCross Region Registry Replication Openshift comes with a local registry that is used for local builds etc, but it is likely that you use a centralized registry for your own applications and images. Ensure that your registry supports replication to the DR region. Ensure that you understand if it supports active/active replication or if its a one way replication.\nIn a Hot/Warm scenario where you’ll only ever use the DR region as a backup to the primary region its likely okay for one-way replication to be used.\nRedhat Quay Azure Container Registry (must use Premium SKU for geo-replication) Example - Create a ACR in the Primary Region Create a new ACR in the primary region.\naz acr create --resource-group ARO-DR-1 \\ --name acrdr1 --sku Premium Log into and push an Image to the ACR.\naz acr login --name acrdr1 docker pull mcr.microsoft.com/hello-world docker tag mcr.microsoft.com/hello-world acrdr1.azurecr.io/hello-world:v1 docker push acrdr1.azurecr.io/hello-world:v1 Replicate the registry to the DR2 region.\naz acr replication create --location centralus --registry acrdr1 Wait a few moments and then check the replication status.\naz acr replication show --name centralus --registry acrdr1 --query status Red Hat Advanced Cluster Management Advanced Cluster Management (ACM) is a set of tools that can be used to manage the lifecycle of multiple OpenShift clusters. ACM gives you a single view into your clusters and provides gitops style management of you workloads and also has compliance features.\nYou can run ACM from a central infrastructure (or your Primary DR) cluster and connect your ARO clusters to it.\nFailover for Application Ingress If you want to expose your Applications to the internet you can use Azure’s Front Door or Traffic Manager resources which you can use to fail the routing over to the DR site.\nHowever if you are running private clusters your choices are a bit more limited.\nYou could provide people with the application ingress URL for each cluster and expect them to know to use the DR one if the Primary is down You can add a custom domain (and TLS certificate) and use your internal DNS to switch from the Primary to the DR site. You can provision a Load Balancer in your network that you can point the custom domain at and use that to switch from the Primary to the DR site as needed. Example using simple DNS:\nCreate a new wildcard DNS record with a low TTL pointing to the Primary Cluster’s Ingress/Route ExternalIP in your private DNS zone. (in our case it was *.aro-dr.mobb.ninja)\nModify the route for both apache examples to use the new wildcard DNS record.\nTest access\nUpdate the DNS record to point to the DR site’s Ingress/Route ExternalIP.\nTest access\n","description":"","tags":["ARO","Azure"],"title":"ARO - Considerations for Disaster Recovery","uri":"/docs/aro/disaster-recovery/"},{"content":"ARO guide to deploying an ARO cluster with custom domain and automating certificate management with cert-manager and letsencrypt certificates to manage the *.apps and api endpoints.\nAuthor: Byron Miller\nPrerequisites az cli oc cli jq gettext OpenShift 4.10 domain name to use I’m going to be running this setup through Fedora in WSL2. Be sure to always use the same terminal/session for all commands since we’ll reference environment variables set or created through the steps.\nFedora Linux\nSee Azure Docs for alternative install options.\nImport the Microsoft Keys\nsudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository\ncat \u003c\u003c EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI\nsudo dnf install -y azure-cli Install jq \u0026 gettext\nI’m going to rely on using shell variables interpolated into Kubernetes config and jq to build variables. Installing or ensuring the gettext \u0026 jq package is installed will allow us to use envsubst to simplify some of our configuration so we can use output of CLI’s as input into Yamls to reduce the complexity of manual editing.\nsudo dnf install gettext jq Prepare Azure Account for Azure OpenShift Log into the Azure CLI by running the following and then authorizing through your Web Browser\naz login Register resource providers\naz provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait Get Red Hat pull secret Log into cloud.redhat.com\nBrowse to https://cloud.redhat.com/openshift/install/azure/aro-provisioned\nclick the Download pull secret button and remember where you saved it, you’ll reference it later.\nDeploy Azure OpenShift Variables and Resource Group Set some environment variables to use later, and create an Azure Resource Group.\nSet the following environment variables\nChange the values to suit your environment\nPULL_SECRET=./pull-secret.txt # the path to pull-secret LOCATION=southcentralus # the location of your cluster RESOURCEGROUP=aro-rg # the name of the resource group where you want to create your cluster CLUSTER=aro-cluster # the name of your cluster DOMAIN=lab.domain.com # Domain or subdomain zone for cluster \u0026 cluster api Create an Azure resource group\naz group create \\ --name $RESOURCEGROUP \\ --location $LOCATION Networking Create a virtual network with two empty subnets\nCreate virtual network\naz network vnet create \\ --resource-group $RESOURCEGROUP \\ --name aro-vnet \\ --address-prefixes 10.0.0.0/22 Create control plane subnet\naz network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name master-subnet \\ --address-prefixes 10.0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet\naz network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name worker-subnet \\ --address-prefixes 10.0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies on the control plane subnet\nThis is required for the service to be able to connect to and manage the cluster.\naz network vnet subnet update \\ --name master-subnet \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --disable-private-link-service-network-policies true Create the cluster\nThis will take between 30 and 45 minutes.\naz aro create \\ --resource-group $RESOURCEGROUP \\ --name $CLUSTER \\ --vnet aro-vnet \\ --master-subnet master-subnet \\ --worker-subnet worker-subnet \\ --pull-secret @$PULL_SECRET \\ --domain $DOMAIN Wait until the ARO cluster is fully provisioned.\nGet OpenShift console URL\nYou can use these to login to the web console (will get a cert warning that you can bypass with typing “thisisunsafe” in a chrome browser or login with oc)\naz aro show -g $RESOURCEGROUP -n $CLUSTER --query \"consoleProfile.url\" -o tsv Get OpenShift credentials\naz aro list-credentials --name $CLUSTER --resource-group $RESOURCEGROUP Create DNS Zones \u0026 Service Principal In order for cert-manager to work with AzureDNS, we need to create the zone and add a CAA record as well as create a Service Principal that we can use to manage records in this zone so CertManager can use DNS01 authentication for validating requests.\nThis zone should be a public zone since letsencrypt will need to be able to read records created here.\nIf you use a subdomain, please be sure to create the NS records in your primary domain to the subdomain.\nFor ease of management, we’re using the same resource group for domain as we have the cluster in.\nCreate Zone\naz network dns zone create -g $RESOURCEGROUP -n $DOMAIN You will need to configure your nameservers to point to azure. The output of running this zone create will show you the nameservers for this record that you will need to set up within your domain registrar.\nCreate API DNS record\nAPIREC=$(az aro show -g $RESOURCEGROUP -n $CLUSTER --query apiserverProfile.ip -o tsv) az network dns record-set a add-record -g $RESOURCEGROUP -z $DOMAIN \\ -n api -a $APIREC Create Wildcard DNS record\nWILDCARDREC=$(az aro show -n $CLUSTER -g $RESOURCEGROUP --query '{ingress:ingressProfiles[0].ip}' -o tsv) az network dns record-set a add-record -g $RESOURCEGROUP -z $DOMAIN \\ -n \"*.apps\" -a $WILDCARDREC Add CAA Record\nCAA is a type of DNS record that allows owners to specify which Certificate Authorities are allowed to issue certificates containing their domain names.\naz network dns record-set caa add-record -g $RESOURCEGROUP -z $DOMAIN \\ -n @ --flags 0 --tag \"issuewild\" --value \"letsencrypt.org\" You should be able to view the records in your console\nNote - You may have to create NS records in your root zone for a subdomain if you use a subdomain zone to point to the subdomains name servers.\nSet environment variables to build new service principal and credentials to allow cert-manager to create records in this zone.\nAZURE_CERT_MANAGER_NEW_SP_NAME = the name of the service principal to create that will manage the DNS zone automation for cert-manager.\nAZURE_CERT_MANAGER_NEW_SP_NAME=aro-dns-sp LETSENCRYPTEMAIL=youremail@work.com DNS_SP=$(az ad sp create-for-rbac --name $AZURE_CERT_MANAGER_NEW_SP_NAME --output json) AZURE_CERT_MANAGER_SP_APP_ID=$(echo $DNS_SP | jq -r '.appId') AZURE_CERT_MANAGER_SP_PASSWORD=$(echo $DNS_SP | jq -r '.password') AZURE_TENANT_ID=$(echo $DNS_SP | jq -r '.tenant') AZURE_SUBSCRIPTION_ID=$(az account show --output json | jq -r '.id') Restrict service principal - remove contributor role.\nNote: This may not exist, safe to proceed. We’re going to grant the DNS Management Role to it next.\naz role assignment delete --assignee $AZURE_CERT_MANAGER_SP_APP_ID --role Contributor Grant DNS Zone Contributor to our Service Principal\nWe’ll grant DNS Zone Contributor to our DNS Service principal.\naz role assignment create --assignee $AZURE_CERT_MANAGER_SP_APP_ID --role befefa01-2a29-4197-83a8-272ff33ce314 Assign service principal to DNS zone\nDNS_ID=$(az network dns zone show --name $DOMAIN --resource-group $RESOURCEGROUP --query \"id\" --output tsv) az role assignment create --assignee $AZURE_CERT_MANAGER_SP_APP_ID --role \"DNS Zone Contributor\" --scope $DNS_ID Login to Cluster Login to your cluster through oc cli\nYou may need to flush your local dns resolver/cache before you can see the DNS/Hostnames. On Windows you can open up a command prompt as administrator and type “ipconfig /flushdns”\napiServer=$(az aro show -g $RESOURCEGROUP -n $CLUSTER --query apiserverProfile.url -o tsv) loginCred=$(az aro list-credentials --name $CLUSTER --resource-group $RESOURCEGROUP --query \"kubeadminPassword\" -o tsv) oc login $apiServer -u kubeadmin -p $loginCred You may get a warning that the certificate isn’t trusted. We can ignore that now since we’re in the process of adding a trusted certificate.\nSet up Cert-Manager We’ll install cert-manager from operatorhub. If you experience any issues installing here, it probably means that you didn’t provide a pull-secret when you installed your ARO cluster.\nCreate namespace\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: annotations: openshift.io/display-name: Red Hat Certificate Manager Operator labels: openshift.io/cluster-monitoring: 'true' name: openshift-cert-manager-operator EOF Switch openshift-cert-manager-operator project (namespace)\noc project openshift-cert-manager-operator Create OperatorGroup\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: openshift-cert-manager-operator spec: {} EOF Create subscription for cert-manager operator\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: openshift-cert-manager-operator namespace: openshift-cert-manager-operator spec: channel: tech-preview installPlanApproval: Automatic name: openshift-cert-manager-operator source: redhat-operators sourceNamespace: openshift-marketplace EOF It will take a few minutes for this operator to install and complete it’s set up. May be a good time to take a coffee break :)\nWait for cert-manager operator to finish installing.\nOur next steps can’t complete until the operator has finished installing. I recommend that you login to your cluster with the URL and credentials you captured after you ran the az aro create and view the installed operators to see that everything is complete and successful.\nConfigure cert-manager LetsEncrypt We’re going to set up cert-manager to use DNS verification for letsencrypt certificates. We’ll need to generate a service principal that can update the DNS zone and create short term records needed to validate certificate requests and associate this service principal with the cluster issuer.\nConfigure Certificate Requestor Switch openshift-cert-manager project (namespace)\noc project openshift-cert-manager Create azuredns-config secret for storing service principal credentials to manage zone.\noc create secret generic azuredns-config --from-literal=client-secret=$AZURE_CERT_MANAGER_SP_PASSWORD -n openshift-cert-manager Create Cluster Issuer\nenvsubst \u003c\u003cEOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: $LETSENCRYPTEMAIL # This key doesn't exist, cert-manager creates it privateKeySecretRef: name: prod-letsencrypt-issuer-account-key solvers: - dns01: azureDNS: clientID: $AZURE_CERT_MANAGER_SP_APP_ID clientSecretSecretRef: name: azuredns-config key: client-secret subscriptionID: $AZURE_SUBSCRIPTION_ID tenantID: $AZURE_TENANT_ID resourceGroupName: $RESOURCEGROUP hostedZoneName: $DOMAIN environment: AzurePublicCloud EOF Describe issuer\noc describe clusterissuer letsencrypt-prod You should see some output that the issuer is Registered/Ready\nConditions: Last Transition Time: 2022-06-17T17:29:37Z Message: The ACME account was registered with the ACME server Observed Generation: 1 Reason: ACMEAccountRegistered Status: True Type: Ready Events: \u003cnone\u003e Once the above command is complete, you should be able to login to openshift, click view operators and make sure you’re in the “openshift-cert-manager-operator” project and you should see a screen like this. Again, if you have an ssl error and use a chrome browser - type “thisisunsafe” to get in if you get an error its an invalid cert.\nCreate \u0026 Install API Certificate Switch openshift-config project\noc project openshift-config Configure API certificate\nenvsubst \u003c\u003cEOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: openshift-api namespace: openshift-config spec: secretName: openshift-api-certificate issuerRef: name: letsencrypt-prod kind: ClusterIssuer dnsNames: - api.$DOMAIN EOF View certificate status\noc describe certificate openshift-api -n openshift-config Create cluster api cert job\nThis job will install the certificate\nenvsubst \u003c\u003cEOF | oc apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: patch-cluster-api-cert rules: - apiGroups: - \"\" resources: - secrets verbs: - get - list - apiGroups: - config.openshift.io resources: - apiservers verbs: - get - list - patch - update --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: patch-cluster-api-cert roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: patch-cluster-api-cert subjects: - kind: ServiceAccount name: patch-cluster-api-cert namespace: openshift-config --- apiVersion: v1 kind: ServiceAccount metadata: name: patch-cluster-api-cert --- apiVersion: batch/v1 kind: Job metadata: name: patch-cluster-api-cert annotations: argocd.argoproj.io/hook: PostSync argocd.argoproj.io/hook-delete-policy: HookSucceeded spec: template: spec: containers: - image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest env: - name: API_HOST_NAME value: api.$DOMAIN command: - /bin/bash - -c - | #!/usr/bin/env bash if oc get secret openshift-api-certificate -n openshift-config; then oc patch apiserver cluster --type=merge -p '{\"spec\":{\"servingCerts\": {\"namedCertificates\": [{\"names\": [\"'$API_HOST_NAME'\"], \"servingCertificate\": {\"name\": \"openshift-api-certificate\"}}]}}}' else echo \"Could not execute sync as secret 'openshift-api-certificate' in namespace 'openshift-config' does not exist, check status of CertificationRequest\" exit 1 fi name: patch-cluster-api-cert dnsPolicy: ClusterFirst restartPolicy: Never terminationGracePeriodSeconds: 30 serviceAccount: patch-cluster-api-cert serviceAccountName: patch-cluster-api-cert EOF Create \u0026 Install APPS Wildcard Certificate Switch openshift-ingress project (namespace)\noc project openshift-ingress Configure Wildcard Certificate\nenvsubst \u003c\u003cEOF | oc apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: openshift-wildcard namespace: openshift-ingress spec: secretName: openshift-wildcard-certificate issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: \"*.apps.$DOMAIN\" dnsNames: - \"*.apps.$DOMAIN\" EOF This will generate our API and wildcard certificate requests. We’ll now create two jobs that will install these certificates.\nView certificate status\noc describe certificate openshift-wildcard -n openshift-ingress Install Wildcard Certificate Job\ncat \u003c\u003cEOF | oc apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: patch-cluster-wildcard-cert rules: - apiGroups: - operator.openshift.io resources: - ingresscontrollers verbs: - get - list - patch - apiGroups: - \"\" resources: - secrets verbs: - get - list --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: patch-cluster-wildcard-cert roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: patch-cluster-wildcard-cert subjects: - kind: ServiceAccount name: patch-cluster-wildcard-cert namespace: openshift-ingress --- apiVersion: v1 kind: ServiceAccount metadata: name: patch-cluster-wildcard-cert --- apiVersion: batch/v1 kind: Job metadata: name: patch-cluster-wildcard-cert annotations: argocd.argoproj.io/hook: PostSync argocd.argoproj.io/hook-delete-policy: HookSucceeded spec: template: spec: containers: - image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest command: - /bin/bash - -c - | #!/usr/bin/env bash if oc get secret openshift-wildcard-certificate -n openshift-ingress; then oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch='{\"spec\": { \"defaultCertificate\": { \"name\": \"openshift-wildcard-certificate\" }}}' else echo \"Could not execute sync as secret 'openshift-wildcard-certificate' in namespace 'openshift-ingress' does not exist, check status of CertificationRequest\" exit 1 fi name: patch-cluster-wildcard-cert dnsPolicy: ClusterFirst restartPolicy: Never terminationGracePeriodSeconds: 30 serviceAccount: patch-cluster-wildcard-cert serviceAccountName: patch-cluster-wildcard-cert EOF Debugging One of the most helpful commands i’ve seen for debugging is in regards to challenges failing. The order says pending in perpetuity and you can run this to see why.\noc describe challenges This is a very helpful guide in debugging certificates as well.\nValidate Certificates It will take a few minutes for the jobs to successfully complete.\nOnce the certificate requests are complete, you should no longer see a browser security warning and you should have a valid SSL lock in your browser and no more warnings about SSL on cli.\nYou may want to open an InPrivate/Private browser tab to visit the console/api so you can see the new SSL cert without having to expire your cache.\nDelete Cluster Once you’re done its a good idea to delete the cluster to ensure that you don’t get a surprise bill.\nDelete the cluster\naz aro delete -y \\ --resource-group $RESOURCEGROUP \\ --name $CLUSTER Delete the Azure resource group\nOnly do this if there’s nothing else in the resource group.\naz group delete -y \\ --name $RESOURCEGROUP ","description":"","tags":["ARO","Azure"],"title":"ARO Custom domain with cert-manager and LetsEncrypt","uri":"/docs/aro/cert-manager/"},{"content":"A Quickstart guide to deploying an Azure Red Hat OpenShift cluster with IBM Cloud Paks 4 Data.\nAuthor: Kristopher White\nVideo Walkthrough If you prefer a more visual medium, you can watch [Kristopher White] walk through this quickstart on YouTube.\nPrerequisites Azure CLI Obviously you’ll need to have an Azure account to configure the CLI against.\nMacOS\nSee Azure Docs for alternative install options.\nInstall Azure CLI using homebrew\nbrew update \u0026\u0026 brew install azure-cli Linux\nSee Azure Docs for alternative install options.\nImport the Microsoft Keys\nsudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository\ncat \u003c\u003c EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI\nsudo dnf install -y azure-cli Prepare Azure Account for Azure OpenShift Log into the Azure CLI by running the following and then authorizing through your Web Browser\naz login Make sure you have enough Quota (change the location if you’re not using East US)\naz vm list-usage --location \"East US\" -o table see Addendum - Adding Quota to ARO account if you have less than 36 Quota left for Total Regional vCPUs.\nRegister resource providers\naz provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait Get Red Hat pull secret This step is optional, but highly recommended\nLog into https://console.redhat.com\nBrowse to https://console.redhat.com/openshift/install/azure/aro-provisioned\nclick the Download pull secret button and remember where you saved it, you’ll reference it later.\nDeploy Azure OpenShift Variables and Resource Group Set some environment variables to use later, and create an Azure Resource Group.\nSet the following environment variables\nChange the values to suit your environment, but these defaults should work.\nAZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=openshift AZR_CLUSTER=cluster AZR_PULL_SECRET=~/Downloads/pull-secret.txt Create an Azure resource group\naz group create \\ --name $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION Networking Create a virtual network with two empty subnets\nCreate virtual network\naz network vnet create \\ --address-prefixes 10.0.0.0/22 \\ --name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP Create control plane subnet\naz network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.0.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet\naz network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes 10.0.2.0/23 \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies on the control plane subnet\nThis is required for the service to be able to connect to and manage the cluster.\naz network vnet subnet update \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --disable-private-link-service-network-policies true Create the cluster\nThis will take between 30 and 45 minutes.\naz aro create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER \\ --vnet \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --master-subnet \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-subnet \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-vm-size Standard_D16s_v3 \\ --pull-secret @$AZR_PULL_SECRET Get OpenShift console URL\naz aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query consoleProfile Get OpenShift credentials\naz aro list-credentials \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv Use the URL and the credentials provided by the output of the last two commands to log into OpenShift via a web browser.\nCloud Paks 4 Data Operator Setup Adding IBM Operator Catalog to Openshift Log into the OpenShift web console with your OpenShift cluster admin credentials.\nIn the top banner, click the plus (+) icon to open the Import YAML dialog box.\nPaste this resource definition into the dialog box:\napiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: ibm-operator-catalog namespace: openshift-marketplace spec: displayName: IBM Operator Catalog image: 'icr.io/cpopen/ibm-operator-catalog:latest' publisher: IBM sourceType: grpc updateStrategy: registryPoll: interval: 45m Click Create. IBM Cloud Paks 4 Data Operator Install Log into the OpenShift web console with your OpenShift cluster admin credentials.\nMake sure you have selected the Administrator view.\nClick Operators \u003e OperatorHub \u003e Integration \u0026 Delivery.\nSearch for and click the tile for the IBM Cloud Pak for Integration operator.\nClick Install.\nIn the Install Operator pane:\nSelect the latest update channel.\nSelect the option to install Cloud Pak for Integration in one namespace or for all namespaces on your cluster. If in doubt, choose the All namespaces on the cluster installation mode, and accept the default Installed Namespace.\nSelect the Automatic approval strategy.\nClick Install.\nSuccessful Install Delete Cluster Once you’re done its a good idea to delete the cluster to ensure that you don’t get a surprise bill.\nDelete the cluster\naz aro delete -y \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER Delete the Azure resource group\nOnly do this if there’s nothing else in the resource group.\naz group delete -y \\ --name $AZR_RESOURCE_GROUP ","description":"","tags":["ARO","Azure"],"title":"ARO IBM Cloud Paks 4 Data","uri":"/docs/aro/ibm-cloud-paks-for-data/"},{"content":"ARO guide to running Nvidia GPU workloads.\nAuthor: Byron Miller, Stuart Kirk\nPrerequisites oc cli jq, moreutils, and gettext package ARO 4.10 If you need to install an ARO cluster, please read our ARO Quick start guide. Please be sure if you’re installing or using an existing ARO cluster that it is 4.10.x or higher.\nAs of OpenShift 4.10, it is no longer necessary to set up entitlements to use the nVidia Operator. This has greatly simplified the setup of the cluster for GPU workloads.\nLinux:\nsudo dnf install jq moreutils gettext MacOS\nbrew install jq moreutils gettext Helm Prerequisites If you plan to use Helm to deploy the GPU operator, you will need do the following\nAdd the MOBB chart repository to your Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories\nhelm repo update GPU Quota All GPU quotas in Azure are 0 by default. You will need to login to the azure portal and request GPU quota. There is a lot of competition for GPU workers, so you may have to provision an ARO cluster in a region where you can actually reserve GPU. ARO supports the following GPU workers:\nNC4as T4 v3 NC8as T4 v3 NC16as T4 v3 NC64as T4 v3 Please remember that when you request quota that Azure is per core. To request a single NC4as T4 v3 node, you will need to request quota in groups of 4. If you wish to request an NC16as T4 v3 you will need to request quota of 16.\nLogin to azure\nLogin to portal.azure.com, type “quotas” in search by, click on Compute and in the search box type “NCAsv3_T4”. Select the region your cluster is in (select checkbox) and then click Request quota increase and ask for quota (I chose 8 so i can build two demo clusters of NC4as T4s).\nConfigure quota\nLog in to your ARO cluster Login to OpenShift - we’ll use the kubeadmin account here but you can login with your user account as long as you have cluster-admin.\noc login \u003capiserver\u003e -u kubeadmin -p \u003ckubeadminpass\u003e Pull secret (Conditional) We’ll update our pull secret to make sure that we can install operators as well as connect to cloud.redhat.com.\nIf you have already re-created a full pull secret with cloud.redhat.com enabled you can skip this step\nUsing Helm Before Deploying the chart you need it to adopt the existing pull secret\nkubectl -n openshift-config annotate secret \\ pull-secret meta.helm.sh/release-name=pull-secret kubectl -n openshift-config annotate secret \\ pull-secret meta.helm.sh/release-namespace=openshift-config kubectl -n openshift-config label secret \\ pull-secret app.kubernetes.io/managed-by=Helm Download your new pull secret from https://console.redhat.com/openshift/downloads -\u003e Tokens -\u003e Pull secret and use it to update create the pull secret in your cluster.\nUpdate the pull secret\nThis chart will merge the in-cluster pull secret with the new pull secret.\nhelm upgrade --install pull-secret mobb/aro-pull-secret \\ -n openshift-config --set-file pullSecret=$HOME/Downloads/pull-secret.txt Enable Operator Hub\noc patch configs.samples.operator.openshift.io cluster --type=merge \\ -p='{\"spec\":{\"managementState\":\"Managed\"}}' oc patch operatorhub cluster --type=merge \\ -p='{\"spec\":{\"sources\":[ {\"name\":\"redhat-operators\",\"disabled\":false}, {\"name\":\"certified-operators\",\"disabled\":false}, {\"name\":\"community-operators\",\"disabled\":false}, {\"name\":\"redhat-marketplace\",\"disabled\":false} ]}}' Skip to GPU Machine Set\nManually Log into cloud.redhat.com\nBrowse to https://cloud.redhat.com/openshift/install/azure/aro-provisioned\nclick the Download pull secret button and save it as pull-secret.txt\nThe following steps will need to be ran in the same working directory as your pull-secret.txt\nExport existing pull secret\noc get secret pull-secret -n openshift-config -o json | jq -r '.data.\".dockerconfigjson\"' | base64 --decode \u003e export-pull.json Merge downloaded pull secret with system pull secret to add cloud.redhat.com\njq -s '.[0] * .[1]' export-pull.json pull-secret.txt | tr -d \"\\n\\r\" \u003e new-pull-secret.json Upload new secret file\noc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=new-pull-secret.json You may need to wait for about ~1hr for everything to sync up with cloud.redhat.com.\nDelete secrets\nrm pull-secret.txt export-pull.json new-pull-secret.json GPU Machine Set ARO still uses Kubernetes Machinsets to create a machine set. I’m going to export the first machine set in my cluster (az 1) and use that as a template to build a single GPU machine in southcentralus region 1.\nHelm Create a new machine-set (replicas of 1), see the Chart’s values file for configuration options\nhelm upgrade --install -n openshift-machine-api \\ gpu mobb/aro-gpu Wait for the new GPU nodes to be available\nwatch oc get machines Skip to Install Nvidia GPU Operator\nManually View existing machine sets\nFor ease of set up, I’m going to grab the first machine set and use that as the one I will clone to create our GPU machine set.\nMACHINESET=$(oc get machineset -n openshift-machine-api -o=jsonpath='{.items[0]}' | jq -r '[.metadata.name] | @tsv') Save a copy of example machine set\noc get machineset -n openshift-machine-api $MACHINESET -o json \u003e gpu_machineset.json Change the .metadata.name field to a new unique name\nI’m going to create a unique name for this single node machine set that shows nvidia-worker- that follows a similar pattern as all the other machine sets.\njq '.metadata.name = \"nvidia-worker-southcentralus1\"' gpu_machineset.json| sponge gpu_machineset.json Ensure spec.replicas matches the desired replica count for the MachineSet\njq '.spec.replicas = 1' gpu_machineset.json| sponge gpu_machineset.json Change the .spec.selector.matchLabels.machine.openshift.io/cluster-api-machineset field to match the .metadata.name field\njq '.spec.selector.matchLabels.\"machine.openshift.io/cluster-api-machineset\" = \"nvidia-worker-southcentralus1\"' gpu_machineset.json| sponge gpu_machineset.json Change the .spec.template.metadata.labels.machine.openshift.io/cluster-api-machineset to match the .metadata.name field\njq '.spec.template.metadata.labels.\"machine.openshift.io/cluster-api-machineset\" = \"nvidia-worker-southcentralus1\"' gpu_machineset.json| sponge gpu_machineset.json Change the spec.template.spec.providerSpec.value.vmSize to match the desired GPU instance type from Azure.\nThe machine we’re using is Standard_NC4as_T4_v3.\njq '.spec.template.spec.providerSpec.value.vmSize = \"Standard_NC4as_T4_v3\"' gpu_machineset.json | sponge gpu_machineset.json Change the spec.template.spec.providerSpec.value.zone to match the desired zone from Azure\njq '.spec.template.spec.providerSpec.value.zone = \"1\"' gpu_machineset.json | sponge gpu_machineset.json Delete the .status section of the yaml file\njq 'del(.status)' gpu_machineset.json | sponge gpu_machineset.json Verify the other data in the yaml file.\nCreate GPU machine set These steps will create the new GPU machine. It may take 10-15 minutes to provision a new GPU machine. If this step fails, please login to the azure portal and ensure you didn’t run across availability issues. You can go “Virtual Machines” and search for the worker name you created above to see the status of VMs.\nCreate GPU Machine set\noc create -f gpu_machineset.json This command will take a few minutes to complete.\nVerify GPU machine set\nMachines should be getting deployed. You can view the status of the machine set with the following commands\noc get machineset -n openshift-machine-api oc get machine -n openshift-machine-api Once the machines are provisioned, which could take 5-15 minutes, machines will show as nodes in the node list.\noc get nodes You should see a node with the “nvidia-worker-southcentralus1” name it we created earlier.\nInstall Nvidia GPU Operator This will create the nvidia-gpu-operator name space, set up the operator group and install the Nvidia GPU Operator.\nHelm Create namespaces\noc create namespace openshift-nfd oc create namespace nvidia-gpu-operator Use the mobb/operatorhub chart to deploy the needed operators\nhelm upgrade -n nvidia-gpu-operator nvidia-gpu-operator \\ mobb/operatorhub --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/nvidia-gpu/files/operatorhub.yaml Wait until the two operators are running\nwatch kubectl get pods -n openshift-nfd NAME READY STATUS RESTARTS AGE nfd-controller-manager-7b66c67bd9-rk98w 2/2 Running 0 47s watch oc get pods -n nvidia-gpu-operator NAME READY STATUS RESTARTS AGE gpu-operator-5d8cb7dd5f-c4ljk 1/1 Running 0 87s Install the Nvidia GPU Operator chart\n```bash helm upgrade --install -n nvidia-gpu-operator nvidia-gpu \\ mobb/nvidia-gpu --disable-openapi-validation Skip to Validate GPU\nManually Create Nvidia namespace\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: name: nvidia-gpu-operator EOF Create Operator Group\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: nvidia-gpu-operator-group namespace: nvidia-gpu-operator spec: targetNamespaces: - nvidia-gpu-operator EOF Get latest nvidia channel\nCHANNEL=$(oc get packagemanifest gpu-operator-certified -n openshift-marketplace -o jsonpath='{.status.defaultChannel}') Get latest nvidia package\nPACKAGE=$(oc get packagemanifests/gpu-operator-certified -n openshift-marketplace -ojson | jq -r '.status.channels[] | select(.name == \"'$CHANNEL'\") | .currentCSV') Create Subscription\nenvsubst \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: gpu-operator-certified namespace: nvidia-gpu-operator spec: channel: \"$CHANNEL\" installPlanApproval: Automatic name: gpu-operator-certified source: certified-operators sourceNamespace: openshift-marketplace startingCSV: \"$PACKAGE\" EOF Wait for Operator to finish installing\nDon’t proceed until you have verified that the operator has finished installing. It’s also a good point to ensure that your GPU worker is online.\nInstall Node Feature Discovery Operator The node feature discovery operator will discover the GPU on your nodes and appropriately label the nodes so you can target them for workloads. We’ll install the NFD operator into the opneshift-ndf namespace and create the “subscription” which is the configuration for NFD.\nOfficial Documentation for Installing Node Feature Discovery Operator\nSet up Name Space\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: name: openshift-nfd EOF Create OperatorGroup\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: generateName: openshift-nfd- name: openshift-nfd namespace: openshift-nfd EOF Create Subscription\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: nfd namespace: openshift-nfd spec: channel: \"stable\" installPlanApproval: Automatic name: nfd source: redhat-operators sourceNamespace: openshift-marketplace EOF Wait for Node Feature discovery to complete installation\nYou can login to your openshift console and view operators or simply wait a few minutes. The next step will error until the operator has finished installing.\nCreate NFD Instance\ncat \u003c\u003cEOF | oc apply -f - kind: NodeFeatureDiscovery apiVersion: nfd.openshift.io/v1 metadata: name: nfd-instance namespace: openshift-nfd spec: customConfig: configData: | # - name: \"more.kernel.features\" # matchOn: # - loadedKMod: [\"example_kmod3\"] # - name: \"more.features.by.nodename\" # value: customValue # matchOn: # - nodename: [\"special-.*-node-.*\"] operand: image: \u003e- registry.redhat.io/openshift4/ose-node-feature-discovery@sha256:07658ef3df4b264b02396e67af813a52ba416b47ab6e1d2d08025a350ccd2b7b servicePort: 12000 workerConfig: configData: | core: # labelWhiteList: # noPublish: false sleepInterval: 60s # sources: [all] # klog: # addDirHeader: false # alsologtostderr: false # logBacktraceAt: # logtostderr: true # skipHeaders: false # stderrthreshold: 2 # v: 0 # vmodule: ## NOTE: the following options are not dynamically run-time ## configurable and require a nfd-worker restart to take effect ## after being changed # logDir: # logFile: # logFileMaxSize: 1800 # skipLogHeaders: false sources: # cpu: # cpuid: ## NOTE: whitelist has priority over blacklist # attributeBlacklist: # - \"BMI1\" # - \"BMI2\" # - \"CLMUL\" # - \"CMOV\" # - \"CX16\" # - \"ERMS\" # - \"F16C\" # - \"HTT\" # - \"LZCNT\" # - \"MMX\" # - \"MMXEXT\" # - \"NX\" # - \"POPCNT\" # - \"RDRAND\" # - \"RDSEED\" # - \"RDTSCP\" # - \"SGX\" # - \"SSE\" # - \"SSE2\" # - \"SSE3\" # - \"SSE4.1\" # - \"SSE4.2\" # - \"SSSE3\" # attributeWhitelist: # kernel: # kconfigFile: \"/path/to/kconfig\" # configOpts: # - \"NO_HZ\" # - \"X86\" # - \"DMI\" pci: deviceClassWhitelist: - \"0200\" - \"03\" - \"12\" deviceLabelFields: # - \"class\" - \"vendor\" # - \"device\" # - \"subsystem_vendor\" # - \"subsystem_device\" # usb: # deviceClassWhitelist: # - \"0e\" # - \"ef\" # - \"fe\" # - \"ff\" # deviceLabelFields: # - \"class\" # - \"vendor\" # - \"device\" # custom: # - name: \"my.kernel.feature\" # matchOn: # - loadedKMod: [\"example_kmod1\", \"example_kmod2\"] # - name: \"my.pci.feature\" # matchOn: # - pciId: # class: [\"0200\"] # vendor: [\"15b3\"] # device: [\"1014\", \"1017\"] # - pciId : # vendor: [\"8086\"] # device: [\"1000\", \"1100\"] # - name: \"my.usb.feature\" # matchOn: # - usbId: # class: [\"ff\"] # vendor: [\"03e7\"] # device: [\"2485\"] # - usbId: # class: [\"fe\"] # vendor: [\"1a6e\"] # device: [\"089a\"] # - name: \"my.combined.feature\" # matchOn: # - pciId: # vendor: [\"15b3\"] # device: [\"1014\", \"1017\"] # loadedKMod : [\"vendor_kmod1\", \"vendor_kmod2\"] EOF Verify NFD is ready.\nThis operator should say Available in the status\nApply nVidia Cluster Config We’ll now apply the nvidia cluster config. Please read the nvidia documentation on customizing this if you have your own private repos or specific settings. This will be another process that takes a few minutes to complete.\nApply cluster config\ncat \u003c\u003cEOF | oc apply -f - apiVersion: nvidia.com/v1 kind: ClusterPolicy metadata: name: gpu-cluster-policy spec: migManager: enabled: true operator: defaultRuntime: crio initContainer: {} runtimeClass: nvidia deployGFD: true dcgm: enabled: true gfd: {} dcgmExporter: config: name: '' driver: licensingConfig: nlsEnabled: false configMapName: '' certConfig: name: '' kernelModuleConfig: name: '' repoConfig: configMapName: '' virtualTopology: config: '' enabled: true use_ocp_driver_toolkit: true devicePlugin: {} mig: strategy: single validator: plugin: env: - name: WITH_WORKLOAD value: 'true' nodeStatusExporter: enabled: true daemonsets: {} toolkit: enabled: true EOF Verify Cluster Policy\nLogin to OpenShift console and browse to operators and make sure you’re in nvidia-gpu-operator namespace. You should see it say State: Ready once everything is complete.\nValidate GPU It may take some time for the nVidia Operator and NFD to completely install and self-identify the machines. These commands can be ran to help validate that everything is running as expected.\nVerify NFD can see your GPU(s)\noc describe node | egrep 'Roles|pci-10de' | grep -v master You should see output like:\nRoles: worker feature.node.kubernetes.io/pci-10de.present=true Verify node labels\nYou can see the node labels by logging into the OpenShift console -\u003e Compute -\u003e Nodes -\u003e nvidia-worker-southcentralus1-. You should see a bunch of nvidia GPU labels and the pci-10de device from above.\nNvidia SMI tool verification\noc project nvidia-gpu-operator for i in $(oc get pod -lopenshift.driver-toolkit=true --no-headers |awk '{print $1}'); do echo $i; oc exec -it $i -- nvidia-smi ; echo -e '\\n' ; done You should see output that shows the GPUs available on the host such as this example screenshot. (Varies depending on GPU worker type)\nCreate Pod to run a GPU workload\noc project nvidia-gpu-operator cat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: cuda-vector-add spec: restartPolicy: OnFailure containers: - name: cuda-vector-add image: \"quay.io/giantswarm/nvidia-gpu-demo:latest\" resources: limits: nvidia.com/gpu: 1 nodeSelector: nvidia.com/gpu.present: true EOF View logs\noc logs cuda-vector-add --tail=-1 Please note, if you get an error “Error from server (BadRequest): container “cuda-vector-add” in pod “cuda-vector-add” is waiting to start: ContainerCreating” try running “oc delete pod cuda-vector-add” and then re-run the create statement above. I’ve seen issues where if this step is ran before all of the operator consolidation is done it may just sit there.\nYou should see Output like the following (mary vary depending on GPU):\n[Vector addition of 5000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done If successful, the pod can be deleted\noc delete pod cuda-vector-add ","description":"","tags":["ARO","Azure","GPU"],"title":"ARO with Nvidia GPU Workloads","uri":"/docs/aro/gpu/"},{"content":"Securing exposing an Internet facing application with a private ARO Cluster.\nWhen you create a cluster on ARO you have several options in making the cluster public or private. With a public cluster you are allowing Internet traffic to the api and *.apps endpoints. With a private cluster you can make either or both the api and .apps endpoints private.\nHow can you allow Internet access to an application running on your private cluster where the .apps endpoint is private? This document will guide you through using Azure Frontdoor to expose your applications to the Internet. There are several advantages of this approach, namely your cluster and all the resources in your Azure account can remain private, providing you an extra layer of security. Azure FrontDoor operates at the edge so we are controlling traffic before it even gets into your Azure account. On top of that, Azure FrontDoor also offers WAF and DDoS protection, certificate management and SSL Offloading just to name a few benefits.\nKevin Collins 06/16/2022\nAdopted from ARO Reference Architecture\nPrerequisites az cli oc cli a custom domain a DNS zone that you can easily modify To build and deploy the application\nmaven cli quarkus cli OpenJDK Java 8 Make sure to use the same terminal session while going through guide for all commands as we will reference envrionment variables set or created through the guide.\nGet Started Create a private ARO cluster.\nFollow this guide to Create a private ARO cluster or simply run this bash script\nSet Evironment Variables Manually set environment variables\nAROCLUSTER=\u003ccluster name\u003e ARORG=\u003cresource group for the cluster\u003e AFD_NAME=\u003cname you want to use for the front door instance\u003e DOMAIN='e.g. aro.kmobb.com' This is the domain that you will be adding to Azure DNS to manage. ARO_APP_FQDN='e.g. minesweeper.aro.kmobb.com' (note - we will be deploying an application called minesweeper to test front door. Select a domain you would like to use for the application. For example minesweeper.aro.kmobb.com ... where aro.kmobb.com is the domain you manage and have DNS access to.) AFD_MINE_CUSTOM_DOMAIN_NAME='minesweeper-aro-kmobb-com' (note - this should be your domain name without and .'s for example minesweeper-aro-kmobb-com) PRIVATEENDPOINTSUBNET_PREFIX= subnet in the VNET you cluster is in. If you following the example above to create a custer where you virtual network is 10.0.0.0/20 then you can use '10.0.6.0/24' PRIVATEENDPOINTSUBNET_NAME='PrivateEndpoint-subnet' Set environment variables with Bash\nUNIQUEID=$RANDOM ARO_RGNAME=$(az aro show -n $AROCLUSTER -g $ARORG --query \"clusterProfile.resourceGroupId\" -o tsv | sed 's/.*\\///') LOCATION=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query location -o tsv) INTERNAL_LBNAME=$(az network lb list --resource-group $ARO_RGNAME --query \"[? contains(name, 'internal')].name\" -o tsv) WORKER_SUBNET_NAME=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query 'workerProfiles[0].subnetId' -o tsv | sed 's/.*\\///') WORKER_SUBNET_ID=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query 'workerProfiles[0].subnetId' -o tsv) VNET_NAME=$(az network vnet list -g $ARORG --query '[0].name' -o tsv) LBCONFIG_ID=$(az network lb frontend-ip list -g $ARO_RGNAME --lb-name $INTERNAL_LBNAME --query \"[? contains(subnet.id,'$WORKER_SUBNET_ID')].id\" -o tsv) LBCONFIG_IP=$(az network lb frontend-ip list -g $ARO_RGNAME --lb-name $INTERNAL_LBNAME --query \"[? contains(subnet.id,'$WORKER_SUBNET_ID')].privateIpAddress\" -o tsv) Create a Private Link Service After we have the cluster up and running, we need to create a private link service. The private link service will provide private and secure connectivity between the Front Door Service and our cluster.\nDisable the worker subnet private link service network policy for the worker subnet\naz network vnet subnet update \\ --disable-private-link-service-network-policies true \\ --name $WORKER_SUBNET_NAME \\ --resource-group $ARORG \\ --vnet-name $VNET_NAME Create a private link service targeting the worker subnets\naz network private-link-service create \\ --name $AROCLUSTER-pls \\ --resource-group $ARORG \\ --private-ip-address-version IPv4 \\ --private-ip-allocation-method Dynamic \\ --vnet-name $VNET_NAME \\ --subnet $WORKER_SUBNET_NAME \\ --lb-frontend-ip-configs $LBCONFIG_ID privatelink_id=$(az network private-link-service show -n $AROCLUSTER-pls -g $ARORG --query 'id' -o tsv) Create and Configure an instance of Azure Front Door Create a Front Door Instance\naz afd profile create \\ --resource-group $ARORG \\ --profile-name $AFD_NAME \\ --sku Premium_AzureFrontDoor afd_id=$(az afd profile show -g $ARORG --profile-name $AFD_NAME --query 'id' -o tsv) Create an endpoint for the ARO Internal Load Balancer\naz afd endpoint create \\ --resource-group $ARORG \\ --enabled-state Enabled \\ --endpoint-name 'aro-ilb'$UNIQUEID \\ --profile-name $AFD_NAME Create a Front Door Origin Group that will point to the ARO Internal Loadbalancer\naz afd origin-group create \\ --origin-group-name 'afdorigin' \\ --probe-path '/' \\ --probe-protocol Http \\ --probe-request-type GET \\ --probe-interval-in-seconds 100 \\ --profile-name $AFD_NAME \\ --resource-group $ARORG \\ --probe-interval-in-seconds 120 \\ --sample-size 4 \\ --successful-samples-required 3 \\ --additional-latency-in-milliseconds 50 Create a Front Door Origin with the above Origin Group that will point to the ARO Internal Loadbalancer\naz afd origin create \\ --enable-private-link true \\ --private-link-resource $privatelink_id \\ --private-link-location $LOCATION \\ --private-link-request-message 'Private link service from AFD' \\ --weight 1000 \\ --priority 1 \\ --http-port 80 \\ --https-port 443 \\ --origin-group-name 'afdorigin' \\ --enabled-state Enabled \\ --host-name $LBCONFIG_IP \\ --origin-name 'afdorigin' \\ --profile-name $AFD_NAME \\ --resource-group $ARORG Approve the private link connection\nprivatelink_pe_id=$(az network private-link-service show -n $AROCLUSTER-pls -g $ARORG --query 'privateEndpointConnections[0].id' -o tsv) az network private-endpoint-connection approve \\ --description 'Approved' \\ --id $privatelink_pe_id Add your custom domain to Azure Front Door\naz afd custom-domain create \\ --certificate-type ManagedCertificate \\ --custom-domain-name $AFD_MINE_CUSTOM_DOMAIN_NAME \\ --host-name $ARO_APP_FQDN \\ --minimum-tls-version TLS12 \\ --profile-name $AFD_NAME \\ --resource-group $ARORG Create an Azure Front Door endpoint for your custom domain\naz afd endpoint create \\ --resource-group $ARORG \\ --enabled-state Enabled \\ --endpoint-name 'aro-mine-'$UNIQUEID \\ --profile-name $AFD_NAME Add an Azure Front Door route for your custom domain\naz afd route create \\ --endpoint-name 'aro-mine-'$UNIQUEID \\ --forwarding-protocol HttpOnly \\ --https-redirect Disabled \\ --origin-group 'afdorigin' \\ --profile-name $AFD_NAME \\ --resource-group $ARORG \\ --route-name 'aro-mine-route' \\ --supported-protocols Http Https \\ --patterns-to-match '/*' \\ --custom-domains $AFD_MINE_CUSTOM_DOMAIN_NAME Update DNS\nGet a validation token from Front Door so Front Door can validate your domain\nafdToken=$(az afd custom-domain show \\ --resource-group $ARORG \\ --profile-name $AFD_NAME \\ --custom-domain-name $AFD_MINE_CUSTOM_DOMAIN_NAME \\ --query \"validationProperties.validationToken\") Create a DNS Zone\naz network dns zone create -g $ARORG -n $DOMAIN You will need to configure your nameservers to point to azure. The output of running this zone create will show you the nameservers for this record that you will need to set up within your domain registrar.\nCreate a new text record in your DNS server\naz network dns record-set txt add-record -g $ARORG -z $DOMAIN -n _dnsauth.$(echo $ARO_APP_FQDN | sed 's/\\..*//') --value $afdToken --record-set-name _dnsauth.$(echo $ARO_APP_FQDN | sed 's/\\..*//') Check if the domain has been validated:\nNote this can take several hours Your FQDN will not resolve until Front Door validates your domain.\naz afd custom-domain list -g $ARORG --profile-name $AFD_NAME --query \"[? contains(hostName, '$ARO_APP_FQDN')].domainValidationState\" Add a CNAME record to DNS\nGet the Azure Front Door endpoint:\nafdEndpoint=$(az afd endpoint show -g $ARORG --profile-name $AFD_NAME --endpoint-name aro-mine-$UNIQUEID --query \"hostName\" -o tsv) Create a cname record for the application\naz network dns record-set cname set-record -g $ARORG -z $DOMAIN \\ -n $(echo $ARO_APP_FQDN | sed 's/\\..*//') -z $DOMAIN -c $afdEndpoint Deploy an application Now the fun part, let’s deploy an application! We will be deploying a Java based application called microsweeper. This is an application that runs on OpenShift and uses a PostgreSQL database to store scores. With ARO being a first class service on Azure, we will create an Azure Database for PostgreSQL service and connect it to our cluster with a private endpoint.\nCreate a Azure Database for PostgreSQL servers service\naz postgres server create --name microsweeper-database --resource-group $ARORG --location $LOCATION --admin-user quarkus --admin-password r3dh4t1! --sku-name GP_Gen5_2 POSTGRES_ID=$(az postgres server show -n microsweeper-database -g $ARORG --query 'id' -o tsv) Create a private endpoint connection for the database\naz network vnet subnet create \\ --resource-group $ARORG \\ --vnet-name $VNET_NAME \\ --name $PRIVATEENDPOINTSUBNET_NAME \\ --address-prefixes $PRIVATEENDPOINTSUBNET_PREFIX \\ --disable-private-endpoint-network-policies true az network private-endpoint create \\ --name 'postgresPvtEndpoint' \\ --resource-group $ARORG \\ --vnet-name $VNET_NAME \\ --subnet $PRIVATEENDPOINTSUBNET_NAME \\ --private-connection-resource-id $POSTGRES_ID \\ --group-id 'postgresqlServer' \\ --connection-name 'postgresdbConnection' Create and configure a private DNS Zone for the Postgres database\naz network private-dns zone create \\ --resource-group $ARORG \\ --name 'privatelink.postgres.database.azure.com' az network private-dns link vnet create \\ --resource-group $ARORG \\ --zone-name 'privatelink.postgres.database.azure.com' \\ --name 'PostgresDNSLink' \\ --virtual-network $VNET_NAME \\ --registration-enabled false az network private-endpoint dns-zone-group create \\ --resource-group $ARORG \\ --name 'PostgresDb-ZoneGroup' \\ --endpoint-name 'postgresPvtEndpoint' \\ --private-dns-zone 'privatelink.postgres.database.azure.com' \\ --zone-name 'postgresqlServer' NETWORK_INTERFACE_ID=$(az network private-endpoint show --name postgresPvtEndpoint --resource-group $ARORG --query 'networkInterfaces[0].id' -o tsv) POSTGRES_IP=$(az resource show --ids $NETWORK_INTERFACE_ID --api-version 2019-04-01 --query 'properties.ipConfigurations[0].properties.privateIPAddress' -o tsv) az network private-dns record-set a create --name $UNIQUEID-microsweeper-database --zone-name privatelink.postgres.database.azure.com --resource-group $ARORG az network private-dns record-set a add-record --record-set-name $UNIQUEID-microsweeper-database --zone-name privatelink.postgres.database.azure.com --resource-group $ARORG -a $POSTGRES_IP Create a postgres database that will contain scores for the minesweeper application\naz postgres db create \\ --resource-group $ARORG \\ --name score \\ --server-name microsweeper-database Deploy the minesweeper application Clone the git repository\ngit clone -b ARO https://github.com/redhat-mw-demos/microsweeper-quarkus.git change to the root directory\ncd microsweeper-quarkus Ensure Java 1.8 is set at your Java version\nmvn --version Look for Java version - 1.8XXXX if not set to Java 1.8 you will need to set your JAVA_HOME variable to Java 1.8 you have installed. To find your java versions run:\njava -version then export your JAVA_HOME variable\nexport JAVA_HOME=`/usr/libexec/java_home -v 1.8.0_332` Log into your openshift cluster\nBefore you deploy your application, you will need to be connected to a private network that has access to the cluster.\nA great way to establish this connectity is with a VPN connection. Follow this guide to setup a VPN connection with your Azure account.\nkubeadmin_password=$(az aro list-credentials --name $AROCLUSTER --resource-group $ARORG --query kubeadminPassword --output tsv) apiServer=$(az aro show -g $ARORG -n $AROCLUSTER --query apiserverProfile.url -o tsv) oc login $apiServer -u kubeadmin -p $kubeadmin_password Create a new OpenShift Project\noc new-project minesweeper add the openshift extension to quarkus\nquarkus ext add openshift Edit microsweeper-quarkus/src/main/resources/application.properties\nMake sure your file looks like the one below, changing the IP address on line 3 to the private ip address of your postgres instance.\nTo find your Postgres private IP address run the following commands:\nNETWORK_INTERFACE_ID=$(az network private-endpoint show --name postgresPvtEndpoint --resource-group $ARORG --query 'networkInterfaces[0].id' -o tsv) az resource show --ids $NETWORK_INTERFACE_ID --api-version 2019-04-01 --query 'properties.ipConfigurations[0].properties.privateIPAddress' -o tsv Sample microsweeper-quarkus/src/main/resources/application.properties\n# Database configurations %prod.quarkus.datasource.db-kind=postgresql %prod.quarkus.datasource.jdbc.url=jdbc:postgresql://10.1.6.9:5432/score %prod.quarkus.datasource.jdbc.driver=org.postgresql.Driver %prod.quarkus.datasource.username=quarkus@microsweeper-database %prod.quarkus.datasource.password=r3dh4t1! %prod.quarkus.hibernate-orm.database.generation=drop-and-create %prod.quarkus.hibernate-orm.database.generation=update # OpenShift configurations %prod.quarkus.kubernetes-client.trust-certs=true %prod.quarkus.kubernetes.deploy=true %prod.quarkus.kubernetes.deployment-target=openshift #%prod.quarkus.kubernetes.deployment-target=knative %prod.quarkus.openshift.build-strategy=docker #%prod.quarkus.openshift.expose=true # Serverless configurations #%prod.quarkus.container-image.group=microsweeper-%prod.quarkus #%prod.quarkus.container-image.registry=image-registry.openshift-image-registry.svc:5000 # macOS configurations #%prod.quarkus.native.container-build=true Build and deploy the quarkus application to OpenShift\nquarkus build --no-tests Create a route to your custom domain Change the snippet below replacing your hostname for the host:\ncat \u003c\u003c EOF | oc apply -f - apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app.kubernetes.io/name: microsweeper-appservice app.kubernetes.io/version: 1.0.0-SNAPSHOT app.openshift.io/runtime: quarkus name: microsweeper-appservice namespace: minesweeper spec: host: minesweeper.aro.kmobb.com to: kind: Service name: microsweeper-appservice weight: 100 targetPort: port: 8080 wildcardPolicy: None EOF Check the dns settings of your application.\nnotice that the application URL is routed through Azure Front Door at the edge. The only way this application that is running on your cluster can be access is through Azure Front Door which is connected to your cluster through a private endpoint.\nnslookup $ARO_APP_FQDN sample output:\nServer:\t2600:1700:850:d220::1 Address:\t2600:1700:850:d220::1#53 Non-authoritative answer: minesweeper.aro.kmobb.com\tcanonical name = aro-mine-13947-dxh0ahd7fzfyexgx.z01.azurefd.net. aro-mine-13947-dxh0ahd7fzfyexgx.z01.azurefd.net\tcanonical name = star-azurefd-prod.trafficmanager.net. star-azurefd-prod.trafficmanager.net\tcanonical name = dual.part-0013.t-0009.t-msedge.net. dual.part-0013.t-0009.t-msedge.net\tcanonical name = part-0013.t-0009.t-msedge.net. Name:\tpart-0013.t-0009.t-msedge.net Address: 13.107.213.41 Name:\tpart-0013.t-0009.t-msedge.net Address: 13.107.246.41 Test the application Point your broswer to your domain!! Clean up To clean up everything you created, simply delete the resource group\naz group delete -g $ARORG ","description":"","tags":["ARO","Azure"],"title":"Azure Front Door with ARO ( Azure Red Hat OpenShift )","uri":"/docs/aro/frontdoor/"},{"content":"","description":"","tags":null,"title":"Azure Service Operator","uri":"/docs/aro/azure-service-operator/"},{"content":"Paul Czarkowski\nlast edit - 02/16/2022\nThe Azure Service Operator (ASO) provides Custom Resource Definitions (CRDs) for Azure resources that can be used to create, update, and delete Azure services from an OpenShift cluster.\nThis example uses ASO V1, which has now been replaced by ASO V2. ASO V2 does not (as of 5/19/2022) yet have an entry in the OCP OperatorHub, but is functional and should be preferred for use, especially if V1 isn’t already installed on a cluster. MOBB has documented the [install of ASO V2 on ROSA]. MOBB has not tested running the two in parallel.\nPrerequisites Azure CLI An Azure Red Hat OpenShift (ARO) cluster Prepare your Azure Account and ARO Cluster Set the following environment variables:\nNote: modify the cluster name, region and resource group to match your cluster\nAZURE_TENANT_ID=$(az account show -o tsv --query tenantId) AZURE_SUBSCRIPTION_ID=$(az account show -o tsv --query id) CLUSTER_NAME=\"openshift\" AZURE_RESOURCE_GROUP=\"openshift\" AZURE_REGION=\"eastus\" Create a Service Principal with Contributor permissions to your subscription:\nNote: You may want to lock this down to a specific resource group.\nread -r ASO_USER ASO_PASS \u003c \u003c(az ad sp create-for-rbac -n \"$CLUSTER_NAME-ASO\" \\ --role contributor --scopes /subscriptions/$AZURE_SUBSCRIPTION_ID -o tsv \\ --query \"[name,password]\" | xargs) Create a secret containing your Service Principal credentials:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: azureoperatorsettings namespace: openshift-operators stringData: AZURE_TENANT_ID: $AZURE_TENANT_ID AZURE_SUBSCRIPTION_ID: $AZURE_SUBSCRIPTION_ID AZURE_CLIENT_ID: $ASO_USER AZURE_CLIENT_SECRET: $ASO_PASS AZURE_CLOUD_ENV: AzurePublicCloud EOF Deploy the ASO Operator:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/azure-service-operator.openshift-operators: \"\" name: azure-service-operator namespace: openshift-operators spec: channel: stable installPlanApproval: Automatic name: azure-service-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: azure-service-operator.v1.0.28631 EOF Deploy an Azure Redis Cache Create a Project:\noc new-project redis-demo Allow the redis app to run as any user:\noc adm policy add-scc-to-user anyuid -z default Create a random string to use as the unique redis hostname:\nREDIS_HOSTNAME=$(cat /dev/urandom | tr -dc 'a-z0-9' | fold -w 8 | head -n 1) Deploy a Redis service using the ASO Operator and an example application\ncat \u003c\u003cEOF | oc apply -f - apiVersion: azure.microsoft.com/v1alpha1 kind: RedisCache metadata: name: $REDIS_HOSTNAME spec: location: $AZURE_REGION resourceGroup: $AZURE_RESOURCE_GROUP properties: sku: name: Basic family: C capacity: 1 enableNonSslPort: true --- apiVersion: apps/v1 kind: Deployment metadata: name: azure-vote-front spec: replicas: 1 selector: matchLabels: app: azure-vote-front template: metadata: labels: app: azure-vote-front spec: containers: - name: azure-vote-front image: mcr.microsoft.com/azuredocs/azure-vote-front:v1 resources: requests: cpu: 100m memory: 128Mi limits: cpu: 250m memory: 256Mi ports: - containerPort: 80 env: - name: REDIS_NAME value: $REDIS_HOSTNAME - name: REDIS value: $REDIS_HOSTNAME.redis.cache.windows.net - name: REDIS_PWD valueFrom: secretKeyRef: name: rediscache-$REDIS_HOSTNAME key: primaryKey --- apiVersion: v1 kind: Service metadata: name: azure-vote-front spec: ports: - port: 80 selector: app: azure-vote-front --- apiVersion: route.openshift.io/v1 kind: Route metadata: name: azure-vote spec: port: targetPort: 80 tls: insecureEdgeTerminationPolicy: Redirect termination: edge to: kind: Service name: azure-vote-front EOF Wait for Redis to be ready\nThis may take 10 to 15 minutes.\nwatch oc get rediscache $REDIS_HOSTNAME the output should eventually show the following:\nNAME PROVISIONED MESSAGE l67for49 true successfully provisioned Get the URL of the example app\noc get route azure-vote Browse to the URL provided by the previous command and validate that the app is working\nCleanup Delete the project containing the demo app\noc delete project redis-demo ","description":"","tags":["ARO","Azure"],"title":"Azure Service Operator V1 in ARO","uri":"/docs/aro/azure-service-operator/v1/"},{"content":"Thatcher Hubbard\nThe Azure Service Operator (ASO) provides Custom Resource Definitions (CRDs) for Azure resources that can be used to create, update, and delete Azure services from an OpenShift cluster.\nThis example uses ASO V2, which is a replacement for ASO V1. Equivalent documentation for ASO V1 can be found here. For new installs, V2 is recommended. MOBB has not tested running them in parallel.\nPrerequisites Azure CLI An Azure Red Hat OpenShift (ARO) cluster The helm CLI tool Prepare your Azure Account and ARO Cluster Install cert-manager:\nASO relies on having the CRDs provided by cert-manager so it can request self-signed certificates. By default, cert-manager creates an Issuer of type SelfSigned, so it will work for ASO out-of-the-box. On an OpenShift cluster, the easiest way to do this is by using the OCP console, navigating to ‘Operators | OperatorHub’ and installing it from there; both the Red Hat certified and community versions will work. It’s also possible to install by applying manifests directly as covered here.\nSet the following environment variables:\nNote: modify the cluster name, region and resource group to match your cluster\nAZURE_TENANT_ID=$(az account show -o tsv --query tenantId) AZURE_SUBSCRIPTION_ID=$(az account show -o tsv --query id) CLUSTER_NAME=\"test-cluster\" AZURE_RESOURCE_GROUP=\"test-rg\" AZURE_REGION=\"westus2\" Create a Service Principal with Contributor permissions to your subscription:\nNote: You may want to lock this down to a specific resource group.\naz ad sp create-for-rbac -n \"$CLUSTER_NAME-aso\" \\ --role contributor --scopes /subscriptions/$AZURE_SUBSCRIPTION_ID The result should look something like this:\n{ \"appId\": \"12f48391-31ac-4565-936a-8249232aeb18\", \"displayName\": \"test-cluster-aso\", \"password\": \"xsr5Pz3IsPnnYxhsc7LhnNkY00cYxe.IPk\", \"tenant\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" } You’ll need two of these values for the Helm deploy of ASO:\nAZURE_CLIENT_ID=\u003cthe_appId_from_above\u003e AZURE_CLIENT_SECRET=\u003cthe_password_from_above\u003e Deploy the ASO Operator using Helm:\nFirst, add the ASO repo (this may already be present, Helm will thow a status message if so):\nhelm repo add aso2 \\ https://raw.githubusercontent.com/Azure/azure-service-operator/main/v2/charts Then install the operator itself:\nhelm upgrade --install --devel aso2 aso2/azure-service-operator \\ --create-namespace \\ --namespace=azureserviceoperator-system \\ --set azureSubscriptionID=$AZURE_SUBSCRIPTION_ID \\ --set azureTenantID=$AZURE_TENANT_ID \\ --set azureClientID=$AZURE_CLIENT_ID \\ --set azureClientSecret=$AZURE_CLIENT_SECRET It will typically take 2-3 minutes for resources to converge and for the controller to be read to provision Azure resources. There will be one Pod created in the azureserviceoperator-system namespace with two containers, an oc -n azureserviceoperator-system logs \u003cpod_name\u003e manager will likely show a string of ‘TLS handshake error’ messages as the operator waits for a Certificate to be issued, but when they stop, the operator will be ready.\nDeploy an Azure Redis Cache Create a Project:\noc new-project redis-demo Allow the redis app to run as any user:\noc adm policy add-scc-to-user anyuid -z redis-demo Create an Azure Resource Group to hold project resources. Make sure the namespace matches the project name, and that the location is in the same region the cluster is:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: resources.azure.com/v1beta20200601 kind: ResourceGroup metadata: name: redis-demo namespace: redis-demo spec: location: westus EOF Deploy a Redis service using the ASO Operator. This also shows creating a random string as part of the hostname because the Azure DNS namespace is global, and a name like sampleredis is likely to be taken. Also make sure the location spec matches.\nREDIS_HOSTNAME=redis-$(head -c24 \u003c /dev/random | base64 | LC_CTYPE=C tr -dc 'a-z0-9' | cut -c -8) cat \u003c\u003cEOF | oc apply -f - apiVersion: cache.azure.com/v1beta20201201 kind: Redis metadata: name: $REDIS_HOSTNAME namespace: redis-demo spec: location: westus owner: name: redis-demo sku: family: C name: Basic capacity: 0 enableNonSslPort: true redisConfiguration: maxmemory-delta: \"10\" maxmemory-policy: allkeys-lru redisVersion: \"6\" operatorSpec: secrets: primaryKey: name: redis-secret key: primaryKey secondaryKey: name: redis-secret key: secondaryKey hostName: name: redis-secret key: hostName port: name: redis-secret key: port EOF This will take a couple of minutes to complete as well. Also note that there is typically a bit of lag between a resource being created and showing up in the Azure Portal.\nDeploy the sample application This uses a published sample application from Microsoft:\ncat \u003c\u003cEOF | oc -n redis-demo apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: azure-vote-front spec: replicas: 1 selector: matchLabels: app: azure-vote-front template: metadata: labels: app: azure-vote-front spec: containers: - name: azure-vote-front image: mcr.microsoft.com/azuredocs/azure-vote-front:v1 resources: requests: cpu: 100m memory: 128Mi limits: cpu: 250m memory: 256Mi ports: - containerPort: 80 env: - name: REDIS valueFrom: secretKeyRef: name: redis-secret key: hostName - name: REDIS_NAME value: $REDIS_HOSTNAME - name: REDIS_PWD valueFrom: secretKeyRef: name: redis-secret key: primaryKey --- apiVersion: v1 kind: Service metadata: name: azure-vote-front spec: ports: - port: 80 selector: app: azure-vote-front --- apiVersion: route.openshift.io/v1 kind: Route metadata: name: azure-vote spec: port: targetPort: 80 tls: insecureEdgeTerminationPolicy: Redirect termination: edge to: kind: Service name: azure-vote-front EOF Get the URL of the example app\noc get route azure-vote Browse to the URL provided by the previous command and validate that the app is working\nCleanup Delete the project containing the demo app\noc delete project redis-demo Further Resources There is a library of examples for creating various Azure resource types here: https://github.com/Azure/azure-service-operator/tree/main/v2/config/samples\n","description":"","tags":["ARO","Azure"],"title":"Azure Service Operator V2 in ARO","uri":"/docs/aro/azure-service-operator/v2/"},{"content":"Author: Paul Czarkowski\nlast edited: 2023-01-04\nThis guide shows how to deploy the Cluster Log Forwarder operator and configure it to use STS authentication to forward logs to CloudWatch.\nPrerequisites A ROSA cluster (configured with STS) The jq cli command The aws cli command Environment Setup Configure the following environment variables\nChange the cluster name to match your ROSA cluster and ensure you’re logged into the cluster as an Administrator. Ensure all fields are outputted correctly before moving on.\nexport ROSA_CLUSTER_NAME=$(oc get infrastructure cluster -o=jsonpath=\"{.status.infrastructureName}\" | sed 's/-[a-z0-9]\\+$//') export REGION=$(rosa describe cluster -c ${ROSA_CLUSTER_NAME} --output json | jq -r .region.id) export OIDC_ENDPOINT=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer | sed 's|^https://||') export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` export AWS_PAGER=\"\" export SCRATCH=\"/tmp/${ROSA_CLUSTER_NAME}/clf-cloudwatch-sts\" mkdir -p ${SCRATCH} echo \"Cluster: ${ROSA_CLUSTER_NAME}, Region: ${REGION}, OIDC Endpoint: ${OIDC_ENDPOINT}, AWS Account ID: ${AWS_ACCOUNT_ID}\" Prepare AWS Account Create an IAM Policy for OpenShift Log Forwarding\nPOLICY_ARN=$(aws iam list-policies --query \"Policies[?PolicyName=='RosaCloudWatch'].{ARN:Arn}\" --output text) if [[ -z \"${POLICY_ARN}\" ]]; then cat \u003c\u003c EOF \u003e ${SCRATCH}/policy.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:DescribeLogGroups\", \"logs:DescribeLogStreams\", \"logs:PutLogEvents\", \"logs:PutRetentionPolicy\" ], \"Resource\": \"arn:aws:logs:*:*:*\" } ] } EOF POLICY_ARN=$(aws iam create-policy --policy-name \"RosaCloudWatch\" \\ --policy-document file:///${SCRATCH}/policy.json --query Policy.Arn --output text) fi echo ${POLICY_ARN} Create an IAM Role trust policy for the cluster\ncat \u003c\u003cEOF \u003e ${SCRATCH}/trust-policy.json { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_ENDPOINT}\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${OIDC_ENDPOINT}:sub\": \"system:serviceaccount:openshift-logging:logcollector\" } } }] } EOF ROLE_ARN=$(aws iam create-role --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" \\ --assume-role-policy-document file://${SCRATCH}/trust-policy.json \\ --query Role.Arn --output text) echo ${ROLE_ARN} Attach the IAM Policy to the IAM Role\naws iam attach-role-policy --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" \\ --policy-arn ${POLICY_ARN} Deploy Operators Deploy the Cluster Logging operator\ncat \u003c\u003c EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/cluster-logging.openshift-logging: \"\" name: cluster-logging namespace: openshift-logging spec: channel: stable installPlanApproval: Automatic name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace EOF Create a secret\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: Secret metadata: name: cloudwatch-credentials namespace: openshift-logging stringData: role_arn: $ROLE_ARN EOF Configure Cluster Logging Create a cluster log forwarding resource\ncat \u003c\u003c EOF | oc apply -f - apiVersion: \"logging.openshift.io/v1\" kind: ClusterLogForwarder metadata: name: instance namespace: openshift-logging spec: outputs: - name: cw type: cloudwatch cloudwatch: groupBy: namespaceName groupPrefix: rosa-${ROSA_CLUSTER_NAME} region: ${REGION} secret: name: cloudwatch-credentials pipelines: - name: to-cloudwatch inputRefs: - infrastructure - audit - application outputRefs: - cw EOF Create a cluster logging resource\ncat \u003c\u003c EOF | oc apply -f - apiVersion: logging.openshift.io/v1 kind: ClusterLogging metadata: name: instance namespace: openshift-logging spec: collection: logs: type: fluentd forwarder: fluentd: {} managementState: Managed EOF Check AWS CloudWatch for logs Use the AWS console or CLI to validate that there are log streams from the cluster\nNote: If this is a fresh cluster you may not see a log group for application logs as there are no applications running yet.\naws logs describe-log-groups --log-group-name-prefix rosa-${ROSA_CLUSTER_NAME} { \"logGroups\": [ { \"logGroupName\": \"rosa-xxxx.audit\", \"creationTime\": 1661286368369, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:us-east-2:xxxx:log-group:rosa-xxxx.audit:*\", \"storedBytes\": 0 }, { \"logGroupName\": \"rosa-xxxx.infrastructure\", \"creationTime\": 1661286369821, \"metricFilterCount\": 0, \"arn\": \"arn:aws:logs:us-east-2:xxxx:log-group:rosa-xxxx.infrastructure:*\", \"storedBytes\": 0 } ] } Cleanup Delete the Cluster Log Forwarding resource\noc delete -n openshift-logging clusterlogforwarder instance Delete the Cluster Logging resource\noc delete -n openshift-logging clusterlogging instance Detach the IAM Policy to the IAM Role\naws iam detach-role-policy --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" \\ --policy-arn \"${POLICY_ARN}\" Delete the IAM Role\naws iam delete-role --role-name \"${ROSA_CLUSTER_NAME}-RosaCloudWatch\" Delete the IAM Policy\nOnly run this command if there are no other resources using the Policy\naws iam delete-policy --policy-arn \"${POLICY_ARN}\" Delete the CloudWatch Log Groups\naws logs delete-log-group --log-group-name \"rosa-${ROSA_CLUSTER_NAME}.audit\" aws logs delete-log-group --log-group-name \"rosa-${ROSA_CLUSTER_NAME}.infrastructure\" ","description":"","tags":["AWS","ROSA"],"title":"Configuring the Cluster Log Forwarder for CloudWatch Logs and STS","uri":"/docs/rosa/clf-cloudwatch-sts/"},{"content":"Author: Roberto Carratalá, Paul Czarkowski, Andrea Bozzoni\nBy default, within OSD in GCP only the GCE-PD StorageClass is available in the cluster. With this StorageClass, only ReadWriteOnce mode is permitted, and the gcePersistentDisks can only be mounted by a single consumer in read-write mode.\nBecause of that, and for provide Storage with Shared Access (RWX) Access Mode to our OpenShift clusters a GCP Filestore could be used.\nGCP Filestore is not managed neither supported by Red Hat or Red Hat SRE team.\nPrerequisites gcloud CLI jq oc CLI The GCP Cloud Shell can be used as well and have all the prerequisites installed already.\nSteps From the CLI or GCP Cloud Shell, login within your account and your GCP project:\ngcloud auth login \u003cgoogle account user\u003e gcloud config set project \u003cgoogle project name\u003e Create a Filestore instance in GCP:\nexport ZONE_FS=\"us-west1-a\" export NAME_FS=\"nfs-server\" export TIER_FS=\"BASIC_HDD\" export VOL_NAME_FS=\"osd4\" export CAPACITY=\"1TB\" export VPC_NETWORK=\"projects/my-project/global/networks/demo-vpc\" gcloud filestore instances create $NAME_FS --zone=$ZONE_FS --tier=$TIER_FS --file-share=name=\"$VOL_NAME_FS\",capacity=$CAPACITY --network=name=\"$VPC_NETWORK\" Due to the Static Provisioning through the creation of the PV/PVC the Filestore for the RWX storage needs to be created upfront.\nAfter the creation, check the Filestore instance generated in the GCP project:\ngcloud filestore instances describe $NAME_FS --zone=$ZONE_FS Extract the ipAddresses from the NFS share for use them into the PV definition:\nNFS_IP=$(gcloud filestore instances describe $NAME_FS --zone=$ZONE_FS --format=json | jq -r .networks[0].ipAddresses[0]) echo $NFS_IP Login your OSD in GCP cluster\nCreate a Persistent Volume using the NFS_IP of the Filestore as the nfs server into the PV definition, specifying the path of the shared Filestore:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: capacity: storage: 500Gi accessModes: - ReadWriteMany nfs: server: $NFS_IP path: \"/$VOL_NAME_FS\" EOF As you can check the PV is generated with the accessMode of ReadWriteMany (RWX)\nCheck that the PV is generated properly:\n$ oc get pv nfs NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs 500Gi RWX Retain Available 12s Create a PersistentVolumeClaim for this PersistentVolume:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadWriteMany storageClassName: \"\" resources: requests: storage: 500Gi EOF As we can check the storageClassName is empty because we’re using the Static Provisioning in this case.\nCheck that the PVC is generated properly and with the Bound status:\noc get pvc nfs NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfs Bound nfs 500Gi RWX 7s Generate an example app with more than replicas sharing the same Filestore NFS volume share:\ncat \u003c\u003cEOF | oc apply -f - apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: nfs-web2 name: nfs-web spec: replicas: 2 selector: matchLabels: app: nfs-web strategy: {} template: metadata: creationTimestamp: null labels: app: nfs-web spec: containers: - image: nginxinc/nginx-unprivileged name: nginx-unprivileged ports: - name: web containerPort: 8080 volumeMounts: - name: nfs mountPath: \"/usr/share/nginx/html\" volumes: - name: nfs persistentVolumeClaim: claimName: nfs EOF Check that the pods are up \u0026\u0026 running:\noc get pod NAME READY STATUS RESTARTS AGE nfs-web2-54f9fb5cd8-8dcgh 1/1 Running 0 118s nfs-web2-54f9fb5cd8-bhmkw 1/1 Running 0 118s Check that the pods mount the same volume provided by the Filestore NFS share:\nfor i in $(oc get pod --no-headers | awk '{ print $1 }'); do echo \"POD -\u003e $i\"; oc exec -ti $i -- df -h | grep nginx; echo \"\"; done POD -\u003e nfs-web2-54f9fb5cd8-8dcgh 10.124.186.98:/osd4 1007G 0 956G 0% /usr/share/nginx/html POD -\u003e nfs-web2-54f9fb5cd8-bhmkw 10.124.186.98:/osd4 1007G 0 956G 0% /usr/share/nginx/html ","description":"","tags":["GCP","OSD"],"title":"Create Filestore Storage for OSD in GCP","uri":"/docs/gcp/filestore/"},{"content":"Roberto Carratalá, Andrea Bozzoni\nLast updated 07/06/2022\nTip The official documentation for installing a OSD cluster in GCP can be found here.\nFor deploy an OSD cluster in GCP using existing Virtual Private Cloud (VPC) you need to implement some prerequisites that you must create before starting the OpenShift Dedicated installation though the OCM.\nPrerequisites gcloud CLI jq NOTE: Also the GCloud Shell can be used, and have the gcloud cli among other tools preinstalled.\nGenerate GCP VPC and Subnets This is a diagram showing the GCP infra prerequisites that are needed for the OSD installation:\nTo deploy the GCP VPC and subnets among other prerequisites for install the OSD in GCP using the preexisting VPCs you have two options:\nOption 1 - GCloud CLI Option 2 - Terraform Automation Please select one of these two options and proceed with the OSD install steps.\nOption 1 - Generate OSD VPC and Subnets using GCloud CLI As mentioned before, for deploy OSD in GCP using existing GCP VPC, you need to provide and create beforehand a GCP VPC and two subnets (one for the masters and another for the workers nodes).\nLogin and configure the proper GCP project where the OSD will be deployed:\nexport PROJECT_NAME=\u003cgoogle project name\u003e gcloud auth list gcloud config set project $PROJECT_NAME gcloud config list project Export the names of the vpc and subnets:\nexport REGION=\u003cregion name\u003e export OSD_VPC=\u003cvpc name\u003e export MASTER_SUBNET=\u003cmaster subnet name\u003e export WORKER_SUBNET=\u003cworker subnet name\u003e Create a custom mode VPC network:\ngcloud compute networks create $OSD_VPC --subnet-mode=custom gcloud compute networks describe $OSD_VPC NOTE: we need to create the mode custom for the VPC network, because the auto mode generates automatically the subnets with IPv4 ranges with predetermined set of ranges.\nThis example is using the standard configuration for these two subnets:\nmaster-subnet - CIDR 10.0.0.0/17 - Gateway 10.0.0.1 worker-subnet - CIDR 10.0.128.0/17 - Gateway 10.0.128.1 Create the GCP Subnets for the masters and workers within the previous GCP VPC network:\ngcloud compute networks subnets create $MASTER_SUBNET \\ --network=$OSD_VPC --range=10.0.0.0/17 --region=$REGION gcloud compute networks subnets create $WORKER_SUBNET \\ --network=$OSD_VPC --range=10.0.128.0/17 --region=$REGION Once the VPC and the two subnets are provided it is needed to create one GCP Cloud Router:\nexport OSD_ROUTER=\u003crouter name\u003e gcloud compute routers create $OSD_ROUTER \\ --project=$PROJECT_NAME --network=$OSD_VPC --region=$REGION {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“350” }\nThen, we will deploy two GCP Cloud NATs and attach them within the GCP Router:\nGenerate the GCP Cloud Nat for the Master Subnets: export NAT_MASTER=\u003cmaster subnet name\u003e gcloud compute routers nats create $NAT_MASTER \\ --region=$REGION \\ --router=$OSD_ROUTER \\ --auto-allocate-nat-external-ips \\ --nat-custom-subnet-ip-ranges=$MASTER_SUBNET {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“350” }\nGenerate the GCP Cloud NAT for the Worker Subnets: export NAT_WORKER=\u003cworker subnet name\u003e gcloud compute routers nats create $NAT_WORKER \\ --region=$REGION \\ --router=$OSD_ROUTER \\ --auto-allocate-nat-external-ips \\ --nat-custom-subnet-ip-ranges=$WORKER_SUBNET {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“350” }\nAs you can check the Cloud NATs GW are attached now to the Cloud Router:\n{:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“350” }\nOption 2 - Deploy OSD VPC and Subnets using Terraform You can use also automation code in Terraform to deploy all the GCP infrastructure required to deploy the OSD in preexistent VPCs.\nClone the tf-osd-gcp repository: git clone https://github.com/rh-mobb/tf-osd-gcp.git cd tf-osd-gcp Copy and modify the tfvars file in order to custom to your scenario: cp -pr terraform.tfvars.example terraform.tfvars Deploy the network infrastructure in GCP needed for deploy the OSD cluster: make all Install the OSD cluster using pre-existent VPCs These steps are based in the official OSD installation documentation.\nLog in to OpenShift Cluster Manager and click Create cluster.\nIn the Cloud tab, click Create cluster in the Red Hat OpenShift Dedicated row.\nUnder Billing model, configure the subscription type and infrastructure type {:style=“display:block; margin-left:auto; margin-right:auto”}\nSelect Run on Google Cloud Platform.\nClick Prerequisites to review the prerequisites for installing OpenShift Dedicated on GCP with CCS.\nProvide your GCP service account private key in JSON format. You can either click Browse to locate and attach a JSON file or add the details in the Service account JSON field. {:style=“display:block; margin-left:auto; margin-right:auto”}\nValidate your cloud provider account and then click Next. On the Cluster details page, provide a name for your cluster and specify the cluster details: {:style=“display:block; margin-left:auto; margin-right:auto”}\nNOTE: the Region used to be installed needs to be the same as the VPC and Subnets deployed in the early step.\nOn the Default machine pool page, select a Compute node instance type and a Compute node count: {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“600” }\nIn the Cluster privacy section, select Public endpoints and application routes for your cluster.\nSelect Install into an existing VPC to install the cluster in an existing GCP Virtual Private Cloud (VPC): {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“600” }\nProvide your Virtual Private Cloud (VPC) subnet settings, that you deployed as prerequisites in the previous section: {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“600” }\nIn the CIDR ranges dialog, configure custom classless inter-domain routing (CIDR) ranges or use the defaults that are provided: {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“600” }\nOn the Cluster update strategy page, configure your update preferences.\nReview the summary of your selections and click Create cluster to start the cluster installation. Check that the Install into Existing VPC is enabled and the VPC and Subnets are properly selected and defined: {:style=“display:block; margin-left:auto; margin-right:auto”}{: width=“600” }\nCleanup Deleting a ROSA cluster consists of two parts:\nDeleting the OSD cluster can be done using the OCM console described in the official OSD docs.\nDeleting the GCP infrastructure resources (VPC, Subnets, Cloud NAT, Cloud Router). Depending of which option you selected you must perform:\nOption 1: Delete GCP resources using GCloud CLI:\ngcloud compute routers nats delete $NAT_WORKER \\ --region=$REGION --router=$OSD_ROUTER --quiet gcloud compute routers nats delete $NAT_MASTER \\ --region=$REGION --router=$OSD_ROUTER --quiet gcloud compute routers delete $OSD_ROUTER --region=$REGION --quiet gcloud compute networks subnets delete $MASTER_SUBNET --region=$REGION --quiet gcloud compute networks subnets delete $WORKER_SUBNET --region=$REGION --quiet gcloud compute networks delete $OSD_VPC --quiet Option 2: Delete GCP resources using Terraform:\nmake destroy ","description":"","tags":["GCP","OSD"],"title":"Creating a OSD in GCP with Existing VPCs","uri":"/docs/gcp/osd_preexisting_vpc/"},{"content":"Paul Czarkowski\nLast updated 05/31/2022\nTip The official documentation for installing a ROSA cluster in STS mode can be found here.\nQuick Introduction by Ryan Niksch (AWS) and Shaozen Ding (Red Hat) on YouTube\nSTS allows us to deploy ROSA without needing a ROSA admin account, instead it uses roles and policies with Amazon STS (secure token service) to gain access to the AWS resources needed to install and operate the cluster.\nThis is a summary of the official docs that can be used as a line by line install guide and later used as a basis for automation in your favorite automation tool.\nNote that some commands (OIDC for STS) will be hard coded to US-EAST-1, do not be tempted to change these to use $REGION instead or you will fail installation.\nPrerequisites AWS CLI Rosa CLI v1.2.2 OpenShift CLI (run rosa download openshift-client) jq Prepare local environment set some environment variables\nexport VERSION=4.10.15 \\ ROSA_CLUSTER_NAME=mycluster \\ AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` \\ REGION=us-east-2 \\ AWS_PAGER=\"\" Prepare AWS and Red Hat accounts If this is your first time deploying ROSA you need to do some preparation as described here. Stop just before running rosa init we don’t need to do that for STS mode.\nIf this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following\naws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\" Associate your AWS account\nTo perform ROSA cluster provisioning tasks, you must create ocm-role and user-role IAM resources in your AWS account and link them to your Red Hat organization.\nOCM Role\nThe first role you will create is the ocm-role which the OpenShift Cluster Manager will use to be able to administer and Create ROSA clusters. If you haven’t already created the ocm-role, you can create and link the role with one command.\nrosa create ocm-role Tip If you have multiple AWS accounts that you want to associate with your Red Hat Organization, you can use the --profile option to specify the AWS profile you would like to associate.\nIf you have already created the ocm-role, you can just link the ocm-role to your Red Hat organization.\nrosa link ocm-user --role-arm \u003carn\u003e Tip You can get your OCM role arn from AWS IAM:\naws iam list-roles | grep OCM User Role\nThe second is the user-role that allows OCM to verify that users creating a cluster have access to the current AWS account. If you haven’t already created the user-role, you can create and link the role with one command.\nrosa create user-role Tip If you have multiple AWS accounts that you want to associate with your Red Hat Organization, you can use the --profile option to specify the AWS profile you would like to associate.\nIf you have already created the user-role, you can just link the user-role to your Red Hat organization. rosa link user-role --role-arn \u003carn\u003e Tip You can get your User role arn from the ROSA cli: rosa whoami\nlook for the AWS ARN: field Deploy ROSA cluster Make you your ROSA CLI version is correct (v1.2.2 or higher)\nrosa version Run the rosa cli to create your cluster\nYou can run the command as provided in the ouput of the previous step to deploy in interactive mode.\nAdd any other arguments to this command to suit your cluster. for example --private-link and --subnet-ids=subnet-12345678,subnet-87654321.\nrosa create cluster --sts --cluster-name ${ROSA_CLUSTER_NAME} \\ --region ${REGION} --version ${VERSION} --mode auto -y Validate The cluster is now installing\nThe State should have moved beyond pending and show installing or ready.\nwatch \"rosa describe cluster -c $ROSA_CLUSTER_NAME\" Watch the install logs\nrosa logs install -c $ROSA_CLUSTER_NAME --watch --tail 10 Validate the cluster Once the cluster has finished installing we can validate we can access it\nCreate an Admin user\nrosa create admin -c $ROSA_CLUSTER_NAME Wait a few moments and run the oc login command it provides.\nCleanup Delete the ROSA cluster\nrosa delete cluster -c $ROSA_CLUSTER_NAME Clean up the STS roles\nOnce the cluster is deleted we can delete the STS roles.\n\u003e Note you can get the correct commands with the ID filled in from the output of the previous step. ```bash rosa delete operator-roles -c \u003cid\u003e --yes --mode auto rosa delete oidc-provider -c \u003cid\u003e --yes --mode auto ``` ","description":"","tags":["AWS","ROSA","STS"],"title":"Creating a ROSA cluster in STS mode","uri":"/docs/rosa/sts/"},{"content":"Prerequisites AWS CLI Rosa CLI v1.0.8 jq Create VPC and Subnets The following instructions use the AWS CLI to create the necessary networking to deploy a Private Link ROSA cluster into a Single AZ and are intended to be a guide. Ideally you would use an Automation tool like Ansible or Terraform to manage your VPCs.\nWhen creating subnets, make sure that subnet(s) are created to an availability zone that has ROSA instances types available. If AZ is not “forced”, subnet is created to random AZ in the region. Force the AZ using --availability-zone argument in create-subnet command.\nUse rosa list instance-types to list ROSA instance types and check available types availability in AZ with the following\naws ec2 describe-instance-type-offerings \\ --location-type availability-zone \\ --filters Name=location,Values=AZ_NAME_HERE \\ --region REGION_HERE --output text | egrep \"YOU_PREFERRED_INSTANCE_TYPE\" As an example, you cannot install ROSA to us-east-1e AZ, but us-east-1b works fine.\nOption 1 - VPC with a private subnet and AWS Site-to-Site VPN access. Todo\nOption 2 - VPC with public and private subnets and AWS Site-to-Site VPN access Todo\nOption 3 - VPC with public and private subnets (NAT) This will create both a Private and Public subnet. All cluster resources will live in the private subnet, the public subnet only exists to NAT the egress traffic to the Internet.\nAs an alternative use the Terraform instructions provided here then skip down to the rosa create command.\nSet a Cluster name\nROSA_CLUSTER_NAME=private-link Create a VPC to install a ROSA cluster into\nVPC_ID=`aws ec2 create-vpc --cidr-block 10.0.0.0/16 | jq -r .Vpc.VpcId` aws ec2 create-tags --resources $VPC_ID \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . aws ec2 modify-vpc-attribute --vpc-id $VPC_ID --enable-dns-hostnames | jq . Create a Public Subnet for the cluster to NAT egress traffic out of\nPUBLIC_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.128.0/17 | jq -r .Subnet.SubnetId` aws ec2 create-tags --resources $PUBLIC_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-public | jq . Create a Private Subnet for the cluster machines to live in\nPRIVATE_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.0.0/17 | jq -r .Subnet.SubnetId` aws ec2 create-tags --resources $PRIVATE_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private | jq . Create an Internet Gateway for NAT egress traffic\nI_GW=`aws ec2 create-internet-gateway | jq -r .InternetGateway.InternetGatewayId` aws ec2 attach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $I_GW | jq . aws ec2 create-tags --resources $I_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . Create a Route Table for NAT egress traffic\nR_TABLE=`aws ec2 create-route-table --vpc-id $VPC_ID | jq -r .RouteTable.RouteTableId` aws ec2 create-route --route-table-id $R_TABLE --destination-cidr-block 0.0.0.0/0 --gateway-id $I_GW | jq . aws ec2 describe-route-tables --route-table-id $R_TABLE | jq . aws ec2 associate-route-table --subnet-id $PUBLIC_SUBNET --route-table-id $R_TABLE | jq . aws ec2 create-tags --resources $R_TABLE \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . Create a NAT Gateway for the Private network\nEIP=`aws ec2 allocate-address --domain vpc | jq -r .AllocationId` NAT_GW=`aws ec2 create-nat-gateway --subnet-id $PUBLIC_SUBNET \\ --allocation-id $EIP | jq -r .NatGateway.NatGatewayId` aws ec2 create-tags --resources $EIP --resources $NAT_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME | jq . Create a Route Table for the Private subnet to the NAT\nR_TABLE_NAT=`aws ec2 create-route-table --vpc-id $VPC_ID | jq -r .RouteTable.RouteTableId` while ! aws ec2 describe-route-tables --route-table-id $R_TABLE_NAT \\ | jq .; do sleep 1; done aws ec2 create-route --route-table-id $R_TABLE_NAT --destination-cidr-block 0.0.0.0/0 --gateway-id $NAT_GW | jq . aws ec2 associate-route-table --subnet-id $PRIVATE_SUBNET --route-table-id $R_TABLE_NAT | jq . aws ec2 create-tags --resources $R_TABLE_NAT $EIP \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private | jq . Deploy ROSA Create ROSA cluster in the private subnet\nrosa create cluster --private-link \\ --cluster-name=$ROSA_CLUSTER_NAME \\ --machine-cidr=10.0.0.0/16 \\ --subnet-ids=$PRIVATE_SUBNET Test Connectivity Create an Instance to use as a jump host\nTODO: CLI instructions\nThrough the GUI:\nNavigate to the EC2 console and launch a new instance\nSelect the AMI for your instance, if you don’t have a standard, the Amazon Linux 2 AMI works just fine\nChoose your instance type, the t2.micro/free tier is sufficient for our needs, and click Next: Configure Instance Details\nChange the Network settings to setup this host inside your private-link VPC Change the Subnet setting to use the private-link-public subnet Change Auto-assign Public IP to Enable Default settings for Storage and Tags are OK, if you do not need to change them for your own reasons, select 6. Configure Security Group from the top navigation or click through using the Next buttons\nIf you already have a security group created to allow access from your computer to AWS, choose Select an existing security group and choose that group from the list and skip to Review and Launch. Otherwise, select Create a new security group and continue.\nTo allow access only from your current public IP, change the Source heading to use My IP Click Review and Launch, verify all settings are correct and follow the standard AWS instructions for finalizing the setup and selecting/creating the security keys.\nOnce launched, open the instance summary for the jump host instance and note the public IP address.\nCreate a ROSA admin user and save the login command for use later\nrosa create admin -c $ROSA_CLUSTER_NAME Note the DNS name of your private cluster, use the rosa describe command if needed\nrosa describe cluster -c private-link update /etc/hosts to point the openshift domains to localhost. Use the DNS of your openshift cluster as described in the previous step in place of $YOUR_OPENSHIFT_DNS below\n127.0.0.1 api.$YOUR_OPENSHIFT_DNS 127.0.0.1 console-openshift-console.apps.$YOUR_OPENSHIFT_DNS 127.0.0.1 oauth-openshift.apps.$YOUR_OPENSHIFT_DNS SSH to that instance, tunneling traffic for the appropriate hostnames. Be sure to use your new/existing private key, the OpenShift DNS for $YOUR_OPENSHIFT_DNS and your jump host IP for $YOUR_EC2_IP\nsudo ssh -i PATH/TO/YOUR_KEY.pem \\ -L 6443:api.$YOUR_OPENSHIFT_DNS:6443 \\ -L 443:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:443 \\ -L 80:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:80 \\ ec2-user@$YOUR_EC2_IP Log into the cluster using oc login command from the create admin command above. ex.\noc login https://api.private-test.3d1n.p1.openshiftapps.com:6443 --username cluster-admin --password GQSGJ-daqfN-8QNY3-tS9gU Check that you can access the Console by opening the console url in your browser.\nCleanup Delete ROSA\nrosa delete cluster -c $ROSA_CLUSTER_NAME -y Delete AWS resources\naws ec2 delete-nat-gateway --nat-gateway-id $NAT_GW | jq . aws ec2 release-address --allocation-id=$EIP | jq . aws ec2 detach-internet-gateway --vpc-id $VPC_ID \\ --internet-gateway-id $I_GW | jq . aws ec2 delete-subnet --subnet-id=$PRIVATE_SUBNET | jq . aws ec2 delete-subnet --subnet-id=$PUBLIC_SUBNET | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE_NAT | jq . aws ec2 delete-vpc --vpc-id=$VPC_ID | jq . ","description":"","tags":["AWS","ROSA","Private Link"],"title":"Creating a ROSA cluster with Private Link enabled","uri":"/docs/rosa/private-link/"},{"content":"Steve Mirman, Paul Czarkowski\nLast updated 1/28/2022\nThis is a combination of the private-link and sts setup documents to show the full picture\nPrerequisites AWS CLI Rosa CLI v1.1.7 jq AWS Preparation If this is a brand new AWS account that has never had a AWS Load Balancer installed in it, you should run the following\naws iam create-service-linked-role --aws-service-name \\ \"elasticloadbalancing.amazonaws.com\" Create the AWS Virtual Private Cloud (VPC) and Subnets For this scenario, we will be using a newly created VPC with both public and private subnets. All of the cluster resources will reside in the private subnet. The public subnet will be used for traffic to the Internet (egress)\nNote: If you already have a Transit Gateway (TGW) or similar, you can skip the public subnet configuration\nNote: When creating subnets, make sure that subnet(s) are created in availability zones that have ROSA instances types available. If AZ is not “forced”, the subnet is created in a random AZ in the region. Force AZ using the --availability-zone argument in the create-subnet command.\nUse rosa list instance-types to list the ROSA instance types\nUse aws ec2 describe-instance-type-offerings to check that your desired AZ supports your desired instance type\nExample using us-east-1, us-east-1b, and m5.xlarge: aws ec2 describe-instance-type-offerings --location-type availability-zone \\ --filters Name=location,Values=us-east-1b --region us-east-1 \\ --output text | egrep m5.xlarge Result should display INSTANCETYPEOFFERINGS [instance-type] [az] availability-zone if your selected region supports your desired instance type\nConfigure the following environment variables, adjusting for ROSA_CLUSTER_NAME, VERSION and REGION as necessary\nexport VERSION=4.9.15 \\ ROSA_CLUSTER_NAME=pl-sts-cluster \\ AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` \\ REGION=us-east-1 \\ AWS_PAGER=\"\" Create a VPC for use by ROSA\nCreate the VPC and return the ID as VPC_ID\nVPC_ID=`aws ec2 create-vpc --cidr-block 10.0.0.0/16 | jq -r .Vpc.VpcId` echo $VPC_ID Tag the newly created VPC with the cluster name\naws ec2 create-tags --resources $VPC_ID \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME Configure the VPC to allow DNS hostnames for their public IP addresses\naws ec2 modify-vpc-attribute --vpc-id $VPC_ID --enable-dns-hostnames The new VPC should be visible in the AWS console\nCreate a Public Subnet to allow egress traffic to the Internet\nCreate the public subnet in the VPC CIDR block range and return the ID as PUBLIC_SUBNET\nPUBLIC_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.128.0/17 | jq -r .Subnet.SubnetId` echo $PUBLIC_SUBNET Tag the public subnet with the cluster name\naws ec2 create-tags --resources $PUBLIC_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-public Create a Private Subnet for the cluster\nCreate the private subnet in the VPC CIDR block range and return the ID as PRIVATE_SUBNET\nPRIVATE_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID \\ --cidr-block 10.0.0.0/17 | jq -r .Subnet.SubnetId` echo $PRIVATE_SUBNET Tag the private subnet with the cluster name\naws ec2 create-tags --resources $PRIVATE_SUBNET \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private Both subnets should now be visible in the AWS console\nCreate an Internet Gateway for NAT egress traffic\nCreate the Internet Gateway and return the ID as I_GW\nI_GW=`aws ec2 create-internet-gateway | jq -r .InternetGateway.InternetGatewayId` echo $I_GW Attach the new Internet Gateway to the VPC\naws ec2 attach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $I_GW Tag the Internet Gateway with the cluster name\naws ec2 create-tags --resources $I_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME The new Internet Gateway should be created and attached to your VPC\nCreate a Route Table for NAT egress traffic\nCreate the Route Table and return the ID as R_TABLE\nR_TABLE=`aws ec2 create-route-table --vpc-id $VPC_ID \\ | jq -r .RouteTable.RouteTableId` echo $R_TABLE Create a route with no IP limitations (0.0.0.0/0) to the Internet Gateway\naws ec2 create-route --route-table-id $R_TABLE \\ --destination-cidr-block 0.0.0.0/0 --gateway-id $I_GW Verify the route table settings\naws ec2 describe-route-tables --route-table-id $R_TABLE Example output\nAssociate the Route Table with the Public subnet\naws ec2 associate-route-table --subnet-id $PUBLIC_SUBNET \\ --route-table-id $R_TABLE Example output\nTag the Route Table with the cluster name\naws ec2 create-tags --resources $R_TABLE \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME Create a NAT Gateway for the Private network\nAllocate and elastic IP address and return the ID as EIP\nEIP=`aws ec2 allocate-address --domain vpc | jq -r .AllocationId` echo $EIP Create a new NAT Gateway in the Public subnet with the new Elastic IP address and return the ID as NAT_GW\nNAT_GW=`aws ec2 create-nat-gateway --subnet-id $PUBLIC_SUBNET \\ --allocation-id $EIP | jq -r .NatGateway.NatGatewayId` echo $NAT_GW Tag the Elastic IP with the cluster name\naws ec2 create-tags --resources $EIP --resources $NAT_GW \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME The new NAT Gateway should be created and associated with your VPC\nCreate a Route Table for the Private subnet to the NAT Gateway\nCreate a Route Table in the VPC and return the ID as R_TABLE_NAT\nR_TABLE_NAT=`aws ec2 create-route-table --vpc-id $VPC_ID \\ | jq -r .RouteTable.RouteTableId` echo $R_TABLE_NAT Loop through a Route Table check until it is created\nwhile ! aws ec2 describe-route-tables \\ --route-table-id $R_TABLE_NAT \\ | jq .; do sleep 1; done Example output! Create a route in the new Route Table for all addresses to the NAT Gateway\naws ec2 create-route --route-table-id $R_TABLE_NAT \\ --destination-cidr-block 0.0.0.0/0 \\ --gateway-id $NAT_GW Associate the Route Table with the Private subnet\naws ec2 associate-route-table --subnet-id $PRIVATE_SUBNET \\ --route-table-id $R_TABLE_NAT Tag the Route Table with the cluster name\naws ec2 create-tags --resources $R_TABLE_NAT $EIP \\ --tags Key=Name,Value=$ROSA_CLUSTER_NAME-private Configure the AWS Security Token Service (STS) for use with ROSA The AWS Security Token Service (STS) allows us to deploy ROSA without needing a ROSA admin account, instead it uses roles and policies to gain access to the AWS resources needed to install and operate the cluster.\nThis is a summary of the official OpenShift docs that can be used as a line by line install guide.\nNote that some commands (OIDC for STS) will be hard coded to US-EAST-1, do not be tempted to change these to use $region instead or you will fail installation.\nMake you your ROSA CLI version is correct (v1.1.0 or higher)\nrosa version Create the IAM Account Roles\nrosa create account-roles --mode auto --yes Deploy ROSA cluster Run the rosa cli to create your cluster\nrosa create cluster -y --cluster-name ${ROSA_CLUSTER_NAME} \\ --region ${REGION} --version ${VERSION} \\ --subnet-ids=$PRIVATE_SUBNET \\ --private-link --machine-cidr=10.0.0.0/16 \\ --sts Confirm the Private Link set up Create the Operator Roles\nrosa create operator-roles -c $ROSA_CLUSTER_NAME --mode auto --yes Create the OIDC provider.\nrosa create oidc-provider -c $ROSA_CLUSTER_NAME --mode auto --yes Validate The cluster is now installing\nThe State should have moved beyond pending and show installing or ready.\nwatch \"rosa describe cluster -c $ROSA_CLUSTER_NAME\" Watch the install logs\nrosa logs install -c $ROSA_CLUSTER_NAME --watch --tail 10 Validate the cluster Once the cluster has finished installing it is time to validate. Validation when using Private Link requires the use of a jump host.\nYou can create them using the AWS Console or the AWS CLI as depicted below:\nOption 1: Create a jump host instance through the AWS Console\nNavigate to the EC2 console and launch a new instance\nSelect the AMI for your instance, if you don’t have a standard, the Amazon Linux 2 AMI works just fine Choose your instance type, the t2.micro/free tier is sufficient for our needs, and click Next: Configure Instance Details\nChange the Network settings to setup this host inside your private-link VPC Change the Subnet setting to use the private-link-public subnet Change Auto-assign Public IP to Enable Default settings for Storage and Tags are fine. Make the following changes in the 6. Configure Security Group tab (either by clicking through the screens or selecting from the top bar)\nIf you already have a security group created to allow access from your computer to AWS, choose Select an existing security group and choose that group from the list, otherwise, select Create a new security group and continue.\nTo allow access only from your current public IP, change the Source heading to use My IP Click Review and Launch, verify all settings are correct, and follow the standard AWS instructions for finalizing the setup and selecting/creating the security keys.\nOnce launched, open the instance summary for the jump host instance and note the public IP address.\nOption 2: Create a jumphost instance using the AWS CLI\nCreate an additional Security Group for the jumphost\nTAG_SG=\"$ROSA_CLUSTER_NAME-jumphost-sg\" aws ec2 create-security-group --group-name ${ROSA_CLUSTER_NAME}-jumphost-sg --description ${ROSA_CLUSTER_NAME}-jumphost-sg --vpc-id ${VPC_ID} --tag-specifications \"ResourceType=security-group,Tags=[{Key=Name,Value=$TAG_SG}]\" Grab the Security Group Id generated in the previous step\nPublicSecurityGroupId=$(aws ec2 describe-security-groups --filters \"Name=tag:Name,Values=${ROSA_CLUSTER_NAME}-jumphost-sg\" | jq -r '.SecurityGroups[0].GroupId') echo $PublicSecurityGroupId Add a rule to Allow the ssh into the Public Security Group\naws ec2 authorize-security-group-ingress --group-id $PublicSecurityGroupId --protocol tcp --port 22 --cidr 0.0.0.0/0 (Optional) Create a Key Pair for your jumphost if your have not a previous one\naws ec2 create-key-pair --key-name $ROSA_CLUSTER_NAME-key --query 'KeyMaterial' --output text \u003e PATH/TO/YOUR_KEY.pem chmod 400 PATH/TO/YOUR_KEY.pem Define an AMI_ID to be used for your jump host\nAMI_ID=\"ami-0022f774911c1d690\" This AMI_ID corresponds an Amazon Linux within the us-east-1 region and could be not available in your region. Find your AMI ID and use the proper ID.\nLaunch an ec2 instance for your jumphost using the parameters defined in early steps:\nTAG_VM=\"$ROSA_CLUSTER_NAME-jumphost-vm\" aws ec2 run-instances --image-id $AMI_ID --count 1 --instance-type t2.micro --key-name $ROSA_CLUSTER_NAME-key --security-group-ids $PublicSecurityGroupId --subnet-id $PUBLIC_SUBNET --associate-public-ip-address --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=$TAG_VM}]\" This instance will be associated with a Public IP directly.\nWait until the ec2 instance is in Running state, grab the Public IP associated to the instance and check the if the ssh port and:\nIpPublicBastion=$(aws ec2 describe-instances --filters \"Name=tag:Name,Values=$TAG_VM\" | jq -r '.Reservations[0].Instances[0].PublicIpAddress') echo $IpPublicBastion nc -vz $IpPublicBastion 22 Create a ROSA admin user and save the login command for use later\nrosa create admin -c $ROSA_CLUSTER_NAME Note the DNS name of your private cluster, use the rosa describe command if needed\nrosa describe cluster -c $ROSA_CLUSTER_NAME update /etc/hosts to point the openshift domains to localhost. Use the DNS of your openshift cluster as described in the previous step in place of $YOUR_OPENSHIFT_DNS below\n127.0.0.1 api.$YOUR_OPENSHIFT_DNS 127.0.0.1 console-openshift-console.apps.$YOUR_OPENSHIFT_DNS 127.0.0.1 oauth-openshift.apps.$YOUR_OPENSHIFT_DNS SSH to that instance, tunneling traffic for the appropriate hostnames. Be sure to use your new/existing private key, the OpenShift DNS for $YOUR_OPENSHIFT_DNS and your jump host IP for $YOUR_EC2_IP\nsudo ssh -i PATH/TO/YOUR_KEY.pem \\ -L 6443:api.$YOUR_OPENSHIFT_DNS:6443 \\ -L 443:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:443 \\ -L 80:console-openshift-console.apps.$YOUR_OPENSHIFT_DNS:80 \\ ec2-user@$YOUR_EC2_IP From your EC2 jump instances, download the OC CLI and install it locally\nDownload the OC CLI for Linux wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/openshift-client-linux.tar.gz Unzip and untar the binary gunzip openshift-client-linux.tar.gz tar -xvf openshift-client-linux.tar log into the cluster using oc login command from the create admin command above. ex.\n./oc login https://api.$YOUR_OPENSHIFT_DNS.p1.openshiftapps.com:6443 --username cluster-admin --password $YOUR_OPENSHIFT_PWD Check that you can access the Console by opening the console url in your browser. Cleanup Delete ROSA\nrosa delete cluster -c $ROSA_CLUSTER_NAME -y Watch the logs and wait until the cluster is deleted\nrosa logs uninstall -c $ROSA_CLUSTER_NAME --watch Clean up the STS roles\nNote you can get the correct commands with the ID filled in from the output of the previous step.\nrosa delete operator-roles -c \u003cid\u003e --mode auto --yes rosa delete oidc-provider -c \u003cid\u003e --mode auto --yes Delete AWS resources\naws ec2 delete-nat-gateway --nat-gateway-id $NAT_GW | jq . aws ec2 release-address --allocation-id=$EIP | jq . aws ec2 detach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $I_GW | jq . aws ec2 delete-subnet --subnet-id=$PRIVATE_SUBNET | jq . aws ec2 delete-subnet --subnet-id=$PUBLIC_SUBNET | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE | jq . aws ec2 delete-route-table --route-table-id=$R_TABLE_NAT | jq . aws ec2 delete-internet-gateway --internet-gateway-id $I_GW | jq . aws ec2 delete-vpc --vpc-id=$VPC_ID | jq . ","description":"","tags":["AWS","ROSA","STS","Private Link"],"title":"Creating a ROSA cluster with Private Link enabled (custom VPC) and STS","uri":"/docs/rosa/sts-with-private-link/"},{"content":"ROSA 4.9.x introduces a new way to provide custom AlertManager configuration to receive alerts from User Workload Management.\nThe OpenShift Administrator can use the Prometheus Operator to create a custom AlertManager resource and then use the AlertManagerConfig resource to configure User Workload Monitoring to use the custom AlertManager.\nPrerequisites AWS CLI A Red Hat OpenShift for AWS (ROSA) cluster 4.9.0 or higher Create Environment Variables Before we get started we need to set some environment variables to be used throughout the guide.\nexport PROM_NAMESPACE=custom-alert-manager Install Prometheus Operator If you prefer you can do this from the Operator Hub in the cluster console itself.\nCreate a OperatorGroup and Subscription for the Prometheus Operator\ncat \u003c\u003c EOF | kubectl apply -f - --- apiVersion: v1 kind: Namespace metadata: name: ${PROM_NAMESPACE} --- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: federated-metrics namespace: ${PROM_NAMESPACE} spec: targetNamespaces: - ${PROM_NAMESPACE} --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: prometheus namespace: ${PROM_NAMESPACE} spec: channel: beta installPlanApproval: Automatic name: prometheus source: community-operators sourceNamespace: openshift-marketplace EOF Deploy AlertManager Create an Alert Manager Configuration file\nThis will create a basic AlertManager configuration to send alerts to a slack channel. Configuring slack is outside the scope of this document. Update the variables to suit your slack integration.\nSLACK_API_URL=https://hooks.slack.com/services/XXX/XXX/XXX SLACK_CHANNEL='#paultest' cat \u003c\u003c EOF | kubectl apply -n ${PROM_NAMESPACE} -f - apiVersion: v1 kind: Secret metadata: name: custom-alertmanager namespace: ${PROM_NAMESPACE} stringData: alertmanager.yaml: | global: slack_api_url: \"${SLACK_API_URL}\" route: receiver: slack-notifications group_by: [alertname] receivers: - name: slack-notifications slack_configs: - channel: ${SLACK_CHANNEL} send_resolved: true --- apiVersion: monitoring.coreos.com/v1 kind: Alertmanager metadata: name: custom-alertmanager namespace: ${PROM_NAMESPACE} spec: securityContext: {} replicas: 3 configSecret: custom-alertmanager --- apiVersion: v1 kind: Service metadata: name: custom-alertmanager namespace: ${PROM_NAMESPACE} spec: type: ClusterIP ports: - name: web port: 9093 protocol: TCP targetPort: web selector: alertmanager: custom-alertmanager EOF Configure User Workload Monitoring to use the custom AlertManager Create an AlertManagerConfig for User Workload Monitoring\nNote: This next command assumes the existing config.yaml in the user-workload-monitoring-config config map is empty. You should verify it with kubectl get -n openshift-user-workload-monitoring cm user-workload-monitoring-config -o yaml and simply edit in the differences if its not.\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | thanosRuler: additionalAlertmanagerConfigs: - scheme: http pathPrefix: / timeout: \"30s\" apiVersion: v1 staticConfigs: [\"custom-alertmanager.$PROM_NAMESPACE.svc.cluster.local:9093\"] EOF Create an Example Alert Verify it works by creating a Prometheus Rule that will fire off an alert\ncat \u003c\u003c EOF | kubectl apply -n $PROM_NAMESPACE -f - apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: prometheus-example-rules namespace: ${PROM_NAMESPACE} spec: groups: - name: example.rules rules: - alert: ExampleAlert expr: vector(1) EOF Forward a port to the alert manager service\nkubectl port-forward -n ${PROM_NAMESPACE} svc/custom-alertmanager 9093:9093 Browse to http://localhost:9093/#/alerts to see the alert “ExampleAlert”\nCheck the Alert was sent to Slack\n","description":"","tags":["AWS","ROSA"],"title":"Custom AlertManager in ROSA 4.9.x","uri":"/docs/rosa/custom-alertmanager-4.9/"},{"content":"Paul Czarkowski\n04/12/2022\nPrerequisites an Azure Red Hat OpenShift cluster Get Started Run this oc command to enable the Managed Upgrade Operator (MUO)\noc patch cluster.aro.openshift.io cluster --patch \\ '{\"spec\":{\"operatorflags\":{\"rh.srep.muo.enabled\": \"true\",\"rh.srep.muo.managed\": \"true\",\"rh.srep.muo.deploy.pullspec\":\"arosvc.azurecr.io/managed-upgrade-operator@sha256:f57615aa690580a12c1e5031ad7ea674ce249c3d0f54e6dc4d070e42a9c9a274\"}}}' \\ --type=merge Wait a few moments to ensure the Management Upgrade Operator is ready\noc -n openshift-managed-upgrade-operator \\ get deployment managed-upgrade-operator NAME READY UP-TO-DATE AVAILABLE AGE managed-upgrade-operator 1/1 1 1 2m2s Configure the Managed Upgrade Operator\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: managed-upgrade-operator-config namespace: openshift-managed-upgrade-operator data: config.yaml: | configManager: source: LOCAL localConfigName: managed-upgrade-config watchInterval: 1 maintenance: controlPlaneTime: 90 ignoredAlerts: controlPlaneCriticals: - ClusterOperatorDown - ClusterOperatorDegraded upgradeWindow: delayTrigger: 30 timeOut: 120 nodeDrain: timeOut: 45 expectedNodeDrainTime: 8 scale: timeOut: 30 healthCheck: ignoredCriticals: - PrometheusRuleFailures - CannotRetrieveUpdates - FluentdNodeDown ignoredNamespaces: - openshift-logging - openshift-redhat-marketplace - openshift-operators - openshift-user-workload-monitoring - openshift-pipelines EOF Restart the Managed Upgrade Operator\noc -n openshift-managed-upgrade-operator \\ scale deployment managed-upgrade-operator --replicas=0 oc -n openshift-managed-upgrade-operator \\ scale deployment managed-upgrade-operator --replicas=1 Look for available Upgrades\nIf there output is nil there are no available upgrades and you cannot continue.\noc get clusterversion version -o jsonpath='{.status.availableUpdates}' Schedule an Upgrade\nSet the Channel and Version to the desired values from the above list of available upgrades.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: upgrade.managed.openshift.io/v1alpha1 kind: UpgradeConfig metadata: name: managed-upgrade-config namespace: openshift-managed-upgrade-operator spec: type: \"ARO\" upgradeAt: $(date -u --iso-8601=seconds --date \"+5 minutes\") PDBForceDrainTimeout: 60 capacityReservation: false desired: channel: \"stable-4.9\" version: \"4.9.27\" EOF Check the status of the scheduled upgrade\noc -n openshift-managed-upgrade-operator get \\ upgradeconfigs.upgrade.managed.openshift.io \\ managed-upgrade-config -o jsonpath='{.status}' | jq The output of this command should show upgrades in progress\n{ \"history\": [ { \"conditions\": [ { \"lastProbeTime\": \"2022-04-12T14:42:02Z\", \"lastTransitionTime\": \"2022-04-12T14:16:44Z\", \"message\": \"ControlPlaneUpgraded still in progress\", \"reason\": \"ControlPlaneUpgraded not done\", \"startTime\": \"2022-04-12T14:16:44Z\", \"status\": \"False\", \"type\": \"ControlPlaneUpgraded\" }, You can verify the upgrade has completed successfully via the following\noc get clusterversion version NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.9.27 True False 161m Cluster version is 4.9.27 ","description":"","tags":["ARO","Azure"],"title":"Enable the Managed Upgrade Operator in ARO and schedule Upgrades","uri":"/docs/aro/managed-upgrade-operator/"},{"content":"The Amazon Web Services Elastic File System (AWS EFS) is a Network File System (NFS) that can be provisioned on Red Hat OpenShift Service on AWS clusters. AWS also provides and supports a CSI EFS Driver to be used with Kubernetes that allows Kubernetes workloads to leverage this shared file storage.\nThis is a guide to quickly enable the EFS Operator on ROSA to\nSee here for the official ROSA documentation.\nPrerequisites A Red Hat OpenShift on AWS (ROSA) cluster The OC CLI The AWS CLI JQ Prepare AWS Account Get the Instance Name of one of your worker nodes\nNODE=$(oc get nodes --selector=node-role.kubernetes.io/worker \\ -o jsonpath='{.items[0].metadata.name}') Get the VPC ID of your worker nodes\nVPC=$(aws ec2 describe-instances \\ --filters \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{VpcId:VpcId}' \\ | jq -r '.[0][0].VpcId') Get subnets in your VPC\nSUBNET=$(aws ec2 describe-subnets \\ --filters Name=vpc-id,Values=$VPC Name=tag:kubernetes.io/role/internal-elb,Values='' \\ --query 'Subnets[*].{SubnetId:SubnetId}' \\ | jq -r '.[0].SubnetId') Get the CIDR block of your worker nodes\nCIDR=$(aws ec2 describe-vpcs \\ --filters \"Name=vpc-id,Values=$VPC\" \\ --query 'Vpcs[*].CidrBlock' \\ | jq -r '.[0]') Get the Security Group of your worker nodes\nSG=$(aws ec2 describe-instances --filters \\ \"Name=private-dns-name,Values=$NODE\" \\ --query 'Reservations[*].Instances[*].{SecurityGroups:SecurityGroups}' \\ | jq -r '.[0][0].SecurityGroups[0].GroupId') Add EFS to security group\naws ec2 authorize-security-group-ingress \\ --group-id $SG \\ --protocol tcp \\ --port 2049 \\ --cidr $CIDR | jq . Create EFS File System\nNote: You may want to create separate/additional access-points for each application/shared vol.\nEFS=$(aws efs create-file-system --creation-token efs-token-1 \\ --encrypted | jq -r '.FileSystemId') Configure Mount Target for EFS\nMOUNT_TARGET=$(aws efs create-mount-target --file-system-id $EFS \\ --subnet-id $SUBNET --security-groups $SG \\ | jq -r '.MountTargetId') Create Access Point for EFS\nACCESS_POINT=$(aws efs create-access-point --file-system-id $EFS \\ --client-token efs-token-1 \\ | jq -r '.AccessPointId') Deploy and test the AWS EFS Operator Install the EFS Operator\ncat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: labels: operators.coreos.com/aws-efs-operator.openshift-operators: \"\" name: aws-efs-operator namespace: openshift-operators spec: channel: stable installPlanApproval: Automatic name: aws-efs-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: aws-efs-operator.v0.0.8 EOF Create a namespace\noc new-project efs-demo Create a EFS Shared Volume\ncat \u003c\u003cEOF | oc apply -f - apiVersion: aws-efs.managed.openshift.io/v1alpha1 kind: SharedVolume metadata: name: e fs-volume namespa ce: efs-demo spec: accessP ointID: ${ACCESS_POINT} fileSys temID: ${EFS} EOF Create a POD to write to the EFS Volume\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-efs spec: volumes: - name: efs-storage-vol persistentVolumeClaim: claimName: pvc-efs-volume containers: - name: test-efs image: centos:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"while true; do echo 'hello efs' | tee -a /mnt/efs-data/verify-efs \u0026\u0026 sleep 5; done;\" ] volumeMounts: - mountPath: \"/mnt/efs-data\" name: efs-storage-vol EOF Create a POD to read from the EFS Volume\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-efs-read spec: volumes: - name: efs-storage-vol persistentVolumeClaim: claimName: pvc-efs-volume containers: - name: test-efs-read image: centos:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"tail -f /mnt/efs-data/verify-efs\" ] volumeMounts: - mountPath: \"/mnt/efs-data\" name: efs-storage-vol EOF Verify the second POD can read the EFS Volume\noc logs test-efs-read You should see a stream of “hello efs”\nhello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs hello efs Cleanup Delete the Pods\noc delete pod -n efs-demo test-efs test-efs-read Delete the Volume\noc delete -n efs-demo SharedVolume efs-volume Delete the Namespace\noc delete project efs-demo Delete the EFS Shared Volume via AWS\naws efs delete-mount-target --mount-target-id $MOUNT_TARGET | jq . aws efs delete-access-point --access-point-id $ACCESS_POINT | jq . aws efs delete-file-system --file-system-id $EFS | jq . ","description":"","tags":["AWS","ROSA"],"title":"Enabling the AWS EFS Operator on ROSA","uri":"/docs/rosa/aws-efs-operator-on-rosa/"},{"content":"Paul Czarkowski\n06/04/2021\nBy default Azure Red Hat OpenShift (ARO) stores metrics in Ephemeral volumes, and its advised that users do not change this setting. However its not unreasonable to expect that metrics should be persisted for a set amount of time.\nThis guide shows how to set up Thanos to federate both System and User Workload Metrics to a Thanos gateway that stores the metrics in Azure Files and makes them available via a Grafana instance (managed by the Grafana Operator).\nToDo - Add Authorization in front of Thanos APIs\nPre-Prequsites An ARO cluster\nSet some environment variables to use throughout to suit your environment\nNote: AZR_STORAGE_ACCOUNT_NAME must be unique\nexport AZR_RESOURCE_LOCATION=eastus export AZR_RESOURCE_GROUP=openshift export AZR_STORAGE_ACCOUNT_NAME=arofederatedmetrics export CLUSTER_NAME=openshift export NAMESPACE=aro-thanos-af Azure Preperation Create an Azure storage account\nmodify the arguments to suit your environment\naz storage account create \\ --name $AZR_STORAGE_ACCOUNT_NAME \\ --resource-group $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION \\ --sku Standard_RAGRS \\ --kind StorageV2 Get the account key and update the secret in thanos-store-credentials.yaml\nAZR_STORAGE_KEY=$(az storage account keys list -g $AZR_RESOURCE_GROUP \\ -n $AZR_STORAGE_ACCOUNT_NAME --query \"[0].value\" -o tsv) Create a namespace to use\noc new-project $NAMESPACE Add the MOBB chart repository to your Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories\nhelm repo update Use the mobb/operatorhub chart to deploy the grafana operator\nhelm upgrade -n $NAMESPACE $NAMESPACE-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values ./files/grafana-operator.yaml --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/aro-thanos-af/files/grafana-operator.yaml Use the mobb/operatorhub chart to deploy the resource-locker operator\n\u003e Note: Skip this if you already have the resource-locker operator installed, or if you do not plan to use User Workload Metrics\nhelm upgrade -n resource-locker-operator resource-locker-operator \\ mobb/operatorhub --version 0.1.1 --create-namespace --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/aro-thanos-af/files/resourcelocker-operator.yaml Deploy ARO Thanos Azure Files Helm Chart (mobb/aro-thanos-af)\n\u003e Note: enableUserWorkloadMetrics=true will overwrite configs for cluster and userworkload metrics, remove it from the helm command below if you already have custom settings. The Addendum at the end of this doc will explain the changes you’ll need to make instead.\nhelm upgrade -n $NAMESPACE aro-thanos-af --install mobb/aro-thanos-af --version 0.2.0 \\ --set \"aro.storageAccount=$AZR_STORAGE_ACCOUNT_NAME\" \\ --set \"aro.storageAccountKey=$AZR_STORAGE_KEY\" \\ --set \"aro.storageContainer=$CLUSTER_NAME\" \\ --set \"enableUserWorkloadMetrics=true\" Validate Grafana is installed and seeing metrics from Azure Files get the Route URL for Grafana (remember its https) and login using username root and the password you updated to (or the default of secret).\noc -n $NAMESPACE get route grafana-route Once logged in go to Dashboards-\u003eManage and expand the thanos-receiver group and you should see the cluster metrics dashboards. Click on the Use Method / Cluster Dashboard and you should see metrics. \\o/.\nNote: If it complains about a missing datasource run the following: oc annotate -n $NAMESPACE grafanadatasource aro-thanos-af-prometheus \"retry=1\"\nCleanup Uninstall the aro-thanos-af chart\nhelm delete -n $NAMESPACE aro-thanos-af Uninstall the federated-metrics-operators chart\nhelm delete -n $NAMESPACE federated-metrics-operators Delete the aro-thanos-af namespace\noc delete namespace $NAMESPACE Delete the storage account\naz storage account delete \\ --name $AZR_STORAGE_ACCOUNT_NAME \\ --resource-group $AZR_RESOURCE_GROUP Addendum Enabling User Workload Monitoring See docs for more indepth details.\nCheck the cluster-monitoring-config ConfigMap object\noc -n openshift-monitoring get configmap cluster-monitoring-config -o yaml Enable User Workload Monitoring by doing one of the following\nIf the data.config.yaml is not {} you should edit it and add the enableUserWorkload: true line manually.\noc -n openshift-monitoring edit configmap cluster-monitoring-config Otherwise if its {} then you can run the following command safely.\noc patch configmap cluster-monitoring-config -n openshift-monitoring \\ -p='{\"data\":{\"config.yaml\": \"enableUserWorkload: true\\n\"}}' Check that the User workload monitoring is starting up\noc -n openshift-user-workload-monitoring get pods Append remoteWrite settings to the user-workload-monitoring config to forward user workload metrics to Thanos.\nCheck if the User Workload Config Map exists:\noc -n openshift-user-workload-monitoring get \\ configmaps user-workload-monitoring-config If the config doesn’t exist run:\ncat « EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: remoteWrite: - url: “http://thanos-receive.$NAMESPACE.svc.cluster.local:9091/api/v1/receive” EOF ```\n**Otherwise update it with the following:** ```bash oc -n openshift-user-workload-monitoring edit \\ configmaps user-workload-monitoring-config ``` ```yaml data: config.yaml: | ... prometheus: ... remoteWrite: - url: \"http://thanos-receive.thanos-receiver.svc.cluster.local:9091/api/v1/receive\" ``` ","description":"","tags":["ARO","Azure"],"title":"Federating System and User metrics to Azure Files in Azure Red Hat OpenShift","uri":"/docs/aro/federated-metrics/"},{"content":"Paul Czarkowski\n06/07/2021\nThis guide walks through setting up federating Prometheus metrics to S3 storage.\nToDo - Add Authorization in front of Thanos APIs\nPrerequisites A ROSA cluster deployed with STS aws CLI Set up environment Create environment variables\nexport CLUSTER_NAME=my-cluster export S3_BUCKET=my-thanos-bucket export REGION=us-east-2 export NAMESPACE=federated-metrics export SA=aws-prometheus-proxy export SCRATCH_DIR=/tmp/scratch export OIDC_PROVIDER=$(oc get authentication.config.openshift.io cluster -o json | jq -r .spec.serviceAccountIssuer| sed -e \"s/^https:\\/\\///\") export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) export AWS_PAGER=\"\" rm -rf $SCRATCH_DIR mkdir -p $SCRATCH_DIR Create namespace\noc new-project $NAMESPACE AWS Preperation Create an S3 bucket\naws s3 mb s3://$S3_BUCKET Create a Policy for access to S3\ncat «EOF \u003e $SCRATCH_DIR/s3-policy.json { “Version”: “2012-10-17”, “Statement”: [ { “Sid”: “Statement”, “Effect”: “Allow”, “Action”: [ “s3:ListBucket”, “s3:GetObject”, “s3:DeleteObject”, “s3:PutObject”, “s3:PutObjectAcl” ], “Resource”: [ “arn:aws:s3:::$S3_BUCKET/*”, “arn:aws:s3:::$S3_BUCKET” ] } ] } EOF ```\nApply the Policy\nS3_POLICY=$(aws iam create-policy --policy-name $CLUSTER_NAME-thanos \\ --policy-document file://$SCRATCH_DIR/s3-policy.json \\ --query 'Policy.Arn' --output text) echo $S3_POLICY Create a Trust Policy\ncat «EOF \u003e $SCRATCH_DIR/TrustPolicy.json { “Version”: “2012-10-17”, “Statement”: [ { “Effect”: “Allow”, “Principal”: { “Federated”: “arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}” }, “Action”: “sts:AssumeRoleWithWebIdentity”, “Condition”: { “StringEquals”: { “${OIDC_PROVIDER}:sub”: [ “system:serviceaccount:${NAMESPACE}:${SA}” ] } } } ] } EOF ```\nCreate Role for AWS Prometheus and CloudWatch\nS3_ROLE=$(aws iam create-role \\ --role-name \"$CLUSTER-thanos-s3\" \\ --assume-role-policy-document file://$SCRATCH_DIR/TrustPolicy.json \\ --query \"Role.Arn\" --output text) echo $S3_ROLE Attach the Policies to the Role\naws iam attach-role-policy \\ --role-name \"$CLUSTER-thanos-s3\" \\ --policy-arn $S3_POLICY Deploy Operators Add the MOBB chart repository to your Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your repositories\nhelm repo update Use the mobb/operatorhub chart to deploy the needed operators\nhelm upgrade -n $echNAMESPACE custom-metrics-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/rosa-thanos-s3/files/operatorhub.yaml Deploy Thanos Store Gateway Deploy ROSA Thanos S3 Helm Chart\nhelm upgrade -n $NAMESPACE rosa-thanos-s3 --install mobb/rosa-thanos-s3 \\ --set \"aws.roleArn=$ROLE_ARN\" \\ --set \"rosa.clusterName=$CLUSTER_NAME\" Append remoteWrite settings to the user-workload-monitoring config to forward user workload metrics to Thanos.\nCheck if the User Workload Config Map exists:\noc -n openshift-user-workload-monitoring get \\ configmaps user-workload-monitoring-config If the config doesn’t exist run:\ncat « EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: remoteWrite: - url: “http://thanos-receive.${NAMESPACE}.svc.cluster.local:9091/api/v1/receive” EOF ```\n**Otherwise update it with the following:** ```bash oc -n openshift-user-workload-monitoring edit \\ configmaps user-workload-monitoring-config ``` ```yaml data: config.yaml: | ... prometheus: ... remoteWrite: - url: \"http://thanos-receive.thanos-receiver.svc.cluster.local:9091/api/v1/receive\" ``` Check metrics are flowing by logging into Grafana get the Route URL for Grafana (remember its https) and login using username root and the password you updated to (or the default of secret).\noc -n thanos-receiver get route grafana-route Once logged in go to Dashboards-\u003eManage and expand the federated-metrics group and you should see the cluster metrics dashboards. Click on the Use Method / Cluster Dashboard and you should see metrics. \\o/.\n","description":"","tags":["AWS","ROSA"],"title":"Federating System and User metrics to S3 in Red Hat OpenShift for AWS","uri":"/docs/rosa/federated-metrics/"},{"content":"","description":"","tags":null,"title":"GCP","uri":"/tags/gcp/"},{"content":"Please refer to the The Managed OpenShift Black Belt team maintained Helm chart at here.\nAuthor: Paul Czarkowski\n","description":"","tags":["ARO","Azure"],"title":"Helm Chart to set up extra MachineSets on ARO clusters","uri":"/docs/aro/aro-machinesets/readme/"},{"content":" see here for public clusters.\nThis assumes you’ve already got a private ARO cluster installed. You could also follow the same instructions to create a public Astronomer, just use a regular DNS zone and skip the private parts.\nA default 3-node cluster is a bit small for Astronomer, If you have a three node cluster you can increase it by updating the replicas count machinesets in the openshift-machine-api namespace.\nCreate a private DNS Log into Azure and click to private dns\nClick + Add\nSet the Resource Group to match your ARO Resource Group\nSet Name to your TLD (astro.mobb.ninja in the example)\nClick Review and Create and create the Zone\nInside the Domain settings click Virtual network links -\u003e + Add\nLink Name: astro-aro\nSelect the correct Subscription and Network from the dropdown boxes\nClick OK\nCreate TLS Secret Next we need a TLS Secret to use. You could create a self-signed certificate using a CA that you own, or use certbot (if you have a valid DNS provider, note records don’t need to be public)\ncertbot certonly --manual \\ --preferred-challenges=dns \\ --email username.taken@gmail.com \\ --server https://acme-v02.api.letsencrypt.org/directory \\ --agree-tos \\ --manual-public-ip-logging-ok \\ -d \"*.astro.mobb.ninja\" Follow certbot’s instructions (something like ):\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Please deploy a DNS TXT record under the name _acme-challenge.astro.mobb.ninja with the following value: 8d2HNuZ8rn9McPTzpo2evJsAJI8K4eJuVLaZlz6d-kc Before continuing, verify the record is deployed. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Create a Secret from the Cert (use the paths provided from the above command):\noc new-project astronomer oc create secret tls astronomer-tls --cert=/etc/letsencrypt/live/astro.mobb.ninja/fullchain.pem --key=/etc/letsencrypt/live/astro.mobb.ninja/privkey.pem Deploy Astronomer update the values.yaml and set baseDomain: astro.mobb.ninja\nInstall\nhelm repo add astronomer https://helm.astronomer.io/ helm repo update helm install -f values.yaml --version=0.25.2 \\ --namespace=astronomer astronomer \\ astronomer/astronomer While that’s running add our DNS In another shell run\nkubectl get svc -n astronomer astronomer-nginx Go back to your private DNS zone in Azure and create a record set * and copy the contents of EXTERNAL-IP from the above command.\nFix SCCs for elasticsearch oc adm policy add-scc-to-user privileged -z astronomer-elasticsearch oc patch deployment astronomer-elasticsearch-client -p '{\"spec\":{\"template\":{\"spec\":{ \"containers\": [{\"name\": \"es-client\",\"securityContext\":{\"privileged\": true,\"runAsUser\": 0}}]}}}}' Validate the Install Check the Helm install has finished\nNAME: astronomer LAST DEPLOYED: Mon May 24 18:03:05 2021 NAMESPACE: astronomer STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing Astronomer! Your release is named astronomer. The platform components may take a few minutes to spin up. You can access the platform at: - Astronomer dashboard: https://app.astro.mobb.ninja - Grafana dashboard: https://grafana.astro.mobb.ninja - Kibana dashboard: https://kibana.astro.mobb.ninja Now that you've installed the platform, you are ready to get started and create your first airflow deployment. Download the CLI: curl -sSL https://install.astro.mobb.ninja | sudo bash We have guides available at https://www.astronomer.io/guides/ and are always available to help. Since this is a private LB you’ll need to access it from inside the network. The quick hacky way to do this is\nkubectl exec -ti astronomer-cli-install-6f899c87d5-2c84f -- wget -O - https://install.astro.mobb.ninja and you should see\n#! /usr/bin/env bash TAG=${1:-v0.20.0} if (( EUID != 0 )); then echo \"Please run command as root.\" exit fi DOWNLOADER=\"https://raw.githubusercontent.com/astronomer/astro-cli/main/godownloader.sh\" ","description":"","tags":["ARO","Azure"],"title":"Installing Astronomer on a private ARO cluster","uri":"/docs/aro/astronomer/"},{"content":"This document explain how to integrate ARO cluster with Azure Arc-enabled Kubernetes. When you connect a Kubernetes/OpenShift cluster with Azure Arc, it will:\nBe represented in Azure Resource Manager with a unique ID Be placed in an Azure subscription and resource group Receive tags just like any otherAzure resource Azure Arc-enabled Kubernetes supports the following scenarios for connected clusters:\nConnect Kubernetes running outside of Azure for inventory, grouping, and tagging. Deploy applications and apply configuration using GitOps-based configuration management. View and monitor your clusters using Azure Monitor for containers. Enforce threat protection using Microsoft Defender for Kubernetes. Apply policy definitions using Azure Policy for Kubernetes. Use Azure Active Directory for authentication and authorization checks on your cluster Prerequisites a public ARO cluster azure cli oc cli An identity (user or service principal) which can be used to log in to Azure CLI and connect your cluster to Azure Arc. Enable Extensions and Plugins Install the connectedk8s Azure Cli extension of version \u003e= 1.2.0\naz extension add --name \"connectedk8s\" az extension add --name \"k8s-configuration\" az extension add --name \"k8s-extension\" Register providers for Azure Arc-enabled Kubernetes. Registration may take up to 5 minutes.\naz provider register --namespace Microsoft.Kubernetes az provider register --namespace Microsoft.KubernetesConfiguration az provider register --namespace Microsoft.ExtendedLocation Connect an existing ARO cluster Make sure you are logged into your ARO cluster\nkubeadmin_password=$(az aro list-credentials --name \u003c\u003ccluster name\u003e\u003e --resource-group \u003c\u003cresource group name\u003e\u003e --query kubeadminPassword --output tsv) apiServer=$(az aro show -g \u003c\u003cresource group name\u003e\u003e -n \u003c\u003ccluster name\u003e\u003e --query apiserverProfile.url -o tsv) oc login $apiServer -u kubeadmin -p $kubeadmin_password Run the following command:\naz connectedk8s connect --resource-group $resourceGroupName --name $clusterName --distribution openshift --infrastructure auto After running the commnad. grant the following permissions and restart kube-aad-proxy pod\noc project azure-arc oc adm policy add-scc-to-user privileged system:serviceaccount:azure-arc:azure-arc-kube-aad-proxy-sa oc get pod | grep kube-aad-proxy-6d9b66b9cd-g27xr 0/2 ContainerCreating 0 26s oc delete pod kube-aad-proxy-6d9b66b9cd-g27xr Wait for a few mins and you will see all the pods in azure-arc namespace running\noc get pods NAME READY STATUS RESTARTS AGE cluster-metadata-operator-7dfd94949c-wtvjw 2/2 Running 0 4m47s clusterconnect-agent-7d78db9859-wzthd 3/3 Running 0 4m47s clusteridentityoperator-7b96bcb448-hzthh 2/2 Running 0 4m47s config-agent-dbf66bbc7-r27qs 2/2 Running 0 4m47s controller-manager-67547546f-cmlb9 2/2 Running 0 4m47s extension-manager-548c9d7d6b-jrrdn 2/2 Running 0 4m47s flux-logs-agent-bb994c74f-m5gdc 1/1 Running 0 4m47s kube-aad-proxy-6d9b66b9cd-g27xr 2/2 Running 0 3m16s metrics-agent-7d794679c6-k4b7g 2/2 Running 0 4m47s resource-sync-agent-bb79c44b8-5brjr 2/2 Running 0 4m47s This commands take about 5 mins to complete. Upon the completion of the command you should see the following output and your cluster under Kubernetes - Azure Arc service in Azure Portal\n{ \"agentPublicKeyCertificate\": \"MIICCgKCAgEArNXWSoWVg7q/W5t7vwY24Y8c+dRxy3we/EIRryXx1Orl8GEX94BsHJqvP0iW6ANZ0qoWE675+NR6V3nDMSkis5/aSYMQ8/yWMcUzieKwFfFmTSfCpkzwxy6PSbdRjMwK5H3DDOOXyRQcJV557F5FjHVYfC/0DkPYdhfepcVade+HgOwOOJH28hSNw58pWo/GNNmcwtzFPVdx/TM574CbNVz4OdrtsMy7FKKC63lYW+W3wkzFOqB+qPaITwqwzkruIoSi5HIatONoCPijdTLm3+RoK/CbTYqzHEEId8gFFJd+J4qfSeCYu6jeDNOpwt8DKDLFLvv04oHyxm+Nr34xPBm3+sjggvkLQ5UWpGZ9h7jWTEP2pWEcXF0KqAqAEFPBOOqDKEaYfLtJSJ/yExS1otydDCJEZ1sRPvsjdH5f0DKVXPHgiDa4SoLXomqkarF3g9i6CEK/XE9JTVa8WBJT6wXdXBa0xh8EnzZ9uyVuY1k/2L7d4BR5+sIjqtcDfRSVtxN+LNxgqpo20ltXM1hWkd8WacK7VY+t2lxbYf01zhXWOpaBGgeAMqxqqcHeQor2vzA9PENYYr5zo8eP1LcySmC4LIFiDfN1NxAiZ5SCnrorNFbmrgEDFnWvZzdu2w4r55fsV9qnozUjn6iRqByhyMoeLn5EZLLK5zsW8sA/CeUCAwEAAQ==\", \"agentVersion\": null, \"connectivityStatus\": \"Connecting\", \"distribution\": \"OpenShift\", \"id\": \"/subscriptions/e7f88b1a-04fc-4d00-ace9-eec077a5d6af/resourceGroups/sazed-aro-cluster/providers/Microsoft.Kubernetes/connectedClusters/sazed-aro-cluster\", \"identity\": { \"principalId\": \"xxxx-xxxx-xxxx-xxxx\", \"tenantId\": \"xxxx-xxxx-xxxx-xxxx\", \"type\": \"SystemAssigned\" }, \"infrastructure\": \"azure\", \"kubernetesVersion\": null, \"lastConnectivityTime\": null, \"location\": \"eastus\", \"managedIdentityCertificateExpirationTime\": null, \"name\": \"sazed-aro-cluster\", \"offering\": null, \"provisioningState\": \"Succeeded\", \"resourceGroup\": \"sazed-aro-cluster\", \"systemData\": { \"createdAt\": \"2022-09-15T19:23:40.540376+00:00\", \"createdBy\": \"sazed@redhat.com\", \"createdByType\": \"User\", \"lastModifiedAt\": \"2022-09-15T19:23:40.540376+00:00\", \"lastModifiedBy\": \"sazed@redhat.com\", \"lastModifiedByType\": \"User\" }, \"tags\": {}, \"totalCoreCount\": null, \"totalNodeCount\": null, \"type\": \"microsoft.kubernetes/connectedclusters\" } To check the status of clusters connected to Azure ARC, run the following command\naz connectedk8s list --resource-group \u003c\u003cresource group\u003e\u003e --output table Name Location ResourceGroup ------------------- ---------- ------------------- \u003c\u003c cluster name \u003e\u003e\u003e eastus \u003c\u003c resource group \u003e\u003e Enable observability In order to see ARO resource inside Azure Arc, you need to create a service account and provide it to Azure Arc.\noc project azure-arc oc create serviceaccount azure-arc-observability oc create clusterrolebinding azure-arc-observability-rb --clusterrole cluster-admin --serviceaccount azure-arc:azure-arc-observability apiVersion: v1 kind: Secret metadata: name: azure-arc-observability-secret namespace: azure-arc annotations: kubernetes.io/service-account.name: azure-arc-observability type: kubernetes.io/service-account-token oc apply -f azure-arc-secret.yaml TOKEN=$(oc get secret azure-arc-observability-secret -o jsonpath='{$.data.token}' | base64 -d | sed 's/$/\\\\n/g') echo $TOKEN Copy the token, goto Azure portal and select your cluster under “Kubernetes - Azure Arc” Select Namespaces from the left side menu and paste the token in “Service account bearer token” input field.\nNow you can see all of your ARO rearouses inside ARC UI. you can see the following resources inside Azure ARC portal:\nNamespaces Workloads Services and Ingress Storage Configurations Access Secrets from Azure Key Vault The Azure Key Vault Provider for Secrets Store CSI Driver allows for the integration of Azure Key Vault as a secrets store with a Kubernetes cluster via a CSI volume. For Azure Arc-enabled Kubernetes clusters, you can install the Azure Key Vault Secrets Provider extension to fetch secrets.\nInstall extension az k8s-extension create --cluster-name \u003c\u003ccluster name\u003e\u003e --resource-group \u003c\u003cresource group\u003e\u003e --cluster-type connectedClusters --extension-type Microsoft.AzureKeyVaultSecretsProvider --name akvsecretsprovider { \"aksAssignedIdentity\": null, \"autoUpgradeMinorVersion\": true, \"configurationProtectedSettings\": {}, \"configurationSettings\": {}, \"customLocationSettings\": null, \"errorInfo\": null, \"extensionType\": \"microsoft.azurekeyvaultsecretsprovider\", \"id\": \"/subscriptions/e7f88b1a-04fc-4d00-ace9-eec077a5d6af/resourceGroups/sazed-aro-cluster/providers/Microsoft.Kubernetes/connectedClusters/sazed-aro-cluster-1/providers/Microsoft.KubernetesConfiguration/extensions/akvsecretsprovider\", \"identity\": { \"principalId\": \"xxxx-xxxx-xxxx-xxxx\", \"tenantId\": null, \"type\": \"SystemAssigned\" }, \"installedVersion\": null, \"name\": \"akvsecretsprovider\", \"packageUri\": null, \"provisioningState\": \"Succeeded\", \"releaseTrain\": \"Stable\", \"resourceGroup\": \"sazed-aro-cluster\", \"scope\": { \"cluster\": { \"releaseNamespace\": \"kube-system\" }, \"namespace\": null }, \"statuses\": [], \"systemData\": { \"createdAt\": \"2022-09-15T20:45:47.152390+00:00\", \"createdBy\": null, \"createdByType\": null, \"lastModifiedAt\": \"2022-09-15T20:45:47.152390+00:00\", \"lastModifiedBy\": null, \"lastModifiedByType\": null }, \"type\": \"Microsoft.KubernetesConfiguration/extensions\", \"version\": \"1.3.0\" } Validate the extension installation\naz k8s-extension show --cluster-type connectedClusters --cluster-name \u003c\u003ccluster name\u003e\u003e --resource-group \u003c\u003cresource group\u003e\u003e --name akvsecretsprovider { \"aksAssignedIdentity\": null, \"autoUpgradeMinorVersion\": true, \"configurationProtectedSettings\": {}, \"configurationSettings\": {}, \"customLocationSettings\": null, \"errorInfo\": null, \"extensionType\": \"microsoft.azurekeyvaultsecretsprovider\", \"id\": \"/subscriptions/e7f88b1a-04fc-4d00-ace9-eec077a5d6af/resourceGroups/sazed-aro-cluster-1/providers/Microsoft.Kubernetes/connectedClusters/sazed-aro-cluster-1/providers/Microsoft.KubernetesConfiguration/extensions/akvsecretsprovider\", \"identity\": { \"principalId\": \"xxxx-xxxx-xxxx-xxxx\", \"tenantId\": null, \"type\": \"SystemAssigned\" }, \"installedVersion\": null, \"name\": \"akvsecretsprovider\", \"packageUri\": null, \"provisioningState\": \"Succeeded\", \"releaseTrain\": \"Stable\", \"resourceGroup\": \"sazed-aro-cluster\", \"scope\": { \"cluster\": { \"releaseNamespace\": \"kube-system\" }, \"namespace\": null }, \"statuses\": [], \"systemData\": { \"createdAt\": \"2022-09-15T20:45:47.152390+00:00\", \"createdBy\": null, \"createdByType\": null, \"lastModifiedAt\": \"2022-09-15T20:45:47.152390+00:00\", \"lastModifiedBy\": null, \"lastModifiedByType\": null }, \"type\": \"Microsoft.KubernetesConfiguration/extensions\", \"version\": \"1.3.0\" } Create or Select an Azure Key Vault az keyvault create -n \u003c\u003ccluster name\u003e\u003e -g \u003c\u003cresource group\u003e\u003e -l eastus az keyvault secret set --vault-name \u003c\u003ccluster name\u003e\u003e -n DemoSecret --value MyExampleSecret Provide identity to access Azure Key Vault Currently, the Secrets Store CSI Driver on Arc-enabled clusters can be accessed through a service principal. Follow the steps below to provide an identity that can access your Key Vault.\nUse the provided Service Principal credentials provided with the lab and create a secret in ARO cluster\noc create secret generic secrets-store-creds --from-literal clientid=\"\u003cclient-id\u003e\" --from-literal clientsecret=\"\u003cclient-secret\u003e\" oc label secret secrets-store-creds secrets-store.csi.k8s.io/used=true Create a SecretProviderClass with the following YAML, filling in your values for key vault name, tenant ID, and objects to retrieve from your AKV instance\napiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: akvprovider-demo spec: provider: azure parameters: usePodIdentity: \"false\" keyvaultName: \u003ckey-vault-name\u003e objects: | array: - | objectName: DemoSecret objectType: secret objectVersion: \"\" tenantId: \u003ctenant-Id\u003e oc apply -f azure-arc-secretproviderclass.yaml Create a pod with the following YAML, filling in the name of your identity\nkind: Pod apiVersion: v1 metadata: name: secret-store-pod spec: containers: - name: busybox image: k8s.gcr.io/e2e-test-images/busybox:1.29 command: - \"/bin/sleep\" - \"10000\" volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"akvprovider-demo\" nodePublishSecretRef: name: secrets-store-creds oc apply -f azure-arc-pod.yaml Validate the secrets After the pod starts, the mounted content at the volume path specified in your deployment YAML is available.\n## show secrets held in secrets-store oc exec secret-store-pod -- ls /mnt/secrets-store/ DemoSecret ## print a test secret 'DemoSecret' held in secrets-store oc exec secret-store-pod -- cat /mnt/secrets-store/DemoSecret MyExampleSecret Enable log aggregation In order to collect logs from ARO cluster and store it in Azure ARC. configure azure monitor\nCreate Azure Log Analytics Workspace\naz monitor log-analytics workspace create --resource-group \u003c\u003csame as above\u003e\u003e --workspace-name loganalyticsworkspace Goto Azure ARC portal and click on logs\nClick on configure azure monitor button and select the workspace created in last step and click on configure.\nNow you can go see logs and metrics for your cluster.\nMonitor ARO cluster against Goverance Policies Azure Policy extends Gatekeeper v3, an admission controller webhook for Open Policy Agent (OPA), to apply at-scale enforcements and safeguards on your clusters in a centralized, consistent manner. Azure Policy makes it possible to manage and report on the compliance state of your Kubernetes clusters from one place. The add-on enacts the following functions:\nChecks with Azure Policy service for policy assignments to the cluster. Deploys policy definitions into the cluster as constraint template and constraint custom resources. Reports auditing and compliance details back to Azure Policy service. Azure policy plugin is enabled when you connect your ARO cluster with Azure ARC. you can click on go to Azure Policies to look at the policies assigned to your cluster, check their status and attach more policies.\n","description":"","tags":["ARO","Azure"],"title":"Integrating Azure ARC with ARO","uri":"/docs/aro/azure-arc-integration/"},{"content":"Kevin Collins\n06/28/2022\nNote: This guide demonstrates how to setup and configure self-managed OpenShift Data Foundation in Internal Mode on an ARO Cluster and test it out.\nPrerequisites An Azure Red Hat OpenShift cluster ( verion 4.10+ ) kubectl cli oc cli moreutils (sponge) jq Install compute nodes for ODF A best practice for optimal performance is to run ODF on dedicated nodes with a minimum of one per zone. In this guide, we will be provisioning 3 additional compute nodes, one per zone. Run the following script to create the additional nodes:\nLog into your ARO Cluster\nCreate the new compute nodes\nfor ZONE in 1 2 3 do item=$((ZONE-1)) MACHINESET=$(oc get machineset -n openshift-machine-api -o=jsonpath=\"{.items[$item]}\" | jq -r '[.metadata.name] | @tsv') oc get machineset -n openshift-machine-api $MACHINESET -o json \u003e default_machineset$ZONE.json worker=odf-worker-$ZONE jq \".metadata.name = \\\"$worker\\\"\" default_machineset$ZONE.json | sponge default_machineset$ZONE.json jq '.spec.replicas = 1' default_machineset$ZONE.json| sponge default_machineset$ZONE.json jq \".spec.selector.matchLabels.\\\"machine.openshift.io/cluster-api-machineset\\\" = \\\"$worker\\\"\" default_machineset$ZONE.json| sponge default_machineset$ZONE.json jq \".spec.template.metadata.labels.\\\"machine.openshift.io/cluster-api-machineset\\\" = \\\"$worker\\\"\" default_machineset$ZONE.json| sponge default_machineset$ZONE.json jq '.spec.template.spec.providerSpec.value.vmSize = \"Standard_D16s_v3\"' default_machineset$ZONE.json | sponge default_machineset$ZONE.json jq \".spec.template.spec.providerSpec.value.zone = \\\"$ZONE\\\"\" default_machineset$ZONE.json | sponge default_machineset$ZONE.json jq 'del(.status)' default_machineset$ZONE.json | sponge default_machineset$ZONE.json oc create -f default_machineset$ZONE.json done Label new compute nodes\nIt takes just a couple of minutes for new nodes to appear.\nCheck if the nodes are ready:\noc get nodes | grep odf-worker expected output:\nodf-worker-1-jg7db Ready worker 19h v1.23.5+3afdacb odf-worker-2-ktvct Ready worker 19h v1.23.5+3afdacb odf-worker-3-rk22b Ready worker 19h v1.23.5+3afdacb Once you see the three nodes, the next step we need to do is label and taint the nodes. This will ensure the OpenShift Data Foundation is installed on these nodes and no other workload will be placed on the nodes.\nfor worker in $(oc get nodes | grep odf-worker | awk '{print $1}') do oc label node $worker cluster.ocs.openshift.io/openshift-storage=`` oc adm taint nodes $worker node.ocs.openshift.io/storage=true:NoSchedule done Deploy OpenShift Data Foundation Next, we will install OpenShift Data Foundation via an Operator.\nCreate the openshift-storage namespace cat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Namespace metadata: labels: openshift.io/cluster-monitoring: \"true\" name: openshift-storage spec: {} EOF Create the Operator Group for openshift-storage cat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: openshift-storage-operatorgroup namespace: openshift-storage spec: targetNamespaces: - openshift-storage EOF Subscribe to the ocs-operator cat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ocs-operator namespace: openshift-storage spec: channel: \"stable-4.10\" # \u003c-- Channel should be modified depending on the OCS version to be installed. Please ensure to maintain compatibility with OCP version installPlanApproval: Automatic name: ocs-operator source: redhat-operators # \u003c-- Modify the name of the redhat-operators catalogsource if not default sourceNamespace: openshift-marketplace EOF Subscribe to the odf-operator cat \u003c\u003cEOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: odf-operator namespace: openshift-storage spec: channel: \"stable-4.10\" # \u003c-- Channel should be modified depending on the OCS version to be installed. Please ensure to maintain compatibility with OCP version installPlanApproval: Automatic name: odf-operator source: redhat-operators # \u003c-- Modify the name of the redhat-operators catalogsource if not default sourceNamespace: openshift-marketplace EOF Create a Storage Cluster cat \u003c\u003cEOF | oc apply -f - apiVersion: ocs.openshift.io/v1 kind: StorageCluster metadata: annotations: uninstall.ocs.openshift.io/cleanup-policy: delete uninstall.ocs.openshift.io/mode: graceful generation: 2 name: ocs-storagecluster namespace: openshift-storage spec: storageDeviceSets: - config: {} count: 1 dataPVCTemplate: spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Ti storageClassName: managed-premium volumeMode: Block name: ocs-deviceset-managed-premium portable: true replica: 3 version: 4.10.0 EOF Validate the install List the cluster service version for the ODF operators\noc get csv -n openshift-storage verify that the operators below have succeeded.\nNAME DISPLAY VERSION REPLACES PHASE mcg-operator.v4.10.4 NooBaa Operator 4.10.4 Succeeded ocs-operator.v4.10.4 OpenShift Container Storage 4.10.4 Succeeded odf-operator.v4.10.4 OpenShift Data Foundation 4.10.4 Succeeded Check that the ocs storage classes have been created\nnote: this can take around 5 minutes\noc get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE managed-csi disk.csi.azure.com Delete WaitForFirstConsumer true 118m managed-premium (default) kubernetes.io/azure-disk Delete WaitForFirstConsumer true 119m ocs-storagecluster-ceph-rbd openshift-storage.rbd.csi.ceph.com Delete Immediate true 7s ocs-storagecluster-cephfs openshift-storage.cephfs.csi.ceph.com Delete Immediate true 7s Test it out To test out ODF, we will create ‘writer’ pods on each node across all zones and then a reader pod to read the data that is written. This will prove both regional storage along with “read write many” mode is working correctly.\nCreate a new project\noc new-project odf-demo Create a RWX Persistent Volume Claim for ODF\ncat \u003c\u003cEOF | kubectl apply -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: standard spec: accessModes: - ReadWriteMany resources: requests: storage: 400Gi storageClassName: ocs-storagecluster-cephfs EOF Create writer pods via a DaemonSet Using a deamonset will ensure that we have a ‘writer pod’ on each worker node and will also prove that we correctly set a taint on the ‘ODF Workers’ where which we do not want workload to be added to.\nThe writer pods will write out which worker node the pod is running on, the data, and a hello message.\ncat \u003c\u003cEOF | oc apply -f - apiVersion: apps/v1 kind: DaemonSet metadata: name: test-odf labels: app: test-odf spec: selector: matchLabels: name: test-odf template: metadata: labels: name: test-odf spec: containers: - name: azodf image: centos:latest command: [\"sh\", \"-c\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [ \"while true; do printenv --null NODE_NAME | tee -a /mnt/odf-data/verify-odf; echo ' says hello '$(date) | tee -a /mnt/odf-data/verify-odf; sleep 15; done;\", ] volumeMounts: - name: odf-vol mountPath: \"/mnt/odf-data\" env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName volumes: - name: odf-vol persistentVolumeClaim: claimName: standard EOF Check the writer pods are running.\nnote: there should be 1 pod per non-ODF worker node\noc get pods expected output\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-odf-47p2g 1/1 Running 0 107s 10.128.2.15 aro-kmobb-7zff2-worker-eastus1-xgksq \u003cnone\u003e \u003cnone\u003e test-odf-p5xk6 1/1 Running 0 107s 10.131.0.18 aro-kmobb-7zff2-worker-eastus3-h4gv7 \u003cnone\u003e \u003cnone\u003e test-odf-ss8b5 1/1 Running 0 107s 10.129.2.32 aro-kmobb-7zff2-worker-eastus2-sbfpm \u003cnone\u003e \u003cnone\u003e Create a reader pod The reader pod will simply log data written by the writer pods.\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-odf-read spec: containers: - name: test-odf-read image: centos:latest command: [\"/bin/bash\", \"-c\", \"--\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [\"tail -f /mnt/odf-data/verify-odf\"] volumeMounts: - name: odf-vol mountPath: \"/mnt/odf-data\" volumes: - name: odf-vol persistentVolumeClaim: claimName: standard EOF Now let’s verify the POD is reading from shared volume.\noc logs test-odf-read aro-kmobb-7zff2-worker-eastus1-xgksq says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus3-h4gv7 says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus2-sbfpm says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus1-xgksq says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus3-h4gv7 says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus2-sbfpm says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus1-xgksq says hello Wed Jun 29 10:41:06 EDT 2022 aro-kmobb-7zff2-worker-eastus3-h4gv7 says hello Wed Jun 29 10:41:06 EDT 2022 Notice that pods in different zones are writing to the PVC which is managed by ODF.\n","description":"","tags":["ARO","Azure"],"title":"OpenShift Data Foundation with ARO","uri":"/docs/aro/odf/"},{"content":"A guid to shipping logs and metrics on OpenShift\nAuthor: Aaron Aldrich\nPrerequisites OpenShift CLI (oc) Rights to install operators on the cluster Setup OpenShift Logging This is for setup of centralized logging on OpenShift making use of Elasticsearch OSS edition. This largely follows the processes outlined in the OpenShift documentation here. Retention and storage considerations are reviewed in Red Hat’s primary source documentation.\nThis setup is primarily concerned with simplicity and basic log searching. Consequently it is insufficient for long-lived retention or for advanced visualization of logs. For more advanced observability setups, you’ll want to look at Forwarding Logs to Third Party Systems\nCreate a namespace for the OpenShift Elasticsearch Operator.\nThis is necessary to avoid potential conflicts with community operators that could send similarly named metrics/logs into the stack.\noc create -f - \u003c\u003cEOF apiVersion: v1 kind: Namespace metadata: name: openshift-operators-redhat annotations: openshift.io/node-selector: \"\" labels: openshift.io/cluster-monitoring: \"true\" EOF Create a namespace for the OpenShift Logging Operator\noc create -f - \u003c\u003cEOF apiVersion: v1 kind: Namespace metadata: name: openshift-logging annotations: openshift.io/node-selector: \"\" labels: openshift.io/cluster-monitoring: \"true\" EOF Install the OpenShift Elasticsearch Operator by creating the following objects:\nOperator Group for OpenShift Elasticsearch Operator\noc create -f - \u003c\u003cEOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: openshift-operators-redhat namespace: openshift-operators-redhat spec: {} EOF Subscription object to subscribe a Namespace to the OpenShift Elasticsearch Operator\noc create -f - \u003c\u003cEOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: \"elasticsearch-operator\" namespace: \"openshift-operators-redhat\" spec: channel: \"stable\" installPlanApproval: \"Automatic\" source: \"redhat-operators\" sourceNamespace: \"openshift-marketplace\" name: \"elasticsearch-operator\" EOF Verify Operator Installation\noc get csv --all-namespaces Example Output\nNAMESPACE NAME DISPLAY VERSION REPLACES PHASE default elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded kube-node-lease elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded kube-public elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded kube-system elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-apiserver-operator elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-apiserver elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-authentication-operator elasticsearch-operator.5.0.0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded openshift-authentication elasticsearch-operator.5.0. 0-202007012112.p0 OpenShift Elasticsearch Operator 5.0.0-202007012112.p0 Succeeded ... Install the Red Hat OpenShift Logging Operator by creating the following objects:\nThe Cluster Logging OperatorGroup\noc create -f - \u003c\u003cEOF apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: cluster-logging namespace: openshift-logging spec: targetNamespaces: - openshift-logging EOF Subscription Object to subscribe a Namespace to the Red Hat OpenShift Logging Operator\noc create -f - \u003c\u003cEOF apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: cluster-logging namespace: openshift-logging spec: channel: \"stable\" name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace EOF Verify the Operator installation, the PHASE should be Succeeded\noc get csv -n openshift-logging Example Output\nNAME DISPLAY VERSION REPLACES PHASE cluster-logging.5.0.5-11 Red Hat OpenShift Logging 5.0.5-11 Succeeded elasticsearch-operator.5.0.5-11 OpenShift Elasticsearch Operator 5.0.5-11 Succeeded Create an OpenShift Logging instance:\nNOTE: For the storageClassName below, you will need to adjust for the platform on which you’re running OpenShift. managed-premium as listed below is for Azure Red Hat OpenShift (ARO). You can verify your available storage classes with oc get storageClasses\noc create -f - \u003c\u003cEOF apiVersion: \"logging.openshift.io/v1\" kind: \"ClusterLogging\" metadata: name: \"instance\" namespace: \"openshift-logging\" spec: managementState: \"Managed\" logStore: type: \"elasticsearch\" retentionPolicy: application: maxAge: 1d infra: maxAge: 7d audit: maxAge: 7d elasticsearch: nodeCount: 3 storage: storageClassName: \"managed-premium\" size: 200G resources: requests: memory: \"8Gi\" proxy: resources: limits: memory: 256Mi requests: memory: 256Mi redundancyPolicy: \"SingleRedundancy\" visualization: type: \"kibana\" kibana: replicas: 1 curation: type: \"curator\" curator: schedule: \"30 3 * * *\" collection: logs: type: \"fluentd\" fluentd: {} EOF It will take a few minutes for everything to start up. You can monitor this progress by watching the pods.\nwatch oc get pods -n openshift-logging Your logging instances are now configured and recieving logs. To view them, you will need to log into your Kibana instance and create the appropriate index patterns. For more information on index patterns, see the Kibana documentation.\nNOTE: The following restrictions and notes apply to index patterns:\nAll users can view the app- logs for namespaces they have access to Only cluster-admins can view the infra- and audit- logs For best accuracy, use the @timestamp field for determining chronology ","description":"","tags":["Observability","OCP"],"title":"OpenShift Logging","uri":"/docs/o11y/openshift-logging/"},{"content":"A Quickstart guide to deploying a Private Azure Red Hat OpenShift cluster.\nOnce the cluster is running you will need a way to access the private network that ARO is deployed into.\nAuthor: Paul Czarkowski\nPrerequisites Azure CLI Obviously you’ll need to have an Azure account to configure the CLI against.\nMacOS\nSee Azure Docs for alternative install options.\nInstall Azure CLI using homebrew\nbrew update \u0026\u0026 brew install azure-cli Linux\nSee Azure Docs for alternative install options.\nImport the Microsoft Keys\nsudo rpm --import https://packages.microsoft.com/keys/microsoft.asc Add the Microsoft Yum Repository\ncat \u003c\u003c EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF Install Azure CLI\nsudo dnf install -y azure-cli Prepare Azure Account for Azure OpenShift Log into the Azure CLI by running the following and then authorizing through your Web Browser\naz login Make sure you have enough Quota (change the location if you’re not using East US)\naz vm list-usage --location \"East US\" -o table see Addendum - Adding Quota to ARO account if you have less than 36 Quota left for Total Regional vCPUs.\nRegister resource providers\naz provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait Get Red Hat pull secret Log into cloud.redhat.com\nBrowse to https://cloud.redhat.com/openshift/install/azure/aro-provisioned\nclick the Download pull secret button and remember where you saved it, you’ll reference it later.\nDeploy Azure OpenShift Variables and Resource Group Set some environment variables to use later, and create an Azure Resource Group.\nSet the following environment variables\nChange the values to suit your environment, but these defaults should work.\nAZR_RESOURCE_LOCATION=eastus AZR_RESOURCE_GROUP=openshift-private AZR_CLUSTER=private-cluster AZR_PULL_SECRET=~/Downloads/pull-secret.txt NETWORK_SUBNET=10.0.0.0/20 CONTROL_SUBNET=10.0.0.0/24 MACHINE_SUBNET=10.0.1.0/24 FIREWALL_SUBNET=10.0.2.0/24 JUMPHOST_SUBNET=10.0.3.0/24 Create an Azure resource group\naz group create \\ --name $AZR_RESOURCE_GROUP \\ --location $AZR_RESOURCE_LOCATION Networking Create a virtual network with two empty subnets\nCreate virtual network\naz network vnet create \\ --address-prefixes $NETWORK_SUBNET \\ --name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP Create control plane subnet\naz network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes $CONTROL_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Create machine subnet\naz network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --address-prefixes $MACHINE_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Disable network policies for Private Link Service on the control plane subnet\nThis is required for the service to be able to connect to and manage the cluster.\naz network vnet subnet update \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --disable-private-link-service-network-policies true Firewall + Internet Egress This replaces the routes for the cluster to go through the Firewall for egress vs the LoadBalancer which we can later remove. It does come with extra Azure costs of course.\nYou can skip this step if you don’t need to restrict egress.\nMake sure you have the AZ CLI firewall extensions\naz extension add -n azure-firewall az extension update -n azure-firewall Create a firewall network, IP, and firewall\naz network vnet subnet create \\ -g $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ -n \"AzureFirewallSubnet\" \\ --address-prefixes $FIREWALL_SUBNET az network public-ip create -g $AZR_RESOURCE_GROUP -n fw-ip \\ --sku \"Standard\" --location $AZR_RESOURCE_LOCATION az network firewall create -g $AZR_RESOURCE_GROUP \\ -n aro-private -l $AZR_RESOURCE_LOCATION Configure the firewall and configure IP Config (this may take 15 minutes)\naz network firewall ip-config create -g $AZR_RESOURCE_GROUP \\ -f aro-private -n fw-config --public-ip-address fw-ip \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" FWPUBLIC_IP=$(az network public-ip show -g $AZR_RESOURCE_GROUP -n fw-ip --query \"ipAddress\" -o tsv) FWPRIVATE_IP=$(az network firewall show -g $AZR_RESOURCE_GROUP -n aro-private --query \"ipConfigurations[0].privateIpAddress\" -o tsv) echo $FWPUBLIC_IP echo $FWPRIVATE_IP Create and configure a route table\naz network route-table create -g $AZR_RESOURCE_GROUP --name aro-udr sleep 10 az network route-table route create -g $AZR_RESOURCE_GROUP --name aro-udr \\ --route-table-name aro-udr --address-prefix 0.0.0.0/0 \\ --next-hop-type VirtualAppliance --next-hop-ip-address $FWPRIVATE_IP az network route-table route create -g $AZR_RESOURCE_GROUP --name aro-vnet \\ --route-table-name aro-udr --address-prefix 10.0.0.0/16 --name local-route \\ --next-hop-type VirtualNetworkGateway Create firewall rules for ARO resources\nNote: ARO clusters do not need access to the internet, however your own workloads running on them may. You can skip this step if you don’t need any egress at all.\nCreate a Network Rule to allow all http/https egress traffic (not recommended)\naz network firewall network-rule create -g $AZR_RESOURCE_GROUP -f aro-private \\ --collection-name 'allow-https' --name allow-all \\ --action allow --priority 100 \\ --source-addresses '*' --dest-addr '*' \\ --protocols 'Any' --destination-ports 1-65535 Create Application Rules to allow to a restricted set of destinations\nreplace the target-fqdns with your desired destinations\naz network firewall application-rule create -g $AZR_RESOURCE_GROUP -f aro-private \\ --collection-name 'Allow_Egress' \\ --action allow \\ --priority 100 \\ -n 'required' \\ --source-addresses '*' \\ --protocols 'http=80' 'https=443' \\ --target-fqdns '*.google.com' '*.bing.com' az network firewall application-rule create -g $AZR_RESOURCE_GROUP -f aro-private \\ --collection-name 'Docker' \\ --action allow \\ --priority 200 \\ -n 'docker' \\ --source-addresses '*' \\ --protocols 'http=80' 'https=443' \\ --target-fqdns '*cloudflare.docker.com' '*registry-1.docker.io' 'apt.dockerproject.org' 'auth.docker.io' Update the subnets to use the Firewall\nOnce the cluster is deployed successfully you can update the subnets to use the firewall instead of the default outbound loadbalancer rule.\naz network vnet subnet update -g $AZR_RESOURCE_GROUP \\ --vnet-name $AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION \\ --name \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --route-table aro-udr az network vnet subnet update -g $AZR_RESOURCE_GROUP \\ --vnet-name $AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION \\ --name \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --route-table aro-udr Create the cluster\nThis will take between 30 and 45 minutes.\naz aro create \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER \\ --vnet \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --master-subnet \"$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION\" \\ --worker-subnet \"$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION\" \\ --apiserver-visibility Private \\ --ingress-visibility Private \\ --pull-secret @$AZR_PULL_SECRET Jump Host With the cluster in a private network, we can create a Jump host in order to connect to it. You can do this while the cluster is being created.\nCreate jump subnet\naz network vnet subnet create \\ --resource-group $AZR_RESOURCE_GROUP \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" \\ --name JumpSubnet \\ --address-prefixes $JUMPHOST_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Create a jump host\naz vm create --name jumphost \\ --resource-group $AZR_RESOURCE_GROUP \\ --ssh-key-values $HOME/.ssh/id_rsa.pub \\ --admin-username aro \\ --image \"RedHat:RHEL:8.2:8.2.2021040911\" \\ --subnet JumpSubnet \\ --public-ip-address jumphost-ip \\ --vnet-name \"$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION\" Save the jump host public IP address\nJUMP_IP=$(az vm list-ip-addresses -g $AZR_RESOURCE_GROUP -n jumphost -o tsv \\ --query '[].virtualMachine.network.publicIpAddresses[0].ipAddress') echo $JUMP_IP ssh to jump host forwarding port 1337 as a socks proxy.\nreplace the IP with the IP of the jump box from the previous step.\nssh -D 1337 -C -i $HOME/.ssh/id_rsa aro@$JUMP_IP test the socks proxy\ncurl --socks5-hostname localhost:1337 http://www.google.com/ Install tools\nsudo yum install -y gcc libffi-devel python3-devel openssl-devel jq sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc cat \u003c\u003c EOF | sudo tee /etc/yum.repos.d/azure-cli.repo [azure-cli] name=Azure CLI baseurl=https://packages.microsoft.com/yumrepos/azure-cli enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc EOF sudo yum install -y azure-cli wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-client-linux.tar.gz mkdir openshift tar -zxvf openshift-client-linux.tar.gz -C openshift sudo install openshift/oc /usr/local/bin/oc sudo install openshift/kubectl /usr/local/bin/kubectl Wait until the ARO cluster is fully provisioned.\nLogin to Azure\naz login Get OpenShift console URL\nset these variables to match the ones you set at the start.\nAZR_RESOURCE_GROUP=openshift-private AZR_CLUSTER=private-cluster APISERVER=$(az aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query apiserverProfile.url) echo $APISERVER Get OpenShift credentials\nADMINPW=$(az aro list-credentials \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ --query kubeadminPassword \\ -o tsv) Test Access Test Access to the cluster via the socks proxy\nCONSOLE=$(az aro show \\ --name $AZR_CLUSTER \\ --resource-group $AZR_RESOURCE_GROUP \\ -o tsv --query consoleProfile) echo $CONSOLE curl --socks5-hostname localhost:1337 $CONSOLE Unfortunately you can’t [easily] use the socks proxy with the oc command, but at least you can access the console via the socks proxy.\nSet localhost:1337 as a socks proxy in your browser and verify you can access the cluster by browsing to the $CONSOLE url. Delete Cluster Once you’re done its a good idea to delete the cluster to ensure that you don’t get a surprise bill.\nDelete the cluster\naz aro delete -y \\ --resource-group $AZR_RESOURCE_GROUP \\ --name $AZR_CLUSTER Delete the Azure resource group\nOnly do this if there’s nothing else in the resource group.\naz group delete -y \\ --name $AZR_RESOURCE_GROUP Addendum Adding Quota to ARO account Create an Azure Support Request\nSet Issue Type to “Service and subscription limits (quotas)”\nSet Quota Type to “Compute-VM (cores-vCPUs) subscription limit increases”\nClick Next Solutions »\nClick Enter details\nSet Deployment Model to “Resource Manager\nSet Locations to “(US) East US”\nSet Types to “Standard”\nUnder Standard check “DSv3” and “DSv4”\nSet New vCPU Limit for each (example “60”)\nClick Save and continue\nClick Review + create »\nWait until quota is increased.\n","description":"","tags":["ARO","Azure"],"title":"Private ARO Cluster with access via JumpHost","uri":"/docs/aro/private-cluster/"},{"content":"","description":"","tags":null,"title":"Private Link","uri":"/tags/private-link/"},{"content":"","description":"","tags":null,"title":"Quay on ARO","uri":"/docs/aro/setup-quay/"},{"content":"","description":"","tags":null,"title":"Quickstarts","uri":"/tags/quickstarts/"},{"content":"Registering an ARO cluster to OpenShift Cluster Manager ARO clusters do not come connected to OpenShift Cluster Manager by default, because Azure would like customers to specifically opt-in to connections / data sent outside of Azure. This is the case with registering to OpenShift cluster manager, which enables a telemetry service in ARO.\nPrerequisites An Red Hat account. If you have any subscriptions with Red Hat, you will have a Red Hat account. If not, then you can create an account easily at https://cloud.redhat.com. Steps Login to https://console.redhat.com with you Red Hat account.\nGo to https://console.redhat.com/openshift/downloads and download your pull-secret file. This is a file that includes an authentication for cloud.openshift.com which is used by OpenShift Cluster Manager.\nFollow the Update pull secret instructions to merge your pull-secret (in particular cloud.openshift.com) in your ARO pull secret. Be careful not to overwrite the ARO cluster pull secrets that come by default - it explains how in that article.\nAfter waiting a few minutes (but it could be up to an hour), your cluster should be automatically registered in this list in OpenShift Cluster Manager; https://console.redhat.com/openshift\nYou can check the cluster ID within the Cluster Overview section of the admin console with the ID of the cluster in OCM to make sure the right cluster is registered.\nThe cluster will appear as a 60-day self-supported evaluation cluster. However, again, wait about an hour (but in this case, it can take up to 24 hours), and the cluster will be automatically updated to an ARO type cluster, with full support. You don’t need to change the support level yourself.\nThis makes the cluster a fully supported cluster within the Red Hat cloud console, with access to raise support tickets, also.\n","description":"","tags":["ARO","Azure"],"title":"Registering an ARO cluster to OpenShift Cluster Manager","uri":"/docs/aro/ocm/"},{"content":"Kristopher White x Connor Wooley\n07/25/2022\nPre Requisites An ARO cluster oc cli azure cli Steps Create Azure Resources Create Storage Account\naz login az group create --name \u003cresource-group\u003e --location \u003clocation\u003e az storage account create --name \u003cstorage-account\u003e --resource-group \u003cresource-group\u003e \\ --location eastus --sku Standard_LRS --kind StorageV2 Create Storage Container\naz storage account keys list --account-name \u003cstorage_account_name\u003e --resource-group \u003cresource_group\u003e --output yaml Note: this command returns a json by default with your keyName and Values, command above specifies yaml\naz storage container create --name \u003ccontainer_name\u003e --public-access blob \\ --account-name \u003cAZURE_STORAGE_ACCOUNT\u003e --account-key \u003cAZURE_STORAGE_ACCOUNT_KEY\u003e Note: Will need the storage container creds for later use\nInstall Quay-Operator and Create Quay Registry Login to your cluster’s OCM\nCreate a sub.yaml file with this template to install the quay operator\napiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: quay-operator namespace: \u003cnamespace\u003e spec: channel: \u003crelease_channel\u003e name: quay-operator source: redhat-operators sourceNamespace: openshift-marketplace startingCSV: quay-operator.\u003cversion\u003e oc apply -f sub.yaml Create the Quay Registry\nCreate the Azure Storage Secret Bundle\nCreate a config.yaml file that injects the azure resource info from the storage container created in step 2 of Create Azure Resources DISTRIBUTED_STORAGE_CONFIG: local_us: - AzureStorage - azure_account_key: \u003cAZURE_STORAGE_ACCOUNT_KEY\u003e azure_account_name: \u003cAZURE_STORAGE_ACCOUNT\u003e azure_container: \u003cAZURE_CONTAINER_NAME\u003e storage_path: /datastorage/registry DISTRIBUTED_STORAGE_DEFAULT_LOCATIONS: - local_us DISTRIBUTED_STORAGE_PREFERENCE: - local_us oc create secret generic --from-file config.yaml=./config.yaml -n \u003cnamespace\u003e \u003cconfig_bundle_secret_name\u003e Create the Quay Registry with the Secret\nCreate a quayregistry.yaml file with this format apiVersion: quay.redhat.com/v1 kind: QuayRegistry metadata: name: \u003cregistry_name\u003e namespace: \u003cnamespace\u003e finalizers: - quay-operator/finalizer generation: 3 spec: configBundleSecret: \u003cconfig_bundle_secret_name\u003e components: - kind: clair managed: true - kind: postgres managed: true - kind: objectstorage managed: false - kind: redis managed: true - kind: horizontalpodautoscaler managed: true - kind: route managed: true - kind: mirror managed: true - kind: monitoring managed: true - kind: tls managed: true - kind: quay managed: true - kind: clairpostgres managed: true``` oc create -n \u003cnamespace\u003e -f quayregistry.yaml Login to your Quay Registry and begin pushing images to it!\nNote: This configuration does not support in-cluster authentication integration with the quay deployment. User Management with the registry is handled by the registry.\n","description":"","tags":["ARO","Azure"],"title":"Setting up Quay on an ARO cluster via CLI","uri":"/docs/aro/setup-quay/quay-cli/"},{"content":"\nRed Hat Quay setup on ARO (Azure Openshift) A guide to deploying an Azure Red Hat OpenShift Cluster with Red Hat Quay.\nAuthor: [Kristopher White x Connor Wooley]\nVideo Walkthrough If you prefer a more visual medium, you can watch [Kristopher White] walk through Quay Registry Storage Setup on YouTube.\nRed Hat Quay Setup Backend Storage Setup Login to Azure\nSearch/Click Create Resource Groups\nName Resource Group \u003e Click Review + Create \u003e Click Create\nSearch/Click Create Storage Accounts\nChoose Resource Group \u003e Name Storage Account \u003e Choose Region \u003e Choose Performance \u003e Choose Redundancy \u003e Click Review + Create \u003e Click Create Click Go To Resource\nGo to Data Storage \u003e Click Container \u003e Click New Container \u003e Name Container \u003e Set Privacy to Public Access Blob \u003e Click Create\nGo to Storage Account \u003e Click Access Keys \u003e Go to key 1 \u003e Click Show Key\nStorage Account Name, Container Name, and Access Keys will be used to configure quay registry storage.\nRed Hat Quay Operator Install Log into the OpenShift web console with your OpenShift cluster admin credentials.\nMake sure you have selected the Administrator view.\nClick Operators \u003e OperatorHub \u003e Red Hat Quay.\nSearch for and click the tile for the Red Hat Quay operator.\nClick Install.\nIn the Install Operator pane:\nSelect the latest update channel.\nSelect the option to install Red Hat Quay in one namespace or for all namespaces on your cluster. If in doubt, choose the All namespaces on the cluster installation mode, and accept the default Installed Namespace.\nSelect the Automatic approval strategy.\nClick Install.\nSuccessful Install Redhat Quay Registry Deployment Make sure you have selected the Administrator view.\nClick Operators \u003e Installed Operators \u003e Red Hat Quay \u003e Quay Registry \u003e Create QuayRegistry.\nForm View YAML View Click Create \u003e Click Registry\nSuccessful Registry Deployment Click Config Editor Credentials Secret\nGo to Data \u003e Reveal Values (These values are used to login to Config Editor Endpoint)\nGo to Registry Console \u003e Click Config Editor Endpoint \u003e\nScroll down to Registry Storage \u003e Click Edit Fields \u003e Go to Storage Engine click the drop down and select Azure Blob Storage \u003e Fill in Azure Storage Container with Storage Container Name \u003e Fill in Azure Account Name with Azure Storage Account Name \u003e Fill in Azure Account Key with Azure Storage Account Access Key\nClick Validate Configuration Changes\nClick Reconfigure Quay Go to Registry Console \u003e Click Registry Endpoint\nClick Create Account\nLogin to Quay.\nClick Create Repository\n","description":"","tags":["ARO","Azure"],"title":"Setting up Quay on an ARO cluster via Console","uri":"/docs/aro/setup-quay/quay-console/"},{"content":"Kevin Collins\n06/20/2022\nWhen you configure an Azure Red Hat OpenShift (ARO) cluster with a private only configuration, you will need connectivity to this private network in order to access your cluster. This guide will show you how to configute a point-to0-site VPN connection so you won’t need to setup and configure Jump Boxes.\nPrerequisites a private ARO Cluster git openssl Create certificates to use for your VPN Connection There are many ways and methods to create certificates for VPN, the guide below is one of the ways that works well. Note, that whatever method you use, make sure it supports “X509v3 Extended Key Usage”.\nClone OpenVPN/easy-rsa\ngit clone https://github.com/OpenVPN/easy-rsa.git Change to the easyrsa directory\ncd easy-rsa/easyrsa3 Initialize the PKI\n./easyrsa init-pki Edit certificate parameters\nUncomment and edit the copied template with your values\nvim pki/vars set_var EASYRSA_REQ_COUNTRY \"US\" set_var EASYRSA_REQ_PROVINCE \"California\" set_var EASYRSA_REQ_CITY \"San Francisco\" set_var EASYRSA_REQ_ORG \"Copyleft Certificate Co\" set_var EASYRSA_REQ_EMAIL \"me@example.net\" set_var EASYRSA_REQ_OU \"My Organizational Unit\" Uncomment (remove the #) the folowing field\n#set_var EASYRSA_KEY_SIZE 2048 Create the CA:\n./easyrsa build-ca nopass Generate the Server Certificate and Key\n./easyrsa build-server-full server nopass Generate Diffie-Hellman (DH) parameters\n./easyrsa gen-dh Generate client credentials\n./easyrsa build-client-full azure nopass Set environment variables for the CA certificate you just created.\nCACERT=$(openssl x509 -in pki/ca.crt -outform der | base64) Set Envrionment Variables AROCLUSTER=\u003ccluster name\u003e ARORG=\u003cresource group the cluster is in\u003e UNIQUEID=$RANDOM LOCATION=$(az aro show --name $AROCLUSTER --resource-group $ARORG --query location -o tsv) VNET_NAME=$(az network vnet list -g $ARORG --query '[0].name' -o tsv) GW_NAME=${USER}_${VNET_NAME} GW_SUBNET_PREFIX=e.g. 10.0.7.0/24 choose a new available subnet in the VNET your cluster is in. VPN_PREFIX=172.18.0.0/24 Create an Azure Virtual Network Gateway Request a public IP Address\naz network public-ip create \\ -n $USER-pip-$UNIQUEID \\ -g $ARORG \\ --allocation-method Static \\ --sku Standard \\ --zone 1 2 3 pip=$(az network public-ip show -g $ARORG --name $USER-pip-$UNIQUEID --query \"ipAddress\" -o tsv) Create a Gateway Subnet\naz network vnet subnet create \\ --vnet-name $VNET_NAME \\ -n GatewaySubnet \\ -g $ARORG \\ --address-prefix $GW_SUBNET_PREFIX Create a virtual network gateway\naz network vnet-gateway create \\ --name $GW_NAME \\ --location $LOCATION \\ --public-ip-address $USER-pip-$UNIQUEID \\ --resource-group $ARORG \\ --vnet $VNET_NAME \\ --gateway-type Vpn \\ --sku VpnGw3AZ \\ --address-prefixes $VPN_PREFIX \\ --root-cert-data $CACERT \\ --root-cert-name $USER-p2s \\ --vpn-type RouteBased \\ --vpn-gateway-generation Generation2 \\ --client-protocol IkeV2 OpenVPN go grab a coffee, this takes about 15 - 20 minutes\nConfigure your OpenVPN Client Retrieve the VPN Settings\nFrom the Azure Portal - navigate to your Virtual Network Gateway, point to site configuration, and then click Download VPN Client. This will download a zip file containing the VPN Client\nCreate a VPN Client Configuration\nUncompress the file you downloaded in the previous step and edit the OpenVPN\\vpnconfig.ovpn file.\nNote: The next two commands assume you are still in the easyrsa3 directory.\nIn the vpnconfig.ovpn replace the $CLIENTCERTIFICATE line with the entire contents of:\nopenssl x509 -in pki/issued/azure.crt Make sure to copy the —–BEGIN CERTIFICATE—– and the —–END CERTIFICATE—– lines.\nalso replace $PRIVATEKEY line with the output of:\ncat pki/private/azure.key Make sure to copy the —–BEGIN PRIVATE KEY—– and the —–END PRIVATE KEY—– lines.\nadd the new OpenVPN configuration file to your OpenVPN client.\nmac users - just double click on the vpnserver.ovpn file and it will be automatically imported.\nConnect your VPN.\n","description":"","tags":["ARO","Azure"],"title":"Setup a VPN Connection into an ARO Cluster with OpenVPN","uri":"/docs/aro/vpn/"},{"content":"This document follows the steps outlined by Microsoft in their documentation\nFollow docs.\nStep 4, needs additional command of:\naz resource list --resource-type Microsoft.RedHatOpenShift/OpenShiftClusters -o json to capture resource ID of ARO cluster as well, needed for export in step 6\nbash enable-monitoring.sh --resource-id $azureAroV4ClusterResourceId --workspace-id $logAnalyticsWorkspaceResourceId works successfully\ncan verify pods starting\nVerify logs flowing with container solutions showing in log analytics workbook?\nConfigure Prometheus metric scraping following steps outlined here: https://docs.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-prometheus-integration\nIt looks like config maps are not set in the previous step despite what the article says. This may actually be an OpenShift v3 thing and not a v4 thing. I had to do the apply process after downloading the config.\nAfterward pods did not restart on their own and had to be manually deleted. Automatic recreation pulls in new config and should begins shipping metrics\nVerify metrics with a query: (from https://docs.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-log-search#query-prometheus-metrics-data)\nInsightsMetrics | where TimeGenerated \u003e ago(1h) | where Name == 'reads' | extend Tags = todynamic(Tags) | extend HostName = tostring(Tags.hostName), Device = Tags.name | extend NodeDisk = strcat(Device, \"/\", HostName) | order by NodeDisk asc, TimeGenerated asc | serialize | extend PrevVal = iif(prev(NodeDisk) != NodeDisk, 0.0, prev(Val)), PrevTimeGenerated = iif(prev(NodeDisk) != NodeDisk, datetime(null), prev(TimeGenerated)) | where isnotnull(PrevTimeGenerated) and PrevTimeGenerated != TimeGenerated | extend Rate = iif(PrevVal \u003e Val, Val / (datetime_diff('Second', TimeGenerated, PrevTimeGenerated) * 1), iif(PrevVal == Val, 0.0, (Val - PrevVal) / (datetime_diff('Second', TimeGenerated, PrevTimeGenerated) * 1))) | where isnotnull(Rate) | project TimeGenerated, NodeDisk, Rate | render timechart ","description":"","tags":["Observability","Azure"],"title":"Shipping logs to Azure Log Analytics","uri":"/docs/o11y/az-log-analytics/"},{"content":"Byron Miller\n05/23/2022\nNote: This guide a simple “happy path” to show the path of least friction to showcasing how to use NetApp files with Azure Red Hat OpenShift. This may not be the best behavior for any system beyond demonstration purposes.\nPrerequisites An Azure Red Hat OpenShift cluster installed with Service Principal role/credentials. kubectl cli oc cli helm 3 cli Review official trident documentation In this guide, you will need service principal and region details. Please have these handy.\nAzure subscriptionID Azure tenantID Azure clientID (Service Principal) Azure clientSecret (Service Principal Secret) Azure Region If you don’t have your existing ARO service principal credentials, you can create your own service principal and grant it contributor to be able to manage the required resources. Please review the official Trident documentation regarding Azure NetApp files and required permissions.\nImportant Concepts Persistent Volume Claims are namespaced objects. Mounting RWX/ROX is only possible within the same namespace.\nNetApp files must be have a delegated subnet within your ARO Vnet’s and you must assign it to the Microsoft.Netapp/volumes service.\nConfigure Azure You must first register the Microsoft.NetApp provider and Create a NetApp account on Azure before you can use Azure NetApp Files.\nRegister NetApp files Azure Console\nor az cli\naz provider register --namespace Microsoft.NetApp --wait Create storage account Again, for brevity I am using the same RESOURCE_GROUP and Service Principal that the cluster was created with.\nAzure Console\nor az cli\nRESOURCE_GROUP=\"myresourcegroup\" LOCATION=\"southcentralus\" ANF_ACCOUNT_NAME=\"netappfiles\" az netappfiles account create \\ --resource-group $RESOURCE_GROUP \\ --location $LOCATION \\ --account-name $ANF_ACCOUNT_NAME Create capacity pool Creating one pool for now. The common pattern is to expose all three levels with unique pool names respective of each service level.\nAzure Console\nor az cli:\nPOOL_NAME=\"Standard\" POOL_SIZE_TiB=4 # Size in Azure CLI needs to be in TiB unit (minimum 4 TiB) SERVICE_LEVEL=\"Standard\" # Valid values are Standard, Premium and Ultra az netappfiles pool create \\ --resource-group $RESOURCE_GROUP \\ --location $LOCATION \\ --account-name $ANF_ACCOUNT_NAME \\ --pool-name $POOL_NAME \\ --size $POOL_SIZE_TiB \\ --service-level $SERVICE_LEVEL Delegate subnet to ARO Login to azure console, find the subnets for your ARO cluster and click add subnet. We need to call this subnet anf.subnet since that is the name we refer to in later configuration.\nInstall Trident Operator Login/Authenticate to ARO Login to your ARO cluster. You can create a token to login via cli straight from the web gui\noc login --token=sha256~abcdefghijklmnopqrstuvwxyz --server=https://api.randomseq.eastus.aroapp.io:6443 Helm Install Download latest Trident package\nwget https://github.com/NetApp/trident/releases/download/v22.04.0/trident-installer-22.04.0.tar.gz Extract tar.gz into working director\ntar -xzvf trident-installer-22.04.0.tar.gz cd into installer\ncd trident-installer/helm Helm install\nhelm install trident-operator trident-operator-22.04.0.tgz Example output from installation:\nW0523 17:45:22.189592 30478 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0523 17:45:22.484071 30478 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ NAME: trident-operator LAST DEPLOYED: Mon May 23 17:45:20 2022 NAMESPACE: openshift STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Thank you for installing trident-operator, which will deploy and manage NetApp's Trident CSI storage provisioner for Kubernetes. Your release is named 'trident-operator' and is installed into the 'openshift' namespace. Please note that there must be only one instance of Trident (and trident-operator) in a Kubernetes cluster. To configure Trident to manage storage resources, you will need a copy of tridentctl, which is available in pre-packaged Trident releases. You may find all Trident releases and source code online at https://github.com/NetApp/trident. To learn more about the release, try: $ helm status trident-operator $ helm get all trident-operator Validate\ncd .. ./tridentctl -n openshift version +----------------+----------------+ | SERVER VERSION | CLIENT VERSION | +----------------+----------------+ | 22.04.0 | 22.04.0 | +----------------+----------------+ Install tridentctl I put all my cli’s in /usr/local/bin\nsudo install tridentctl /usr/local/bin example output:\nwhich tridentctl /usr/local/bin/tridentctl Create trident backend FYI - Sample files for review are in sample-input/backends-samples/azure-netapp-files directory from the trident tgz we extracted earlier.\nReplace client ID with service principal ID Replace clientSecret with Service Principal Secret Replace tenantID with your account tenant ID Replace subscriptionID with your azure SubscriptionID Ensure location matches your Azure Region Note: I have used nfsv3 for basic compatibility. You can remove that line and use NetApp files defaults.\nvi backend.json Add the following snippet:\n{ \"version\": 1, \"nfsMountOptions\": \"nfsvers=3\", \"storageDriverName\": \"azure-netapp-files\", \"subscriptionID\": \"12abc678-4774-fake-a1b2-a7abcde39312\", \"tenantID\": \"a7abcde3-edc1-fake-b111-a7abcde356cf\", \"clientID\": \"abcde356-bf8e-fake-c111-abcde35613aa\", \"clientSecret\": \"rR0rUmWXfNioN1KhtHisiSAnoTherboGuskey6pU\", \"location\": \"southcentralus\", \"subnet\": \"anf.subnet\", \"labels\": { \"cloud\": \"azure\" }, \"storage\": [ { \"labels\": { \"performance\": \"Standard\" }, \"serviceLevel\": \"Standard\" } ] } run\ntridentctl -n openshift create backend -f backend.json example output:\n+------------------------+--------------------+--------------------------------------+--------+---------+ | NAME | STORAGE DRIVER | UUID | STATE | VOLUMES | +------------------------+--------------------+--------------------------------------+--------+---------+ | azurenetappfiles_eb177 | azure-netapp-files | f7f211afe-d7f5-41a5-a356-fa67f25ee96b | online | 0 | +------------------------+--------------------+--------------------------------------+--------+---------+ if you get a failure here, you can run to following command to review logs:\ntridentctl logs To view log output that may help steer you in the right direction.\nCreate storage class cat \u003c\u003cEOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard provisioner: csi.trident.netapp.io parameters: backendType: \"azure-netapp-files\" fsType: \"nfs\" selector: \"performance=Standard\" # Matching labels in the backends... allowVolumeExpansion: true # To allow volume resizing. This parameter is optional mountOptions: - nconnect=16 EOF output:\nstorageclass.storage.k8s.io/standard created Provision volume Let’s create a new project and set up a persistent volume claim. Remember that PV Claims are namespaced objects and you must create the pvc in the namespace where it will be allocated. I’ll use the project “netappdemo”.\noc new-project netappdemo Now we’ll create a PV claim in the “netappdemo” project we just created.\ncat \u003c\u003cEOF | kubectl apply -f - kind: PersistentVolumeClaim apiVersion: v1 metadata: name: standard spec: accessModes: - ReadWriteMany resources: requests: storage: 4000Gi storageClassName: standard EOF output:\npersistentvolumeclaim/standard created Verify Quick verification of storage, volumes and services.\nVerify Kubectl ➜ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEmanaged-premium (default) kubernetes.io/azure-disk Delete WaitForFirstConsumer true 3h26m standard csi.trident.netapp.io Delete Immediate true 5m5s ➜ Verify OpenShift Login to your cluster as cluster-admin and verify your storage classes and persistent volumes.\nStorage Class\nPersisent Volumes Create Pods to test Azure NetApp We’ll create two pods here to exercise the Azure NetApp file mount. One to write data and another to read data to show that it is mounted as “read write many” and correctly working.\nWriter Pod This pod will write “hello netapp” to a shared NetApp mount.\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-netapp labels: app: test-aznetapp deploymethod: trident spec: containers: - name: aznetapp image: centos:latest command: [\"/bin/bash\", \"-c\", \"--\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [ \"while true; do echo 'hello netapp' | tee -a /mnt/netapp-data/verify-netapp \u0026\u0026 sleep 5; done;\", ] volumeMounts: - name: disk01 mountPath: \"/mnt/netapp-data\" volumes: - name: disk01 persistentVolumeClaim: claimName: standard EOF You can watch for this container to be ready:\nwatch oc get pod test-netapp Or view it in the OpenShift Pod console for the netappdemo project.\nReader Pod This pod will read back the data from the shared NetApp mount.\ncat \u003c\u003cEOF | oc apply -f - apiVersion: v1 kind: Pod metadata: name: test-netapp-read spec: containers: - name: test-netapp-read image: centos:latest command: [\"/bin/bash\", \"-c\", \"--\"] resources: limits: cpu: 1 memory: \"1Gi\" args: [\"tail -f /mnt/netapp-data/verify-netapp\"] volumeMounts: - name: disk01 mountPath: \"/mnt/netapp-data\" volumes: - name: disk01 persistentVolumeClaim: claimName: standard EOF Now let’s verify the POD is reading from shared volume.\noc logs test-netapp-read hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp hello netapp You can also see the pod details in OpenShift for the reader:\n","description":"","tags":["ARO","Azure"],"title":"Trident NetApp operator setup for Azure NetApp files","uri":"/docs/aro/trident/"},{"content":"User Workload Monitoring on Azure Red Hat OpenShift In Azure Red Hat OpenShift (ARO) Monitoring for User Defined Projects is disabled by default. Follow these instructions to enable it.\nEnabling See docs for more indepth details.\nCheck the cluster-monitoring-config ConfigMap object\noc -n openshift-monitoring get configmap cluster-monitoring-config -o yaml Enable User Workload Monitoring by doing one of the following\nIf the data.config.yaml is not {} you should edit it and add the enableUserWorkload: true line manually.\noc -n openshift-monitoring edit configmap cluster-monitoring-config Otherwise if its {} then you can run the following command safely.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: | enableUserWorkload: true EOF Create a config for User Workload Monitoring to set retention and\nThis will configure the user workload instance to have PVC storage and will set basic data retention values. Feel free to edit it to suit your needs. Remember if you’re going to have PVCs enabled they are tied to an AZ, to for a multi-AZ cluster you should ensure you have at least 2 workers per AZ so that they can failover.\ncat \u003c\u003c EOF | oc apply -f - apiVersion: v1 kind: ConfigMap metadata: name: user-workload-monitoring-config namespace: openshift-user-workload-monitoring data: config.yaml: | prometheus: volumeClaimTemplate: spec: storageClassName: managed-premium volumeMode: Filesystem resources: requests: storage: 40Gi retention: 24h resources: requests: cpu: 200m memory: 2Gi EOF Deploy an example application with a service monitor resource\noc apply -f example-app.yaml Wait a few minutes and then check your cluster metrics.\nSwitch to Developer mode Change the Project to ns1 Click the Monitoring button Grafana Create a Project for the Grafana Operator + Application\noc new-project custom-grafana Install the Grafana Operator (or via the OperatorHub in the GUI)\ncat \u003c\u003c EOF | oc apply -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: grafana-operator namespace: custom-grafana labels: operators.coreos.com/grafana-operator.custom-grafana: '' spec: channel: alpha installPlanApproval: Automatic name: grafana-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: grafana-operator.v3.10.1 EOF Once the Grafana Operator is running create a Grafana Instance\ncat \u003c\u003c EOF | oc apply -f - apiVersion: integreatly.org/v1alpha1 kind: Grafana metadata: name: custom-grafana namespace: custom-grafana spec: adminPassword: bad-password adminUser: admin basicAuth: true config: auth: disable_signout_menu: false auth.anonymous: enabled: false log: level: warn mode: console security: admin_password: secret admin_user: root dashboardLabelSelector: - matchExpressions: - key: app operator: In values: - grafana ingress: enabled: true EOF Once the instance has been created you should be able to log in by getting the route and using the admin user/pass from above.\noc -n custom-grafana get routes The output should look like\nNAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD grafana-route grafana-route-custom-grafana.apps.w4l8w924.eastus.aroapp.io grafana-service 3000 edge None Copy and paste the host into your browser and log in to verify its working.\nGrant the grafana instance access to cluster-metrics\noc adm policy add-cluster-role-to-user \\ cluster-monitoring-view -z grafana-serviceaccount Save the service accounts bearer token as a variable\nBEARER_TOKEN=`oc serviceaccounts get-token grafana-serviceaccount -n custom-grafana` Create a datasource to access the Thanos Querier\ncat \u003c\u003c EOF | oc apply -f - apiVersion: integreatly.org/v1alpha1 kind: GrafanaDataSource metadata: name: prometheus-grafanadatasource namespace: custom-grafana spec: datasources: - access: proxy editable: true isDefault: true jsonData: httpHeaderName1: 'Authorization' timeInterval: 5s tlsSkipVerify: true name: Prometheus secureJsonData: httpHeaderValue1: 'Bearer ${BEARER_TOKEN}' type: prometheus url: 'https://thanos-querier.openshift-monitoring.svc.cluster.local:9091' name: prometheus-grafanadatasource.yaml EOF Add system dashboards to Grafana\nThe dashboards.yaml file was created by running the script generate-dashboards.sh which fetches the dashboard json files from the openshift-monitoring namespace.\noc apply -f dashboards.yaml ","description":"","tags":null,"title":"User Workload Monitoring on Azure Red Hat OpenShift","uri":"/docs/rosa/federated-metrics/user-defined/"},{"content":"Updated: 06/02/2022 by Paul Czarkowski\nAWS Controllers for Kubernetes (ACK) lets you define and use AWS service resources directly from Kubernetes. With ACK, you can take advantage of AWS-managed services for your Kubernetes applications without needing to define resources outside of the cluster or run services that provide supporting capabilities like databases or message queues within the cluster.\nROSA clusters have a set of the ACK controllers in Operator Hub which makes it relatively easy to get started and use it. Caution should be taken as it is a tech preview product from AWS.\nThis tutorial shows how to use the ACK S3 controller as an example, but can be adapted for any other ACK controller that has an operator in the OperatorHub of your cluster.\nPrerequisites A ROSA cluster AWS CLI OpenShift CLI oc Pre-install instructions Set some useful environment variables\nexport CLUSTER=ansible-rosa export NAMESPACE=ack-system export IAM_USER=${CLUSTER}-ack-controller export S3_POLICY_ARN=arn:aws:iam::aws:policy/AmazonS3FullAccess export SCRATCH_DIR=/tmp/ack export ACK_SERVICE=s3 export AWS_PAGER=\"\" mkdir -p $SCRATCH_DIR Create and bind an IAM service account for ACK to use\naws iam create-user --user-name $IAM_USER Create an access key for the user\nread -r ACCESS_KEY_ID ACCESS_KEY \u003c \u003c(aws iam create-access-key \\ --user-name $IAM_USER \\ --query 'AccessKey.[AccessKeyId,SecretAccessKey]' --output text) Find the ARN of the recommended IAM policy\nNote: you can find the recommended policy in each projects github repo, example https://github.com/aws-controllers-k8s/s3-controller/blob/main/config/iam/recommended-policy-arn\naws iam attach-user-policy \\ --user-name $IAM_USER \\ --policy-arn \"$S3_POLICY_ARN\" Install the ACK S3 Controller Log into your OpenShift console, click to OperatorHub and search for “ack” Select the S3 controller and install it.\nCreate a config map for ACK to use\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/config.txt ACK_ENABLE_DEVELOPMENT_LOGGING=true ACK_LOG_LEVEL=debug ACK_WATCH_NAMESPACE= AWS_REGION=us-west-2 AWS_ENDPOINT_URL= ACK_RESOURCE_TAGS=$CLUSTER_NAME EOF Apply the config map\noc create configmap --namespace ack-system \\ --from-env-file=$SCRATCH_DIR/config.txt ack-s3-user-config Create a secret for ACK to use\ncat \u003c\u003cEOF \u003e $SCRATCH_DIR/secrets.txt AWS_ACCESS_KEY_ID=$ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY=$ACCESS_KEY EOF Apply the secret\noc create secret generic --namespace ack-system \\ --from-env-file=$SCRATCH_DIR/secrets.txt ack-s3-user-secrets Check the ack-s3-controller is running\nkubectl -n ack-system get pods NAME READY STATUS RESTARTS AGE ack-s3-controller-6dc4b4c-zgs2m 1/1 Running 0 145m If its not, restart it so that it can read the new configmap/secret.\nkubectl rollout restart deployment ack-s3-controller Deploy an S3 Bucket Resource\ncat \u003c\u003c EOF | oc apply -f - apiVersion: s3.services.k8s.aws/v1alpha1 kind: Bucket metadata: name: $CLUSTER-bucket spec: name: $CLUSTER-bucket EOF Verify the S3 Bucket Resource\naws s3 ls | grep $CLUSTER-bucket 2022-06-02 12:20:25 ansible-rosa-bucket ","description":"","tags":["AWS","ROSA"],"title":"Using AWS Controllers for Kubernetes (ACK) on ROSA","uri":"/docs/rosa/ack/"},{"content":"Paul Czarkowski, Steve Mirman\n08/19/2021\nIn Azure Red Hat OpenShift (ARO) you can fairly easily set up cluster logging to an in-cluster Elasticsearch using the OpenShift Elasticsearch Operator and the Cluster Logging Operator, but what if you want to use the Azure native Log Analytics service?\nThere’s a number of ways to do this, for example installing agents onto the VMs (in this case, it would be a DaemonSet with hostvar mounts) but that isn’t ideal in a managed system like ARO.\nFluentd is the log collection and forwarding tool used by OpenShift, however it does not have native support for Azure Log Analytics. However Fluent-bit which supports many of the same protocols as Fluentd does have native support for Azure Log Analytics.\nArmed with this knowledge we can create a fluent-bit service on the cluster to accept logs from fluentd and forward them to Azure Log Analytics.\nPrepare your ARO cluster Deploy an ARO cluster\nSet some environment variables\nexport NAMESPACE=aro-clf-am export AZR_RESOURCE_LOCATION=eastus export AZR_RESOURCE_GROUP=openshift # this value must be unique export AZR_LOG_APP_NAME=$AZR_RESOURCE_GROUP-$AZR_RESOURCE_LOCATION Set up ARO Monitor workspace Add the Azure CLI log extensions\naz extension add --name log-analytics Create resource group\nIf you plan to reuse the same group as your cluster skip this step\naz group create -n $AZR_RESOURCE_GROUP -l $AZR_RESOURCE_LOCATION Create workspace\naz monitor log-analytics workspace create \\ -g $AZR_RESOURCE_GROUP -n $AZR_LOG_APP_NAME \\ -l $AZR_RESOURCE_LOCATION Create a secret for your Azure workspace\nWORKSPACE_ID=$(az monitor log-analytics workspace show \\ -g $AZR_RESOURCE_GROUP -n $AZR_LOG_APP_NAME \\ --query customerId -o tsv) SHARED_KEY=$(az monitor log-analytics workspace get-shared-keys \\ -g $AZR_RESOURCE_GROUP -n $AZR_LOG_APP_NAME \\ --query primarySharedKey -o tsv) Configure OpenShift Create a Project to run the log forwarding in\noc new-project $NAMESPACE Create namespaces for logging operators\nkubectl create ns openshift-logging kubectl create ns openshift-operators-redhat Add the MOBB chart repository to Helm\nhelm repo add mobb https://rh-mobb.github.io/helm-charts/ Update your Helm repositories\nhelm repo update Deploy the OpenShift Elasticsearch Operator and the Red Hat OpenShift Logging Operator\n\u003e Note: You can skip this if you already have them installed, or install them via the OpenShift Console.\nhelm upgrade -n $NAMESPACE clf-operators \\ mobb/operatorhub --version 0.1.1 --install \\ --values https://raw.githubusercontent.com/rh-mobb/helm-charts/main/charts/aro-clf-am/files/operators.yaml Configure cluster logging forwarder\nhelm upgrade -n $NAMESPACE clf \\ mobb/aro-clf-am --install \\ --set \"azure.workspaceId=$WORKSPACE_ID\" --set \"azure.sharedKey=$SHARED_KEY\" Check for logs in Azure Wait 5 to 15 minutes\nQuery our new Workspace\naz monitor log-analytics query -w $WORKSPACE_ID \\ --analytics-query \"openshift_CL | take 10\" --output tsv or\nLog into Azure Log Insights\nSelect your workspace\nRun the Query\nopenshift_CL | take 10 ","description":"","tags":["ARO","Azure"],"title":"Using Cluster Logging Forwarder in ARO with Azure Monitor","uri":"/docs/aro/clf-to-azure/"},{"content":"Prerequisites A private ARO cluster Deploy the Egressip Ipam Operator Via GUI Log into the ARO cluster’s Console\nSwitch to the Administrator view\nClick on Operators -\u003e Operator Hub\nSearch for “Egressip Ipam Operator”\nInstall it with the default settings\nor\nVia CLI Deploy the egress-ipam-operator\ncat \u003c\u003c EOF | kubectl apply -f - --- apiVersion: v1 kind: Namespace metadata: name: egressip-ipam-operator --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: egressip-ipam-operator namespace: openshift-operators labels: operators.coreos.com/egressip-ipam-operator.egressip-ipam-operator: '' spec: channel: alpha installPlanApproval: Automatic name: egressip-ipam-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: egressip-ipam-operator.v1.2.2 EOF Configure EgressIP Create an EgressIPAM resource for your cluster. Update the CIDR to reflect the worker node subnet.\ncat \u003c\u003c EOF | kubectl apply -f - apiVersion: redhatcop.redhat.io/v1alpha1 kind: EgressIPAM metadata: name: egressipam-azure annotations: egressip-ipam-operator.redhat-cop.io/azure-egress-load-balancer: none spec: cidrAssignments: - labelValue: \"\" CIDR: 10.0.1.0/24 reservedIPs: [] topologyLabel: \"node-role.kubernetes.io/worker\" nodeSelector: matchLabels: node-role.kubernetes.io/worker: \"\" EOF Create test namespaces\ncat \u003c\u003c EOF | kubectl apply -f - --- apiVersion: v1 kind: Namespace metadata: name: egressipam-azure-test annotations: egressip-ipam-operator.redhat-cop.io/egressipam: egressipam-azure --- apiVersion: v1 kind: Namespace metadata: name: egressipam-azure-test-1 annotations: egressip-ipam-operator.redhat-cop.io/egressipam: egressipam-azure EOF Check the namespaces have IPs assigned\nkubectl get namespace egressipam-azure-test \\ egressipam-azure-test-1 -o yaml | grep egressips The output should look like:\negressip-ipam-operator.redhat-cop.io/egressips: 10.0.1.8 egressip-ipam-operator.redhat-cop.io/egressips: 10.0.1.7 Check they’re actually set as Egress IPs\noc get netnamespaces | egrep 'NAME|egress' The output should look like:\nNAME NETID EGRESS IPS egressip-ipam-operator 6374875 egressipam-azure-test 6917470 [\"10.0.1.8\"] egressipam-azure-test-1 16320378 [\"10.0.1.7\"] Finally check the Host Subnets for Egress IPS\noc get hostsubnets The output should look like:\nNAME HOST HOST IP SUBNET EGRESS CIDRS EGRESS IPS private-cluster-bj275-master-0 private-cluster-bj275-master-0 10.0.0.8 10.129.0.0/23 private-cluster-bj275-master-1 private-cluster-bj275-master-1 10.0.0.7 10.128.0.0/23 private-cluster-bj275-master-2 private-cluster-bj275-master-2 10.0.0.9 10.130.0.0/23 private-cluster-bj275-worker-eastus1-zt59t private-cluster-bj275-worker-eastus1-zt59t 10.0.1.4 10.128.2.0/23 [\"10.0.1.8\"] private-cluster-bj275-worker-eastus2-bfrwt private-cluster-bj275-worker-eastus2-bfrwt 10.0.1.5 10.129.2.0/23 [\"10.0.1.7\"] private-cluster-bj275-worker-eastus3-fgjzk private-cluster-bj275-worker-eastus3-fgjzk 10.0.1.6 10.131.0.0/23 Test Egress Log into your jumpbox and allow http into firewall\nsudo firewall-cmd --zone=public --add-service=http Install and start apache httpd\nsudo yum -y install httpd sudo systemctl start httpd Create a index.html\necho HELLO | sudo tee /var/www/html/index.html tail apache logs\nsudo tail -f /var/log/httpd/access_log Start an interactive pod in one of your new namespaces\nkubectl run -n egressipam-azure-test -i \\ --tty --rm debug --image=alpine \\ --restart=Never -- wget -O - 10.0.3.4 The output should look the following (the IP should match the egress IP of your namespace):\n10.0.1.7 - - [03/Feb/2022:19:33:54 +0000] \"GET / HTTP/1.1\" 200 6 \"-\" \"Wget\" ","description":"","tags":["ARO","Azure"],"title":"Using the Egressip Ipam Operator with a Private ARO Cluster","uri":"/docs/aro/egress-ipam-operator/"},{"content":"Observability ROSA ","description":"MOBB Docs and Guides for acm","tags":null,"title":"Advanced Cluster Management","uri":"/docs/redhat/acm/"},{"content":"Quickstarts / Getting Started Red Hat OpenShift on AWS (ROSA) Azure Red Hat OpenShift (ARO) Advanced Managed OpenShift ROSA Deploying ROSA in Private Link mode Add Public Ingress to Private Link Cluster Deploying ROSA in STS mode Deploying ROSA in STS mode with Private Link Deploying ROSA in STS mode with custom KMS Key Installing the AWS Load Balancer Operator on ROSA Adding AWS WAF in front of ROSA / OSD Use AWS Secrets CSI with ROSA in STS mode Use AWS CloudWatch Agent to push prometheus metrics to AWS CloudWatch Federating ROSA metrics to Prometheus with customer alerting Configuring Alerts for User Workloads in ROSA 4.9.x Using Amazon Web Services Elastic File System (EFS) on ROSA Using the AWS EFS CSI Driver Operator on ROSA 4.10.x Configuring a ROSA cluster to pull images from AWS Elastic Container Registry (ECR) Configuring a ROSA cluster to use ECR secret operator Deploy and use the AWS Kubernetes Controller S3 controller Deploy ROSA in an AWS Secure Environment Accelerator (ASEA) Landing Zone Verify Required Permissions for a ROSA STS deployment STS OIDC flow in ROSA Operators Dynamic Certificates for ROSA Custom Domain Security Reference Architecture for ROSA ARO Deploying private ARO Cluster with Jump Host access Using the Egressip Ipam Operator with a Private ARO Cluster Considerations for Disaster Recovery with ARO Getting Started with the Azure Key Vault CSI Driver Deploy and use the Azure Service Operator V1(ASO) Deploy and use the Azure Service Operator V2(ASO) Create an additional Ingress Controller for ARO Configure the Managed Upgrade Operator Configure ARO with Azure NetApp Trident Operator IBM Cloud Paks for Data Operator Setup Install ARO with Custom Domain using LetsEncrypt with cert manager Configure ARO for Nvidia GPU Workloads Configure ARO with Azure Front Door Create a point to site VPN connection for an ARO Cluster Configure access to ARO Image Registry Configure ARO with OpenShift Data Foundation Setting Up Quay on an ARO Cluster using Azure Container Storage via CLI via GUI Configure ARO with Azure Policy Create infrastructure nodes on an ARO Cluster Configure a load balancer service to use a static public IP GCP Deploy OSD in GCP using Pre-Existent VPC and Subnets Using Filestore with OpenShift Dedicated in GCP Advanced Cluster Manager (ACM) Deploy ACM Observability to a ROSA cluster Observability Configuring Alerts for User Workloads ROSA 4.9.x, 4.10.x ROSA 4.11+ Federating ROSA metrics to S3 Federating ROSA metrics to Prometheus with customer alerting Federating ROSA metrics to AWS Prometheus Configure ROSA STS Cluster Logging to CloudWatch Federating ARO metrics to Azure Files Sending ARO cluster logs to Azure Log Analytics Use AWS CloudWatch Agent to push prometheus metrics to AWS CloudWatch Security Kubernetes Secret Store CSI Driver Just the CSI itself + HashiCorp CSI + AWS Secrets CSI with ROSA in STS mode + Azure Key Vault CSI Driver Configuring Specific Identity Providers Configure GitLab as an identity provider for ROSA/OSD Configure GitLab as an identity provider for ARO Configure Azure AD as an identity provider for ARO Configure Azure AD as an identitiy provider for ARO with group claims Configure Azure AD as an identitiy provider for ROSA with group claims Configure Azure AD as an identity provider for ROSA/OSD Configure Azure AD as an identity provider for ARO via the CLI Configuring Group Synchronization Using Group Sync Operator with Azure Active Directory and ROSA/OSD Using Group Sync Operator with Okta and ROSA/OSD Deploying Advanced Security for Kubernetes in ROSA/ARO Deploying ACS in ROSA/ARO Applications Deploying Astronomer to OpenShift Deploying 3scale API Management to ROSA/OSD Ingress Configure a custom ingress TLS profile for ROSA/OSD Data Science on Jupyter Notebook on OpenShift Prerequistes and Concepts Build minimal notebook JupyterHub notebook with GPU Miscellaneous Demonstrating GitOps - ArgoCD Migrate Kubernetes Applications with Konveyor Crane Red Hat Cost Management for Cloud Services Deploy OpenShift Advanced Data Protection on a ROSA STS cluster Fixes / Workarounds Here be dragons - use at your own risk\nFix Cluster Logging Operator Addon for ROSA STS Clusters Stop default router from serving custom domain routes ","description":"MOBB Docs and Guides","tags":null,"title":"Documentation from the MOBB","uri":"/"},{"content":"Quickstarts / Getting Started Red Hat OpenShift on AWS (ROSA) Azure Red Hat OpenShift (ARO) Advanced Managed OpenShift ROSA Deploying ROSA in Private Link mode Add Public Ingress to Private Link Cluster Deploying ROSA in STS mode Deploying ROSA in STS mode with Private Link Deploying ROSA in STS mode with custom KMS Key Installing the AWS Load Balancer Operator on ROSA Adding AWS WAF in front of ROSA / OSD Use AWS Secrets CSI with ROSA in STS mode Use AWS CloudWatch Agent to push prometheus metrics to AWS CloudWatch Federating ROSA metrics to Prometheus with customer alerting Configuring Alerts for User Workloads in ROSA 4.9.x Using Amazon Web Services Elastic File System (EFS) on ROSA Using the AWS EFS CSI Driver Operator on ROSA 4.10.x Configuring a ROSA cluster to pull images from AWS Elastic Container Registry (ECR) Configuring a ROSA cluster to use ECR secret operator Deploy and use the AWS Kubernetes Controller S3 controller Deploy ROSA in an AWS Secure Environment Accelerator (ASEA) Landing Zone Verify Required Permissions for a ROSA STS deployment STS OIDC flow in ROSA Operators Dynamic Certificates for ROSA Custom Domain Security Reference Architecture for ROSA ARO Deploying private ARO Cluster with Jump Host access Using the Egressip Ipam Operator with a Private ARO Cluster Considerations for Disaster Recovery with ARO Getting Started with the Azure Key Vault CSI Driver Deploy and use the Azure Service Operator V1(ASO) Deploy and use the Azure Service Operator V2(ASO) Create an additional Ingress Controller for ARO Configure the Managed Upgrade Operator Configure ARO with Azure NetApp Trident Operator IBM Cloud Paks for Data Operator Setup Install ARO with Custom Domain using LetsEncrypt with cert manager Configure ARO for Nvidia GPU Workloads Configure ARO with Azure Front Door Create a point to site VPN connection for an ARO Cluster Configure access to ARO Image Registry Configure ARO with OpenShift Data Foundation Setting Up Quay on an ARO Cluster using Azure Container Storage via CLI via GUI Configure ARO with Azure Policy Create infrastructure nodes on an ARO Cluster Configure a load balancer service to use a static public IP GCP Deploy OSD in GCP using Pre-Existent VPC and Subnets Using Filestore with OpenShift Dedicated in GCP Advanced Cluster Manager (ACM) Deploy ACM Observability to a ROSA cluster Observability Configuring Alerts for User Workloads ROSA 4.9.x, 4.10.x ROSA 4.11+ Federating ROSA metrics to S3 Federating ROSA metrics to Prometheus with customer alerting Federating ROSA metrics to AWS Prometheus Configure ROSA STS Cluster Logging to CloudWatch Federating ARO metrics to Azure Files Sending ARO cluster logs to Azure Log Analytics Use AWS CloudWatch Agent to push prometheus metrics to AWS CloudWatch Security Kubernetes Secret Store CSI Driver Just the CSI itself + HashiCorp CSI + AWS Secrets CSI with ROSA in STS mode + Azure Key Vault CSI Driver Configuring Specific Identity Providers Configure GitLab as an identity provider for ROSA/OSD Configure GitLab as an identity provider for ARO Configure Azure AD as an identity provider for ARO Configure Azure AD as an identitiy provider for ARO with group claims Configure Azure AD as an identitiy provider for ROSA with group claims Configure Azure AD as an identity provider for ROSA/OSD Configure Azure AD as an identity provider for ARO via the CLI Configuring Group Synchronization Using Group Sync Operator with Azure Active Directory and ROSA/OSD Using Group Sync Operator with Okta and ROSA/OSD Deploying Advanced Security for Kubernetes in ROSA/ARO Deploying ACS in ROSA/ARO Applications Deploying Astronomer to OpenShift Deploying 3scale API Management to ROSA/OSD Ingress Configure a custom ingress TLS profile for ROSA/OSD Data Science on Jupyter Notebook on OpenShift Prerequistes and Concepts Build minimal notebook JupyterHub notebook with GPU Miscellaneous Demonstrating GitOps - ArgoCD Migrate Kubernetes Applications with Konveyor Crane Red Hat Cost Management for Cloud Services Deploy OpenShift Advanced Data Protection on a ROSA STS cluster Fixes / Workarounds Here be dragons - use at your own risk\nFix Cluster Logging Operator Addon for ROSA STS Clusters Stop default router from serving custom domain routes ","description":"MOBB Docs and Guides","tags":null,"title":"Documentation from the MOBB","uri":"/docs/"},{"content":"Misc Topics: Common Managed OpenShift References/Tasks Cost Management for Cloud Services Custom TLS Ciphers Jupyter Notebook K8s Secret Store CSI OADP Sharing Common Images Stop Default Router from Serving Custom Domains ","description":"MOBB Docs and Guides for misc","tags":null,"title":"Miscellaneous","uri":"/docs/misc/"},{"content":"MOBB Docs and Guides for group-claims\n","description":"MOBB Docs and Guides for group-claims","tags":null,"title":"MOBB Docs and Guides - group-claims","uri":"/docs/idp/group-claims/"},{"content":"MOBB Docs and Guides for OADP Deploying OADP on ROSA ","description":"MOBB Docs and Guides for oadp","tags":null,"title":"MOBB Docs and Guides - oadp","uri":"/docs/misc/oadp/"},{"content":" OpenShift Logging Azure Log Analytics ","description":"MOBB Docs and Guides for Observability","tags":null,"title":"Observability","uri":"/docs/o11y/"},{"content":" Filestore Storage for OSD on GCP Creating OSD on GCP w/ Existing VPC ","description":"MOBB Docs and Guides for GCP","tags":["GCP","OSD"],"title":"OSD on Google Cloud","uri":"/docs/gcp/"},{"content":" 3Scale API Management on ROSA and OSD RHACS on ARO/ROSA GitOps w/ ArgoCD Migrate K8s Apps w/ Konveyer Crane ","description":"","tags":null,"title":"Other Red Hat Products","uri":"/docs/redhat/"},{"content":"","description":"","tags":null,"title":"Categories","uri":"/categories/"}]